{
    "abstractText": "One of the major interests in extreme-value statistics is to infer the tail properties of the distribution functions in the domain of attraction of an extreme-value distribution and to predict rare events. In recent years, much effort in developing new methodologies has been made by many researchers in this area so as to diminish the impact of the bias in the estimation and achieve some asymptotic optimality in inference problems such as estimating the optimal sample fractions and constructing confidence intervals of various quantities. In particular, bootstrap and empirical likelihood methods, which have been widely used in many areas of statistics, have drawn attention. This paper reviews some novel applications of the bootstrap and the empirical likelihood techniques in extreme-value statistics.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yongcheng Qi"
        },
        {
            "affiliations": [],
            "name": "Y. Qi"
        }
    ],
    "id": "SP:23e3dbadbdf783ed78d4ff1b5b58b1623e976353",
    "references": [
        {
            "authors": [
                "J.N. Bacro",
                "M. Brito"
            ],
            "title": "A tail bootstrap procedure for estimating the tail Pareto-index",
            "venue": "J. Stat. Plan",
            "year": 1993
        },
        {
            "authors": [
                "J. Caers",
                "J. Van Dyck"
            ],
            "title": "Nonparametric tail estimation using a double bootstrap method",
            "year": 1998
        },
        {
            "authors": [
                "S.H. Lo"
            ],
            "title": "On a mapping approach to investigating the bootstrap",
            "venue": "Stat. Data Anal",
            "year": 1998
        },
        {
            "authors": [
                "L. de Haan"
            ],
            "title": "Penultimate approximation for Hill\u2019s Estimator",
            "venue": "Theory Relat. Fields 107,",
            "year": 1997
        },
        {
            "authors": [
                "J. Danielsson",
                "L. de Haan",
                "L. Peng",
                "C.G. de Vries"
            ],
            "title": "Using a bootstrap method to choose",
            "year": 2001
        },
        {
            "authors": [
                "C. Vries"
            ],
            "title": "sample fraction in tail index estimation",
            "venue": "J. Multivar. Anal",
            "year": 2001
        },
        {
            "authors": [
                "L. de Haan",
                "A. Ferreira"
            ],
            "title": "Extreme Value Theory: An Introduction",
            "venue": "Empir. Finance 4,",
            "year": 1998
        },
        {
            "authors": [
                "Ann. Stat"
            ],
            "title": "A bootstrap-based method to achieve optimality",
            "year": 1989
        },
        {
            "authors": [
                "E. Kaufmann"
            ],
            "title": "Selection of the optimal sample fraction in univariate extreme value",
            "venue": "in estimating the extreme-value index. Extremes",
            "year": 1999
        },
        {
            "authors": [
                "N. Efron"
            ],
            "title": "Bootstrap methods: another look at the jackknife",
            "venue": "estimation. Stoch. Process. Appl. 75,",
            "year": 1979
        },
        {
            "authors": [
                "A. Ferreira"
            ],
            "title": "Optimal asymptotic estimation of small exceedance probabilities",
            "venue": "J. Stat. Plan. Infer-",
            "year": 2000
        },
        {
            "authors": [
                "A. Ferreira",
                "L. de Haan",
                "L. Peng"
            ],
            "title": "On optimising the estimation of high quantiles of a probability",
            "year": 2002
        },
        {
            "authors": [
                "M.I. Fraga Alves"
            ],
            "title": "A location invariant Hill-type estimator. Extremes",
            "year": 1978
        },
        {
            "authors": [
                "P. Groeneboom",
                "H.P. Lopuha\u00e4",
                "P.P. de Wolf"
            ],
            "title": "Kernel-type estimators for the extreme value",
            "year": 2002
        },
        {
            "authors": [
                "index. Ann"
            ],
            "title": "Bootstrap confidence intervals for the Pareto index",
            "venue": "Commun. Stat. Theory Methods",
            "year": 2003
        },
        {
            "authors": [
                "P. Hall"
            ],
            "title": "Using the bootstrap to estimate mean squared error and select smoothing parameter",
            "year": 2000
        },
        {
            "authors": [
                "L. La Scala"
            ],
            "title": "Methodology and algorithms of empirical likelihood",
            "venue": "J. Multivar. Anal",
            "year": 1990
        },
        {
            "authors": [
                "B.M. Hill"
            ],
            "title": "A simple general approach to inference about the tail of a distribution",
            "venue": "Ann. Stat",
            "year": 1990
        },
        {
            "authors": [
                "J. Lu",
                "L. Peng"
            ],
            "title": "Likelihood based confidence intervals for the tail index",
            "venue": "Extremes",
            "year": 1975
        },
        {
            "authors": [
                "D.M. Mason"
            ],
            "title": "Laws of large numbers for sums of extreme values",
            "venue": "Ann. Probab",
            "year": 1982
        },
        {
            "authors": [
                "A. Owen"
            ],
            "title": "Extremes and Integrated Risk Management, Risk Books, pp. 37\u201349",
            "year": 2000
        },
        {
            "authors": [
                "A. Owen"
            ],
            "title": "Empirical likelihood regions",
            "venue": "Ann. Stat",
            "year": 1990
        },
        {
            "authors": [
                "L. Peng",
                "Y. Qi"
            ],
            "title": "A new calibration method of constructing empirical likelihood-based confidence",
            "year": 2004
        },
        {
            "authors": [
                "Y. Qi"
            ],
            "title": "Confidence regions for high quantiles of a heavy tailed distribution",
            "venue": "index. Aust. N.Z. J. Stat. 48,",
            "year": 2006
        },
        {
            "authors": [
                "L. Peng",
                "Y. Qi"
            ],
            "title": "Bootstrap approximation of tail dependence function",
            "venue": "J. Multivar. Anal",
            "year": 2007
        },
        {
            "authors": [
                "J. Lawless"
            ],
            "title": "Empirical likelihood and general estimating equations",
            "venue": "Ann. Stat",
            "year": 1998
        },
        {
            "authors": [
                "J. Shao",
                "D. Tu"
            ],
            "title": "The Jackknife and the Bootstrap",
            "year": 1995
        },
        {
            "authors": [
                "I. Weissman"
            ],
            "title": "Estimation of parameters and large quantiles based on the k largest observations. J",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Keywords Bootstrap \u00b7 Confidence interval \u00b7 Empirical likelihood \u00b7 Extremes \u00b7 High quantile \u00b7 Sample fraction \u00b7 Tail index\nAMS 2000 Subject Classification 60G32 \u00b7 62G09 \u00b7 62G15"
        },
        {
            "heading": "1 Introduction",
            "text": "It is a great pleasure for me to write this review paper in honor of Professor Laurens de Haan on the occasion of his 70th birthday.\nDedicated to Professor Laurens de Haan on the occasion of his 70th birthday.\nY. Qi (B) Department of Mathematics and Statistics, University of Minnesota Duluth, 1117 University Drive, Duluth, MN 55812, USA e-mail: yqi@d.umn.edu\nLet F be a distribution function. If there exist constants an > 0 and bn \u2208 R and a non-degenerate distribution function G such that\nlim n\u2192\u221e F\nn(anx + bn) = G(x) (1.1)\nfor all continuity points x of G, then we say that F is in the domain of attraction of the distribution G and denote F \u2208 D(G). It is well known that the limiting distribution G must be of the type of one of the so-called extreme-value distributions:\nG\u03b3 (x) = exp{\u2212(1 + \u03b3 x)\u22121/\u03b3 } for 1 + \u03b3 x > 0, where (1 + \u03b3 x)\u22121/\u03b3 is interpreted as e\u2212x for \u03b3 = 0, and \u03b3 is called the tail index of F or the extreme-value index. For details, see, e.g., Galambos (1978), or de Haan and Ferreira (2006).\nIn extreme-value statistics, many important characteristics of the underlying distribution F such as the tail index, high quantiles and endpoint are related to the tail behavior of the distribution. The estimation of these characteristics is very important in extreme value statistics, and it plays a central role in predicting rare events. Based on a few very large observations, various estimators for the extreme-value index, high quantiles and the endpoint have been proposed in the literature, and their asymptotic behaviors have been investigated; see, e.g., the Hill estimator (Hill 1975) for the case \u03b3 > 0, Pickands estimator (Pickands 1975; Pereira 1993) and moment estimator (Dekkers et al. 1989) for general \u03b3 \u2208 R, and moment-based estimators for high quantiles and the endpoint (Danielsson and Vries 1997; Ferreira et al. 2003), to just mention a few.\nIn this paper we review some recent developments of bootstrap techniques and empirical likelihood methods in extreme-value statistics. The rest of the paper is organized as follows. In Section 2, we introduce some applications of bootstrap techniques in estimating optimal sample fractions in estimation problems (which is largely due to the contribution of Laurens de Haan and his collaborators) and in constructing confidence intervals for the extreme value indices. In particular, we will present bootstrap procedures for the Hill estimator and the moment estimator with some technical details. In Section 3 we demonstrate how to use empirical likelihood methods to construct confidence intervals for the tail index and high quantiles of a heavy-tailed distribution.\nThis review paper aims to give a clear picture about the applications of the aforementioned methods in extremes. It is not the author\u2019s intention if any contributions in these areas are omitted due to limited reference resources."
        },
        {
            "heading": "2 Bootstrap Methods",
            "text": "The bootstrap is a data-driven method that has a very wide range of applications in statistics. Initiated by Efron (1979), the classic bootstrap approach uses Monte Carlo simulations to generate an empirical estimate for the sampling\ndistribution of a statistic by randomly drawing a large number of samples of the same size n from the data, where n is the size of the sample under consideration.\nAlthough the bootstrap has been widely used in many areas, the method has its limitation in extremes. It is well known that a full-sample bootstrap does not work for the extremes. As Hall (1990) pointed out, the full-sample bootstrap statistics have a zero bias if the statistics are linear in data. Hence, in extreme-value statistics, a full-sample bootstrap approach can be invalid if the original statistics have a non-negligible bias. Angus (1993) showed that the bootstrapping distribution for the extremes does not converge to an extremevalue distribution. In fact, its limit is a random probability measure. As was illustrated by Shao and Tu (1995, Ex. 4, p. 123), a sub-sample bootstrap is still valid in this case. Further, Geluk and de Haan (2002) showed that only the sub-sample bootstrap can estimate the limiting distribution for intermediate order statistics.\nThroughout let F\u2212(u) = inf{x : F(x) \u2265 u} be the generalized inverse function of F and U be the inverse function of 11\u2212F , ie., U(x) = F\u2212(1 \u2212 1x ). Define the (1 \u2212 p)-th quantile of F by xp = F\u2212(1 \u2212 p) = inf{x : F(x) \u2265 1 \u2212 p} for 0 < p < 1 and the (right) endpoint by \u03c9F = sup{x : F(x) < 1}.\nAssume that {X j, j \u2265 1} is a sequence of independent and identically distributed (iid) random variables with distribution function F(x) satisfying Eq. 1.1, and only a finite sample {X j, 1 \u2264 j \u2264 n} is observed. Let Xn,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 Xn,n be the order statistics of X1, \u00b7 \u00b7 \u00b7 , Xn, and Fn(x) = 1n \u2211n i=1 I(Xi \u2264 x) be the empirical distribution. Assume {X\u2217j , j \u2265 1} is a sequence of iid random variables with the distribution function Fn. For each integer m \u2265 1, let X\u2217m,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 X\u2217m,m be the order statistics of (X\u22171 , \u00b7 \u00b7 \u00b7 , X\u2217m) which is a bootstrap sample of size m from{X j, 1 \u2264 j \u2264 n}. A full-sample bootstrap is the case when m = n. In contrast, a sub-sample bootstrap is the case when m < n and usually the conditions m = m(n) = o(n) and m \u2192 \u221e are also required.\nOne of the common properties for the estimators of the tail index and the high quantiles is that their asymptotic biases are dependent on the fraction of the sample that is used in the estimation. The asymptotic biases increase with the increase of the sample fraction. However, when the sample fraction is small, the variances of the estimators are large. As long as accuracy of the estimation is concerned, one has to decide how to choose a sample fraction in order to achieve certain asymptotic optimality such as the minimal asymptotic mean squared error.\n2.1 Optimal Sample Fractions in Estimation\n1. Hill estimator In order to demonstrate how to bootstrap the optimal sample fraction in extreme-value statistics, we first consider estimation of the tail index in a special case when \u03b3 > 0. It is known that in this case, Eq. 1.1 is equivalent to\n1 \u2212 F(x) = x\u22121/\u03b3 (x), for x > 0, (2.1)\nwhere (x) is a slowly varying function with limt\u2192\u221e (tx)/ (t) = 1 for all x > 0. For i \u2265 1 define\nM(i)(n, k) = 1 k\nk\u2211 i=1 (log Xn,n\u2212i+1 \u2212 log Xn,n\u2212k)i.\nThen\n\u03b3\u0302 (1)(n, k) := M(1)(n, k) = 1 k\nk\u2211 i=1 log Xn,n\u2212i+1 \u2212 log Xn,n\u2212k\nis the well-known Hill estimator for \u03b3 proposed by Hill (1975), where k = k(n) satisfies\nk \u2192 \u221e and k/n \u2192 0 as n \u2192 \u221e. (2.2)\nMason (1982) showed that \u03b3\u0302 (1)(n, k) is a consistent estimator of \u03b3 . In the study of the asymptotic distribution of \u03b3\u0302 (1)(n, k), a popular second order condition stronger than Eq. 2.1 has been used in many papers. Assume that there exists a function A(t) not changing sign at infinity such that\nlim t\u2192\u221e U(tx)/U(t) \u2212 x\u03b3 A(t) = x\u03b3 x \u03c1 \u2212 1 \u03c1\n(2.3)\nfor all x > 0, where \u03c1 < 0. In this case, |A| is a regularly varying function with index \u03c1 (write |A| \u2208 R\u03c1); for details see de Haan and Stadtm\u00fcller (1996). Under Eq. 2.3, one can write\n\u03b3\u0302 (1)(n, k) \u2212 \u03b3 = \u03b3\u221a k P(1)n + 1 1 \u2212 \u03c1 ( n k ) + op ( A ( n k )) , (2.4)\nwhere P(1)n is a random variable which converges in distribution to a standard normal distribution; see, e.g., de Haan and Peng (1998). Therefore, the asymptotic mean squared error of \u03b3\u0302 (1)(n, k), denoted by AMSE(\u03b3\u0302 (1)(n, k)), is given by\nAMSE(\u03b3\u0302 (1)(n, k)) = \u03b3 2 k + 1 (1 \u2212 \u03c1)2 A 2 ( n k ) . (2.5)\nThe optimal sample fraction for Hill\u2019s estimator is denoted by\nk1(n) = arg min k AMSE(\u03b3\u0302 (1)(n, k)).\nOur objective is to provide a consistent estimator for k1(n).\nIn Eq. 2.4, the Hill estimator is expanded after centered at \u03b3 . Since \u03b3 is unknown, a more suitable statistic for the bootstrap is M(n, k) := M(2)(n, k) \u2212 2(M(1)(n, k))2, which has a similar expansion:\nM(n, k) = 2\u03b3 2\n\u221a k\nP(2)n + 2\u03b3\u03c1 (1 \u2212 \u03c1)2 A ( n k ) (1 + op(1)), (2.6)\nwhere P(2)n is a random variable converging in distribution to a standard normal distribution. Therefore, the asymptotic mean squared error for M(n, k) is\nAMSE(M(n, k)) = 4\u03b3 4 k + 4\u03b3 2\u03c12 (1 \u2212 \u03c1)4 A 2 ( n k ) . (2.7)\nSet\nk\u03041(n) = arg min k AMSE(M(n, k)).\nThen k\u03041(n) and k1(n) are of the same order. In order to determine the optimal sample fractions, we summarize a theorem from the proofs in de Haan and Peng (1998) or Danielsson et al. (2001).\nTheorem 2.1 For |A| \u2208 R\u03c1 (\u03c1 < 0), constants \u03c3 > 0 and d = 0, define\nk0(n) = arg min k\n( \u03c3 2\nk + d2 A2 ( n k )) .\nThen\nk0(n) = ns\u2212(\u03c3 2/(nd2)) (1 + o(1)) \u2208 R\u22122\u03c1/(1\u22122\u03c1), as n \u2192 \u221e,\nwhere s\u2212 is the inverse function of s, with s given by\nA2(t) = \u222b \u221e\nt s(u)du(1 + o(1)) as t \u2192 \u221e.\nFrom Eqs. 2.5 and 2.7 and Theorem 2.1 one can conclude the following theorem.\nTheorem 2.2 (Danielsson et al. 2001) Suppose Eq. 2.3 holds. Then\nk1(n) = ns\u2212(\u03b3 2(1 \u2212 \u03c1)2/n) (1 + o(1)),\nk\u03041(n) = ns\u2212(\u03b3 2(1 \u2212 \u03c1)4/(n\u03c12)) (1 + o(1)), and therefore\nk\u03041(n) k1(n)\n\u223c (\n1 \u2212 1 \u03c1\n)1/(1\u22122\u03c1) . (2.8)\nEquation 2.8 indicates that estimating k1(n) is equivalent to estimating k\u03041(n) if a consistent estimator for the second order parameter \u03c1 is available.\nSet Xn = (X1, \u00b7 \u00b7 \u00b7 , Xn). For a bootstrap sample (X\u22171 , \u00b7 \u00b7 \u00b7 , X\u2217m), define the following bootstrap statistics\nM(i)\u2217 (m, r) = 1\nr\nr\u2211\ni=1\n( log X\u2217m,m\u2212i+1 \u2212 log X\u2217m,m\u2212r )i , i = 1, 2 (2.9)\nand M\u2217(m, r) = M(2)\u2217 (m, r) \u2212 2(M(1)\u2217 (m, r))2, where r \u2192 \u221e and r/m \u2192 0, and m = O(n1\u2212\u03b5) for some 0 < \u03b5 < 1.\nConditional on Xn, an expansion for M\u2217(m, r), similar to Eq. 2.6, can be obtained. Let r\u2217(m) be the integer such that the conditional second moment E((M\u2217(m, r))2|Xn) is minimal with respect to r. Then r\u2217(m) can obtained by minimizing the estimated second moment E((M\u2217(m, r))2|Xn) via taking a large number of bootstrap samples. In fact, r\u2217(m) has the following asymptotic property.\nTheorem 2.3 (Danielsson et al. 2001) Assume Eq. 2.3 holds. Then\nr\u2217(m)s\u2212(\u03b3 2(1 \u2212 \u03c1)4/(m\u03c12)) m p\u2192 1 as n \u2192 \u221e.\nThe above two theorems play an important role in constructing a consistent bootstrap estimator for k\u03041(n). Now assume Eq. 2.3 holds with A(t) = ct\u03c1 . Then one can choose s(t) = \u22122\u03c1c2t2\u03c1\u22121, and thus s\u2212(u) = (2\u03c1c2)1/(1\u22122\u03c1)u\u22121/(1\u22122\u03c1). It follows from Theorems 2.2 and 2.3 that\n( m n )2\u03c1/(1\u22122\u03c1) r\u2217(m) k\u03041(n) p\u2192 1 as n \u2192 \u221e. (2.10)\nAlthough the second order parameter \u03c1 can be estimated consistently by some estimator, say \u03c1\u0302n, it is not a good idea to use Eq. 2.10 directly to estimate k\u03041(n). This is due to the fact that it requires a faster convergence rate than (log n)\u22121 for \u03c1\u0302n so that\n( m n )2\u03c1\u0302n/(1\u22122\u03c1\u0302n)/(m n )2\u03c1/(1\u22122\u03c1) p\u2192 1.\nFortunately, this obstacle can be overcome by obtaining two sub-sample bootstrap estimators, and the procedure is described as follows.\nStep 1. Set m = m1 = [n1\u2212\u03b5] for some \u03b5 \u2208 (0, 1/2), where [x] denotes the integer part of x. Draw a large number of bootstrap samples of size m from the empirical distribution Fn so that E(M\u2217(m1, r))2|Xn) is well estimated and determine r\u2217(m1) such that the estimated E(M\u2217(m1, r))2|Xn) is minimal at r = r\u2217(m1).\nStep 2. Set m = m2 = [m21/n], and repeat Step 1 to get r\u2217(m2) such that E(M\u2217(m2, r))2|Xn) is minimal at r = r\u2217(m2). Step 3. Estimate \u03c1 by \u03c1\u0302n = log r\u2217(m1)/(\u22122 log m1 + 2 log r\u2217(m1)), which is consistent. Step 4. Define an estimator for k1(n) by\nk\u03021(n) := (r \u2217(m1))2\nr\u2217(m2)\n(\n1 \u2212 1 \u03c1\u0302n\n)1/(2\u03c1\u0302n\u22121) .\nThe consistency of the estimator follows from Eqs. 2.8 and 2.10.\nRemark 1 Note that in the above procedure m1 = [n1\u2212\u03b5] is an appropriate choice for any \u03b5 \u2208 (0, 1/2). A fully automatic procedure for the determination of m1 is given in Danielsson et al. (2001); namely, choose m1 such that the ratio\nR(m1) := ( E(M\u2217(m1, r\u2217(m1)))2|Xn) )2\nE(M\u2217(m2, r\u2217(m2)))2|Xn) is minimal.\nRemark 2 We have seen that the validity of the sub-sample bootstrap procedure requires that mi \u2192 \u221e and mi/n \u2192 0 for i = 1, 2 as n \u2192 \u221e. For a very large sample size n, this shouldn\u2019t be a problem. For a small or moderate sample, it might cause a problem since m2 may be too small. For example, assume n = 500. If m1 = 100, then m2 = [1002/500] = 20, and the number of observations used to construct the bootstrap statistics from a bootstrap sample is very small. Noting that the second bootstrap sample size m2 = [m21/n] is selected in a way that one can cancel the factor in Eq. 2.10, a possible solution is to use the following formula m2 = [m1(m1/n)a] for some fixed a \u2208 (0, 1]. If a is small, this results in a larger m2. Consequently, we have the following consistent estimator for k1(n):\nk\u0302(a)1 (n) := (r\u2217(m1))(a+1)/a\n(r\u2217(m2))1/a\n(\n1 \u2212 1 \u03c1\u0302n\n)1/(2\u03c1\u0302n\u22121) .\nIn particular, when a = 1, k\u0302(1)1 (n) = k\u03021(n).\n2. Moment estimator Let F \u2208 D(G\u03b3 ) for some \u03b3 \u2208 R. The moment estimator for \u03b3 , proposed by Dekkers et al. (1989), is defined as\n\u03b3\u0302 (2)(n, k) = M(1)(n, k) + 1 \u2212 1 2\n(\n1 \u2212 (M (1)(n, k))2\nM(2)(n, k)\n)\u22121 .\nSet \u03b3+ = max(\u03b3, 0) and \u03b3\u2212 = min(\u03b3, 0). The assumption F \u2208 D(G\u03b3 ) is equivalent to the condition that there is a positive function a(t) such that\nlim t\u2192\u221e U(at) \u2212 U(t) a(t) = x \u03b3 \u2212 1 \u03b3 for x > 0,\nwhich implies\nlim t\u2192\u221e log U(at) \u2212 log U(t) a(t)/U(t) = x \u03b3\u2212 \u2212 1 \u03b3\u2212 for x > 0.\nWe need the following second order condition: there exists a function A(t) not changing sign at infinity such that\nlim t\u2192\u221e\nlog U(at)\u2212log U(t) a(t)/U(t) \u2212 x \u03b3\u2212\u22121 \u03b3\u2212\nA(t) = H(x), (2.11)\nwhere\nH(x) = 1 \u03c1 ( x\u03c1+\u03b3\u2212 \u2212 1 \u03c1 + \u03b3\u2212 \u2212 x\u03b3\u2212 \u2212 1 \u03b3\u2212 )\n=\n\u23a7 \u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n(log x)2/2 if \u03c1 = 0, \u03b3 \u2265 0, 1\n\u03b3\n(\nx\u03b3 log x \u2212 x \u03b3 \u2212 1 \u03b3\n)\nif \u03c1 = 0, \u03b3 < 0, 1\n\u03c1 ( x\u03c1+\u03b3\u2212 \u2212 1 \u03c1 + \u03b3\u2212 \u2212 x\u03b3\u2212 \u2212 1 \u03b3\u2212 ) if \u03c1 < 0.\nSimilarly to the Hill estimator, \u03b3\u0302 (2)(n, k) \u2212 \u03b3 can be decomposed into two parts, a non-degenerate term with an asymptotic normal distribution and a degenerate bias term, and the asymptotic mean squared error of \u03b3\u0302 (2)(n, k) is given by\nAMSE(\u03b3\u0302 (2)(n, k)) = V 2(\u03b3 )\nk + b2(\u03b3, \u03c1)A20 ( n k ) ,\nwhere\nV2(\u03b3 ) = \u23a7 \u23a8\n\u23a9\n\u03b3 2 + 1 if \u03b3 > 0, (1 \u2212 \u03b3 )2(1 \u2212 2\u03b3 )(6\u03b3 2 \u2212 \u03b3 + 1)\n(1 \u2212 3\u03b3 )(1 \u2212 4\u03b3 ) if \u03b3 < 0,\nb(\u03b3, \u03c1) =\n\u23a7 \u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\u03b3 \u03c1(1 \u2212 \u03c1) + 1\n(1 \u2212 \u03c1)2 if \u03b3 > 0, 1\n1 \u2212 \u03b3 if \u03c1 < \u03b3 < 0, (1 \u2212 \u03b3 )(1 \u2212 2\u03b3 ) (1 \u2212 \u03c1 \u2212 \u03b3 )(1 \u2212 \u03c1 \u2212 2\u03b3 ) if \u03b3 < \u03c1\nand\nA0(t) =\n\u23a7 \u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\nA(t) if \u03b3 > 0, a(t) U(t) if\u03c1 < \u03b3 < 0, A(t) if \u03b3 < \u03c1.\nTheorem 2.4 (Draisma et al. 1999) Suppose that F \u2208 D(G\u03b3 ) and that Eq. 2.11 holds for \u03c1 < 0, \u03b3 = \u03c1 and \u03b3 = 0. Set k2(n) = arg mink AMSE(\u03b3\u0302 (2)(n, k)). Then\nk2(n) \u223c (\nV2(\u03b3 ) b2(\u03b3, \u03c1) )1/(1\u22122\u03c1\u2217) n s\u2212\n( 1 n )\nas n \u2192 \u221e, where s\u2212 is the inverse function of the decreasing function s satisfying\nA20(t) = \u222b \u221e\nt s(u)du(1 + o(1)).\nDefine an alternative estimator for \u03b3 by\n\u03b3\u0302 (3)(n, k) = \u221a M(2)(n, k)/2 + 1 \u2212 2 3\n(\n1 \u2212 M (1)(n, k)M(2)(n, k)\nM(3)(n, k)\n)\u22121 .\nThen the auxiliary statistic \u03b3\u0302 (2)(n, k) \u2212 \u03b3\u0302 (3)(n, k) has a similar asymptotic behavior to that of \u03b3\u0302 (2)(n, k) \u2212 \u03b3 .\nTheorem 2.5 (Draisma et al. 1999) Assume that the conditions of Theorem 2.4 hold. Set\nk\u03042(n) = arg min k AMSE(\u03b3\u0302 (2)(n, k) \u2212 \u03b3\u0302 (3)(n, k)). Then\nk\u03042(n) \u223c ( V\u03042(\u03b3 )\nb\u03042(\u03b3, \u03c1)\n)1/(1\u22122\u03c1\u2217) n\ns\u2212 (\n1 n\n) as n \u2192 \u221e,\nwhere\nV\u03042(\u03b3 ) =\n\u23a7 \u23aa\u23a8\n\u23aa\u23a9\n1 4 (\u03b3 2+1) if \u03b3 > 0, 1\n4 (1\u2212\u03b3 )2(1\u22128\u03b3 +48\u03b3 2\u2212154\u03b3 3+263\u03b3 4\u2212222\u03b3 5+72\u03b3 6) (1\u22122\u03b3 )(1\u22123\u03b3 )(1\u22124\u03b3 )(1\u22125\u03b3 )(1\u22126\u03b3 ) if \u03b3 < 0,\nb\u0304(\u03b3, \u03c1) =\n\u23a7 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\u2212\u03b3 (1 \u2212 \u03c1) + \u03c1 2(1 \u2212 \u03c1)3 if \u03b3 > 0,\n(1 \u2212 2\u03c1) \u2212 \u221a(1 \u2212 \u03c1)(1 \u2212 2\u03c1) (1 \u2212 \u03c1)(1 \u2212 2\u03c1) if \u03c1 < \u03b3 < 0, 1\n2 \u2212\u03c1(1 \u2212 \u03b3 )2 (1 \u2212 \u03c1 \u2212 \u03b3 )(1 \u2212 \u03c1 \u2212 2\u03b3 )(1 \u2212 \u03c1 \u2212 3\u03b3 ) if \u03b3 < \u03c1.\nFor a bootstrap sample of size m, (X\u22171 , \u00b7 \u00b7 \u00b7 , X\u2217m), define M(i)\u2217 (m, r) as in Eq. 2.9 and set\n\u03b3\u0302 (2)\u2217 (m, r) = M(1)\u2217 (m, r) + 1 \u2212 1\n2\n(\n1 \u2212 (M (1)\u2217 (m, r))2\nM(2)\u2217 (m, r)\n)\u22121 ,\nand\n\u03b3\u0302 (3)\u2217 (m, r) = \u221a\nM(2)\u2217 (m, r)/2 + 1 \u2212 2 3\n(\n1 \u2212 M (1)\u2217 (m, r)M(2)\u2217 (m, r)\nM(3)\u2217 (m, r)\n)\u22121 ,\nwhere r \u2192 \u221e and r/m \u2192 0, and m = O(n1\u2212\u03b5) for some 0 < \u03b5 < 1. Further, set for some \u03b4 > 0\n\u03b3\u0302 (4)\u2217 (m, r) = ( \u03b3\u0302 (2)\u2217 (m, r) \u2212 \u03b3\u0302 (3)\u2217 (m, r) ) I ( |\u03b3\u0302 (2)\u2217 (m, r) \u2212 \u03b3\u0302 (3)\u2217 (m, r)| < r\u03b4\u22121/2 ) .\nTheorem 2.6 (Draisma et al. 1999) Suppose the conditions of Theorem 2.4 hold and m = O(n1\u2212\u03b5) for some 0 < \u03b5 < 1/2. Define r\u22173(m) = arg min\nr E((\u03b3\u0302 (4)\u2217 (m, r)) 2|Xn). Then\nr\u22173(m) /\n\u23a7 \u23a8\n\u23a9\n( V\u03042(\u03b3 )\nb\u0304 2(\u03b3, \u03c1)\n)1/(1\u22122\u03c1\u2217) m\ns\u2212( 1m )\n\u23ab \u23ac\n\u23ad\np\u2192 1 as n \u2192 \u221e.\nSuppose the conditions of Theorem 2.6 hold and A0(t) = ct\u03c1\u2217 . Then from Theorems 2.5 and 2.6 we have\nk\u03042(n) r\u22173(m) (m n ) \u22122\u03c1\u2217 1\u22122\u03c1\u2217 p\u2192 1.\nUsing the same procedure for Hill\u2019s estimator, we obtain r\u22173(m1) and r \u2217 3(m2),\nwhere m2 = [m21/n]. Define\nk\u03022(n) = (k \u2217 3(m1)) 2\nk\u22173(m2) ( V2(\u03b3\u0302n)b\u0304 2(\u03b3\u0302n, \u03c1\u0302\u2217n) V\u03042(\u03b3\u0302n)b 2(\u03b3\u0302n, \u03c1\u0302\u2217n) ) 1 1\u2212\u03c1\u0302\u2217n ,\nwhere \u03b3\u0302n and \u03c1\u0302\u2217n are any consistent estimators of \u03b3 and \u03c1\u2217, respectively. Then\nk\u03022(n) k2(n) p\u2192 1.\n3. More applications We have introduced two novel applications of the bootstrap method. In fact, under certain technical conditions, the bootstrap approach has proved to be valid in estimating the optimal sample fractions for Pickands-type estimators of the tail index (Draisma et al. 1999), estimators of high quantiles and endpoints (Ferreira et al. 2003), and estimators of small exceedances probability (Ferreira 2002). The bootstrap procedures for these estimators are the same. Below is a summary of the fundamental steps used to estimate the optimal sample fraction, say k0(n), that minimizes the asymptotic mean squared error of an estimator of the aforementioned characteristics.\n\u2022 Construct an auxiliary statistic in a way that k\u03040(n)/k0(n) \u223c C, where k\u03040(n) is the sample fraction that minimizes the asymptotic second moment of\nthe auxiliary statistic, and C is a constant depending on some unknown parameters and can be consistently estimated by a statistic, say C\u0302. \u2022 Use Step 1 and Step 2 introduced for Hill\u2019s estimator to estimate two optimal sample fractions that minimize the second moments of the two bootstrap versions of the auxiliary statistic based on two bootstrap samples of sizes m1 and m2, and denote them as r\u2217(m1) and r\u2217(m2) accordingly. \u2022 Define k\u03020(n) = (r \u2217(m1))2\nC\u0302r\u2217(m2) . Then k\u03020(n) is a consistent estimator of k0(n),\nthat is, k\u03020(n) k0(n) p\u2192 1.\nFor a class of kernel estimators of the tail index, the selection of the bandwidth is parallel to the selection of the sample fraction in the above estimators. A similar bootstrap procedure for optimal bandwidth choice for kernel estimators can be found in Groeneboom et al. (2003).\nPictet et al. (1998) derived an algorithm to reduce the bias of the Hill estimator. The algorithm is based on a subsample bootstrap combined with the jackknife method.\nIt is worth mentioning that some methods other than the bootstrap can be found in the literature. For example, Drees and Kaufmann (1998) and Matthys and Beirlant (2000) proposed some automatic methods for momenttype estimators of the tail index, and Fraga Alves (2001) gave an algorithm for estimation of the optimal sample fraction in the location invariant Hill-type estimators.\n2.2 Bootstrap Confidence Intervals\nThe bootstrap method has been commonly used to estimate the sampling distribution of a statistic which allows us to build confidence intervals of a population characteristic. The bootstrap approach avoids estimating the unknown parameters in the limiting distribution of the statistic, and it is extremely useful when the limiting distribution of the statistic is unknown or too complicated to calculate. In many situations such an approach is satisfactory in the sense that the bootstrap confidence intervals have a good coverage accuracy.\nGuillou (2000) assessed the accuracy of the sub-sample bootstrap method for the tail index of a heavy-tailed distribution and found out that the accuracy of the sub-sample bootstrap approach is often worse than that of the asymptotic approximation of the statistic. Bacro and Brito (1998) proposed a different bootstrap procedure for the tail index of a heavy-tailed distribution by resampling directly from k = kn log-spacings, i(log Xn,n\u2212i+1 \u2212 log Xn,n\u2212i), i = 1, . . . , k. They didn\u2019t consider the accuracy of the coverage for the approach. Nevertheless, this procedure is valid if the Hill estimator is asymptotically unbiased.\nWhen applying the sub-sample bootstrap procedure, one has to decide how to choose the size of the sub-samples. Caers and Van Dyck (1998) and Guillou (2000) discussed this issue. Caers and Van Dyck (1998) even proposed a double\nbootstrap method that estimates the optimal sample fraction based on a subsample bootstrap in the first stage and then constructs the confidence intervals based on a sub-sample bootstrap from the original data in the second stage.\nOn the other hand, a full-sample bootstrap still works when the sample fraction in the estimation is in the range that the estimator is asymptotically unbiased. El-Nouty and Guillou (2000) examined an estimator for the tail index of a heavy-tailed distribution and proved the validity of the full-sample bootstrap. Peng and Qi (2007) applied a full-sample bootstrap method to obtain the confidence band for the tail dependence function in bivariate extremes. Both papers obtained the accuracy of the bootstrap method by using an approach proposed by Chen and Lo (1997). In Chen and Lo\u2019s approach, the key is to construct a statistic such that it is identically distributed as the statistic of interest and it is also very close to the bootstrapped statistic."
        },
        {
            "heading": "3 Empirical Likelihood Methods",
            "text": "Empirical likelihood was introduced by Owen (1988, 1990) for a mean vector for iid observations. It is a nonparametric method that allows one to employ likelihood methods to construct confidence intervals without assuming a parametric family for the data. It produces confidence regions whose shape and orientation are determined entirely by the data. Just like the parametric likelihood method, the method involves only a maximization procedure, and the estimation of any nuisance parameter is not necessary. Therefore, the empirical likelihood method possesses some advantages of the likelihood method.\nThere is a large and growing literature extending empirical likelihood methods to many statistical problems. For recent references and development we refer to the book by Owen (2001).\nThe empirical likelihood method was first adopted by Lu and Peng (2002) to construct confidence intervals for the tail index of a heavy-tailed distribution. Peng and Lu applied both the empirical likelihood method and the (pseudo) parametric likelihood method to obtain confidence intervals for the tail index of a heavy-tailed distribution. Later, Peng and Qi (2006a) proposed a new calibration method to overcome the undercoverage problem in constructing confidence intervals. Peng (2004) applied the empirical likelihood method to construct the confidence intervals for a mean from a heavy-tailed distribution. Peng and Qi (2006b) employed a data tilting method to construct the confidence intervals (or regions) for the high quantiles of a heavy-tailed distribution. The data tilting method is a general method and is more flexible compared with the empirical likelihood method. It has been demonstrated in Peng and Qi (2006a, b) that the empirical likelihood method and the data tilting method outperform the normal approximation method in that both the empirical likelihood method and the data tilting method result in confidence intervals with shorter average length and more accurate coverage probability.\nHere we assume that F is a heavy-tailed distribution satisfying the secondorder condition (2.3).\n3.1 Index of a Heavy-Tailed Distribution\nDefine Yi = i(log Xn,n\u2212i+1 \u2212 log Xn,n\u2212i) for i = 1, . . . , k. Then {Yi, 1 \u2264 i \u2264 k} are asymptotically independent with a common exponential limiting distribution with mean \u03b3 for fixed k (see e.g., Weissman 1978). Lu and Peng (2002) applied Owen\u2019s empirical likelihood method to construct confidence interval for the index \u03b3 . The procedure is as follows.\nLet p = (p1, \u00b7 \u00b7 \u00b7 , pk) be a probability vector satisfying \u2211ki=1 pi = 1 and pi \u2265 0 for i = 1, . . . , k. Then the empirical likelihood, evaluated at true value \u03b30 for the tail index \u03b3 , is defined by\nL(\u03b30) = sup { k\u220f\ni=1 pi :\nk\u2211 i=1 piYi = \u03b30\n}\n.\nThen, by the method of Lagrange multipliers, one can show that\npi = 1k {1 + \u03bb(Yi \u2212 \u03b30)} \u22121, i = 1, . . . , k,\nwhere \u03bb is the solution to the equation\n1\nk\nk\u2211\ni=1\nYi \u2212 \u03b30 1 + \u03bb(Yi \u2212 \u03b30) = 0. (3.1)\nNote that \u220fk i=1 pi, subject to \u2211k i=1 pi = 1, achieves its maximum k\u2212k at pi = k\u22121. Thus, the empirical likelihood ratio at \u03b30 is given by\nl(\u03b30) = k\u220f\ni=1 (kpi) =\nk\u220f i=1 {1 + \u03bb(Yi \u2212 \u03b30)}\u22121,\nand the corresponding empirical log-likelihood ratio statistic is defined as\nL(\u03b30) = \u22122 log l(\u03b30) = 2 k\u2211\ni=1 log{1 + \u03bb(Yi \u2212 \u03b30)},\nwhere \u03bb is the solution of Eq. 3.1.\nTheorem 3.1 (Lu and Peng 2002) Under condition Eq. 2.3, if\nk = kn \u2192 \u221e, kn \u2192 0 and \u221a\nkA(n/k) \u2192 0 as n \u2192 \u221e, then\nL(\u03b30) d\u2192\u03c721 ,\nwhere \u03c721 denotes a chi-squared random variable with one degree of freedom, and \u03b30 is the true value of the tail index \u03b3 .\nAccording to Theorem 3.1, a 100(1 \u2212 \u03b1)% confidence interval for \u03b3 based on the empirical likelihood ratio statistic is determined by\nIE(1 \u2212 \u03b1) = {\u03b3 > 0 : L(\u03b3 ) \u2264 c(\u03b1)}, where c(\u03b1) is the \u03b1 level critical value of a chi-squared distribution with one degree of freedom.\nFor a small sample size, the asymptotic \u03c72 calibrated empirical likelihoodbased confidence regions for iid observations may have a lower coverage probability than the nominal level, as indicated by numerical evidence in the literature; see, e.g., Owen (1988), Hall and La Scala (1990), and Qin and Lawless (1994). The reason for the undercoverage was explained in Tsao (2004), that is, the distribution of the empirical likelihood ratio has an atom at infinity, and the atom can be substantial if the sample size is not large. For the empirical likelihood-based confidence interval for the tail index the same problem occurs since only a few upper order statistics are employed in the inference. As a remedy for the empirical likelihood method, Peng and Qi (2006a) proposed the following new calibration method to improve the coverage accuracy.\nSince Yi/\u03b3 in L(\u03b3 ) has an approximate exponential distribution with mean one, we replace Y1/\u03b3, . . . , Yk/\u03b3 by i.i.d. random variables E1, . . . , Ek with an exponential distribution with mean one. Define\nELR(k) = 2 k\u2211\ni=1 log(1 + \u03bb\u2032(Ei \u2212 1)),\nwhere \u03bb\u2032 is the solution to k\u2211\ni=1\nEi \u2212 1 1 + \u03bb\u2032(Ei \u2212 1) = 0.\nThen approximate the distribution of L(\u03b3 ) by the distribution of ELR(k) instead of \u03c721 . Therefore, the following confidence interval with nominal level 100(1 \u2212 \u03b1)% can be defined:\nI\u2217E(1 \u2212 \u03b1) = {\u03b3 : L(\u03b3 ) \u2264 c(k, \u03b1)}, where c(k, \u03b1) is the upper \u03b1-level critical value of the distribution of ELR(k).\nNote that the exact distribution of ELR(k) is not available. The critical value c(k, \u03b1) can be estimated via Monte Carlo simulation. Peng and Qi (2006a) obtained the critical values c(k, \u03b1) for \u03b1 = 10%, 5% and 1% for all k between 10 and 200 based on 1 000 000 random samples. These critical values for 10 \u2264 k \u2264 29 were listed in Table 1 in Peng and Qi (2006a), and for 30 \u2264 k \u2264 200, three linear regression equations are fitted by\nc(k, 0.10) = 2.7055 \u2212 0.51269\u221a k + 18.14242 k ,\nc(k, 0.05) = 3.8415 \u2212 1.12486\u221a k + 32.90613 k\nand\nc(k, 0.01) = 6.6349 \u2212 4.56941\u221a k + 98.98899 k .\nThe three intercepts in the equations are the 10%, 5% and 1% critical values of the \u03c721 distribution.\nThe simulation study in Peng and Qi (2006a) indicates that this new calibration method greatly improves the accuracy of the coverage probability for empirical likelihood-based confidence intervals for small sample proportion k. Numerically it also demonstrates that the empirical likelihood method outperforms both the normal approximation (cf., de Haan and Peng 1998) and the gamma approximation (cf., Cheng and de Haan 2001) of Hill\u2019s estimators in terms of coverage probability and length of confidence intervals.\n3.2 High Quantile\nRecall that xp is called a high quantile if p = pn \u2208 (0, 1) is very small in the sense that npn \u2192 d \u2208 [0, \u221e). Peng and Qi (2006b) employed a data tilting method to construct a confidence interval for xp.\nTo introduce the methodology, we assume temporarily that 1 \u2212 F(x) = cx\u22121/\u03b3 for x > T, where T > 0 is a large number. Set \u03b4i = I(Xi > T). Then the likelihood function for the censored data {(\u03b4i, max(Xi, T))}ni=1 is ni=1(c\u03b3 \u22121 X\u22121/\u03b3\u22121i ) \u03b4i(1 \u2212 cT\u22121/\u03b3 )1\u2212\u03b4i . Below we take T = Xn,n\u2212k.\nFirst, for any fixed weights q = (q1, \u00b7 \u00b7 \u00b7 , qn) such that qi \u2265 0 and \u2211ni=1 qi = 1, define a weighted log-likelihood function by\nL(\u03b3, c) = n\u2211\ni=1 qi log\n(( c\u03b3 \u22121 X\u22121/\u03b3\u22121i )\u03b4i(1 \u2212 cX\u22121/\u03b3n,n\u2212k )1\u2212\u03b4i) .\nWe solve (\u03b3\u0302 (q), c\u0302(q)) = arg max(\u03b3,c) L(\u03b3, c) and obtain \u23a7 \u23aa\u23a8\n\u23aa\u23a9\n\u03b3\u0302 (q) = \u2211n i=1 qi\u03b4i(log Xi \u2212 log Xn,n\u2212k)\u2211n i=1 qi\u03b4i c\u0302(q) = X1/\u03b3\u0302 (q)n,n\u2212k \u2211 n i=1qi\u03b4i.\nDefine D(q) = \u2211ni=1 qi log(nqi), which is a measure of distance between q and the uniform distribution, i.e. qi = 1/n. Next, we shall choose q to minimize this distance. More specifically, solve (2n)\u22121L(xp) = minq D(q) subject to the constraints\nqi \u2265 0, n\u2211\ni=1 qi = 1, log xpXn,n\u2212k = \u03b3\u0302 (q) log \u2211n i=1 qi\u03b4i pn . (3.2)\nPut\nA1(\u03bb1) = 1 \u2212 n \u2212 kn e \u22121\u2212\u03bb1 and A2(\u03bb1) = A1(\u03bb1) log(xp/Xn,n\u2212k) log ( A1(\u03bb1)/pn ) .\nThen, by the standard method of Lagrange multipliers, we have\nqi = qi(\u03bb1, \u03bb2) =\n\u23a7 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n1 n e\u22121\u2212\u03bb1, if \u03b4i = 0,\n1 n exp\n{ \u2212 1 \u2212 \u03bb1 + \u03bb2 (\nlog(xp/Xn,n\u2212k) A2(\u03bb1) \u2212 1 A1(\u03bb1)\n\u2212 A1(\u03bb1) log(Xi/Xn,n\u2212k) log(xp/Xn,n\u2212k)) A22(\u03bb1)\n)}\n, if \u03b4i = 1,\nwhere \u03bb1 and \u03bb2 satisfy Eq. 3.2.\nTheorem 3.2 (Peng and Qi 2006b) Assume Eqs. 2.3 and 2.2 hold. If\u221a kA(n/k) \u2192 0, npn = O(k) and log ( k\nnpn\n) / \u221a\nk \u2192 0 as n \u2192 \u221e, then, with probability tending to one, there exists a solution to Eq. 3.2, and L(xp,0)\nd\u2192 \u03c721 , where xp,0 is the true value of xp.\nAccording to the above theorem, a confidence region at level 1 \u2212 \u03b1 for xp is given by\nIT(1 \u2212 \u03b1) = {xp : L(xp) \u2264 c(\u03b1)}, where c(\u03b1) is the \u03b1-level critical value of \u03c721 . This confidence interval has asymptotically correct coverage probability \u03b1, i.e., P(xp \u2208 IT(1 \u2212 \u03b1)) \u2192 \u03b1 as n \u2192 \u221e. Acknowledgements The author would like to express his gratitude to J\u00fcrg H\u00fcsler and Liang Peng for organizing this special session in honor of Professor Laurens de Haan and inviting me to present this review paper at the Fifth Conference on Extreme Value Analysis. The author also thanks Barry James for his careful reading of the manuscript and helpful comments."
        }
    ],
    "title": "Bootstrap and empirical likelihood methods in extremes",
    "year": 2008
}