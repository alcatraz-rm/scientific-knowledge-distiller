{
    "abstractText": "Estimators of the extreme-value index are based on a set of upper order statistics. We present an adaptive method to choose the number of order statistics involved in an optimal way, balancing variance and bias components. Recently this has been achieved for the similar but some what less involved case of regularly varying tails (Drees and Kaufmann, 1997); Danielsson et al., 1996). The present paper follows the line of proof of the last mentioned paper.",
    "authors": [
        {
            "affiliations": [],
            "name": "G. DRAISMA"
        },
        {
            "affiliations": [],
            "name": "L. DE HAAN"
        }
    ],
    "id": "SP:a431532ce527e0eb6e0bab8e20082071d12e44fa",
    "references": [
        {
            "authors": [
                "L. de Haan",
                "U. Stadtmu\u00c8ller"
            ],
            "title": "``Generalized regular variation of second order,'",
            "venue": "Annals of Probability,",
            "year": 1996
        },
        {
            "authors": [
                "Ann. Statis"
            ],
            "title": "Drees, H., ``On smooth statistical tail functionals,'",
            "venue": "Scandinavian Journal of Statistics 25(1),",
            "year": 1989
        },
        {
            "authors": [
                "W. Feller"
            ],
            "title": "An Introduction to Probability Theory and its Applications, Volume II Wiley, New York, 1966. Hall, P., ``On simple estimates of an exponent of regular variation,'",
            "venue": "Stock: Proc. Appl",
            "year": 1966
        },
        {
            "authors": [
                "III J. Pickands"
            ],
            "title": "``Statistical inference using extreme order statistics,'",
            "venue": "Ann. Statis",
            "year": 1975
        },
        {
            "authors": [
                "E. Willekens"
            ],
            "title": "``p-Variation with remainder,'",
            "venue": "Ann. Probab.,",
            "year": 1986
        },
        {
            "authors": [
                "V. Petrov"
            ],
            "title": "Sums of Independent Random Variables",
            "year": 1975
        }
    ],
    "sections": [
        {
            "text": "components. Recently this has been achieved for the similar but some what less involved case of regularly varying tails (Drees and Kaufmann, 1997); Danielsson et al., 1996). The present paper follows the line of proof of the last mentioned paper.\nKey words. moment estimator, Pickands estimator, bootstrap, mean squared error"
        },
        {
            "heading": "1. Introduction",
            "text": "Suppose we have i.i.d. observations X1;X2; . . . ;Xn whose common distribution function F is in the domain of attraction of an extreme-value distribution Gg (notation: F [D Gg ). The shape parameter g [R of this extreme-value distribution ( functional form: exp \u00ff 1 gx \u00ff 1=g ) can be estimated in various ways starting from the sample X1;X2; . . . ;Xn. Two popular estimators are Pickands' estimator (in its generalized form see e.g. Pereira, 1993):\ng\u0302n;y k : \u00ff log y \u00ff 1 log Xn;n\u00ff ky2 \u00ff Xn;n\u00ff ky\nXn;n\u00ff ky \u00ff Xn;n\u00ff k 1:1\ny [ 0; 1 where Xn;1 Xn;n are the order statistics of X1; . . . ;Xn and z denotes the largest integer which is not larger than z, and the moment estimator\ng\u0302n;2 k : M 1 n k 1\u00ff 1 2 1\u00ff M\n1 n k 2\nM 2 n k\n!\u00ff1 1:2\nwith\nM j n k : 1\nk Xk\u00ff 1 i 0 log Xn;n\u00ff i \u00ff log Xn;n\u00ff k j:\nFor this estimator we have to require that the right end point of the distribution is positive.\nThe estimators from (1.1) and (1.2) are consistent for g provided k k n ??, k n o n n?? . If one increases the speed at which k n goes to in\u00aenity, the asymptotic variance decreases but the asymptotic bias increases. There is an optimal sequence balancing variance and bias components (see Figure 1). This optimal sequence k0 n can be determined when the underlying distribution is known, provided the distribution function has a second order expansion involving an extra unknown parameter (Hall, 1982; Dekkers and de Haan, 1993, Pereira, 1993). Here we develop a purely sample based way of obtaining the optimal sequence k0 n where we assume a second order expansion but do not assume the second order (or \u00aerst order) characteristic known. The procedure is based on a double bootstrap (see also Hall, 1990). Results for the moment estimator and for Pickands' estimator are given in Section 3 and Section 4 respectively. All the proofs are postponed till Section 5. Section 6 reports the result of a small simulation study and Section 7 demonstrates the application of the procedure to North Sea wave height data. In an appendix we explain why we use different second order conditions in Section 3 and Section 4."
        },
        {
            "heading": "2. Outline",
            "text": "We want (in the set-up of (1.2)) the value of k minimizing EF g\u0302n;2 k \u00ff g 2 although this is only meant in an asymptotic sense (second moment of the asymptotic distribution). Call this value k0 n . There are two unknowns in this expression: g and the distribution function F. The idea is to replace g by a second estimator g\u0302n;3 k and to replace F by the empirical distribution function Fn. This amounts to bootstrapping. It is proved that minimizing the resulting expression, which can be calculated purely on the basis of the sample, still leads to the optimal k0 n with the help of a second bootstrap. A similar procedure applies to the estimator from (1.1). Sections 3 and 4 provide the scienti\u00aec background for the bootstrap procedure. Here we explain step by step how to implement the procedure.\nWe start with a sample X1; . . . ;Xn. Step 1: Select randomly and independently n1 times n15 5n a member of the set fX1;X2; . . . ;Xng, We indicate the result by X 1; . . . ;X n1. Form the order statistics X n1;1 X n1;n1 and compute g\u0302 n1;2 k and g\u0302 n1;3 k (according to the formula after Theorem 3.3 below) for k 1; 2; . . . ; n1. Form q n1;k g\u0302 n1;4 k 2 for k 1; 2; . . . ; n1:\nStep 2: Repeat this procedure r times independently. This results in a sequence q n1;k;s, k 1; 2; . . . ; n1 and s 1; 2; . . . ; r. Calculate 1r Pr s 1 q n1;k;s\n: The number r can be taken as big as necessary. Step 3: Minimize 1r Pr s 1 q n1;k;s\nwith respect to k. Denote by k 0;1 n1 the value of k where the minimum is obtained.\nStep 4: Repeat Step 1 up to 3 independently with the number n1 replaced by n2 n1 2=n. So n2 is smaller than n1. This results in k 0;1 n2 .\nStep 5: Calculate k\u03020 n on the basis of k 0;1 n1 and k 0;1 n2 according to its de\u00aenition in Corollary 3.3 below with g\u0302n : g\u0302n;2 n1=2 for example and r\u0302n according to the formula in the same Corollary.\nThis k\u03020 n is the adaptively obtained optimal number of order statistics."
        },
        {
            "heading": "3. Main results for moment estimator",
            "text": "We shall write throughout g for gV0 and g\u00ff for g60. Assume F [D Gg , i.e. there exists a positive function a t such that\nlim t?? U tx \u00ff U t a t\nxg \u00ff 1 g for x40;\nwhich implies that\nlim t?? log U tx \u00ff log U t a t =U t\nxg\u00ff \u00ff 1 g\u00ff for x40:\nThroughout this section we assume U ? 40 and the following second order condition:\nlim t??\nlog U tx \u00ff log U t a t =U t \u00ff x g\u00ff\u00ff1 g\u00ff\nA t H x 3:1\nwhere U t is the inverse function of the function 1= 1\u00ff F , a t is positive and A not changing sign eventually. The function H x is assumed not to be a multiple of xg\u00ff \u00ff 1 =g\u00ff and takes the form (supposing the function a and A are chosen properly)\nH x 1 r xr g\u00ff \u00ff 1 r g\u00ff \u00ff x g\u00ff \u00ff 1 g\u00ff\nlog x 2=2 if r 0; g 0, 1 g xg log x\u00ff x g\u00ff1 g if r 0; g50,\n1 r x r g\u00ff\u00ff1 r g\u00ff \u00ff xg\u00ff\u00ff1 g\u00ff if r 6 0, 8><>>: 3:2 depending on a second order parameter r 0 (see de Haan and Stadtmu\u00c8ller, 1996, relation (2.9) page 387).\nWe present a series of results culminating in Corollary 3.3 that provides a sample based\nsequence k\u03020 n such that for any (random or non-random) sequence k n\nlim sup n?? Ef g\u0302n;2 k\u03020 n \u00ff g 2g Ef g\u0302n;2 k n \u00ff g 2g 1\nFirst we restate in slightly greater generality a result from Dekkers and de Haan (1993) providing the optimal number of order statistics for the moment estimator as a function of g, r and the function A.\nTheorem 3.1: Suppose F [D Gg and that (3.1) and (3.2) hold for r50, g=r and g=0. Let k0 k0 n be a sequence of integers such that the asymptotic second moment of g\u0302n;2 k \u00ff g, notation as. Ef g\u0302n;2 \u00ff g 2g, is minimal when choosing k k0 n . Then\nk0 n , n V2 g b2 g;r 1 1\u00ff2r s\u00ff 1 n \u00ff 1( ) ?1 3:3\nas n??, where\nr r if g40; g if r5g50;\nr if g5r; 8<: V2 g g2 1 if g40, 1\u00ff g 2 1\u00ff 2g 6g2 \u00ff g 1\n1\u00ff 3g 1\u00ff 4g if g50\n( 3:4\n(the variance component) with g 0Vg and g\u00ff 06g and\nb g; r\n1r 1\u00ff r g 1 1\u00ffr 2 for g40; 1\n1\u00ff g for r5g50;\n1\u00ff g 1\u00ff 2g 1\u00ff r\u00ff g 1\u00ff r\u00ff 2g for g5r\n8>>><>>: 3:5\n(the bias component). The function s\u00ff is the inverse function of the decreasing function s satisfying\nA20 t 1 o 1 Z ?\nt\ns u du; 3:6\nwhere\nA0 t\nA t if g40, a t U t if r5g50 and\nA t if g5r.\n8>><>>:\nRemark 3.1: Since we only know anything about the asymptotic mean square error for intermediate k, here and in the rest of the paper, when we minimize over k, we only consider values between log n and n= log n , both being intermediate sequences with the optimal value in between.\nRemark 3.2: We exclude the two cases g 0 and g r in Theorem 3.1. The reason can be seen from Theorem A in the Appendix. This also happens in Dekkers and de Haan (1993).\nWe are going to turn the asymptotic second moment of g\u0302n;2 k \u00ff g into something we can handle adaptively, the \u00aerst step is to replace the unknown g in the formula by an alternative estimator for g. The alternative estimator is\ng\u0302n;3 k : M 2 n k =2\nq 1\u00ff 2\n3\n1\u00ffM\n1 n k M 2 n k\nM 3 n k\n\u00ff 1 :\nThe following theorem is the analogue of Theorem 3.1 for the aymptotic second moment of g\u0302n;2 k \u00ff g\u0302n;3 k .\nTheorem 3.2: Assume the conditions of Theorem 3.1. Determine k0 k0 n such that the asymptotic second moment of g\u0302n;2 k \u00ff g\u0302n;3 k is minimal. Then\nk0 n ,( n V2 g b2 g;r 1 1\u00ff2r s\u00ff 1 n \u00ff1) ?1\nas n??, where\nV2 g 1 4 g2 1 if g40,\n1 4 1\u00ffg 2 1\u00ff8g 48g2\u00ff154g3 263g4\u00ff222g5 72g6 1\u00ff2g 1\u00ff3g 1\u00ff4g 1\u00ff5g 1\u00ff6g if g50 8<: and\nb g; r\n\u00ff g 1\u00ffr r 2 1\u00ffr 3 if g40, 1\u00ff2g\u00ff 1\u00ffg 1\u00ff2g p 1\u00ffg 1\u00ff2g if r5g50,\n1 2 \u00ffr 1\u00ffg 2 1\u00ffg\u00ffr 1\u00ff2g\u00ffr 1\u00ff3g\u00ffr if g5r.\n8>>>><>>>:\nIn order to show the convergence of mean squared error, we consider the following\nquantity\ng\u0302n;4 k : g\u0302n;2 k \u00ff g\u0302n;3 k 1 jg\u0302n;2 k \u00ff g\u0302n;3 k j kd\u00ff1=2 ;\nwhere d40. Then we have\nTheorem 3.3: Assume the conditions of Theorem 3.1. Suppose r50. Determine k0;1 k0;1 n such that Ef g\u0302n;4 k 2g is minimal. Then as n??\nk0;1 n = k0 n ?1:\nHence\nk0;1 n n V2 g b2 g;r 1 1\u00ff2r s\u00ff 1 n\n\u00ff1( ) ?1: ,\nRemark 3.3: Note that Theorem 3.3 holds for any d40 in the de\u00aenition of g\u0302n;4 k . Thus, in our simulation study, we use g\u0302n;2 k \u00ff g\u0302n;3 k instead of g\u0302n;4 k .\nNext we are going to introduce the bootstrap procedure. One takes n1 independent drawings from the empirical distribution function of xn : fX1; . . . ;Xng. This results in observations X 1; . . . ;X\nn1 . We form the order statistics X n1;1 X n1;n1 and de\u00aene\nM j n1 k : 1\nk Xk i 1 log X n1;n1\u00ffi 1 \u00ff log X n1;n1\u00ffk1 j\nfor k5n1 and j 1; 2; 3: Next de\u00aene\ng\u0302 n1;2 k : M 1 n1 k 1\u00ff 1 2 1\u00ff M\n1 n1 k 2\nM 2 n1 k\n!\u00ff1\nand\ng\u0302 n1;3 k : M 2 n1 k =2\nq 1\u00ff 2\n3 1\u00ffM\n1 n1 k M 2 n1 k\nM 3 n1 k\n!\u00ff1 :\nBy bootstrapping we can now estimate\nQ n1; k : Ef g\u0302 n1;4 k 2jxng\nwith\ng\u0302 n;4 k : g\u0302 n;2 k \u00ff g\u0302 n;3 1 jg\u0302 n;2 k \u00ff g\u0302 n;3 k j kd\u00ff1=2\nas well as we wish.\nNow we want to connect the asymptotic behavior of arg inf Q with the corresponding quantity for the asymptotic expectation as considered in e.g. Theorem 3.1.\nTheorem 3.4: Suppose the conditions of Theorem 3.1 hold and n1 O n1\u00ffe for some 05e51=2. The random quantity k 0;1 n1 is de\u00aened as follows:\nk 0;1 n1 : arg inf k\nEf g\u0302 n1;4 k 2jxng:\nThen\nk 0;1 n1 , n1 V2 g b2 g; r 1 1\u00ff2r s\u00ff 1\nn1\n\u00ff1( ) ?1\nin probability.\nWe now use the known quantity k 0;1 to estimate k0 n and do this via k0 n .\nCorollary 3.1: Suppose the conditions of Theorem 3.4 hold and A0 t ctr with c 6 0 and r 50. Then\nk0 n , k 0;1 n1 n\nn1\n\u00ff2r 1\u00ff2r 8<: 9=;?1\nin probability.\nRemark 3.4: Since A0 in Theorem 3.1 is a regularly varying function, the extra requirement means that the slowly varying function is in fact a constant.\nNext we get rid of the factor n=n1 \u00ff2r 1\u00ff2r . We do this via an independent second bootstrap procedure with bootstrap sample size n2.\nTheorem 3.5: Suppose the conditions of Corollary 3.1 hold and n2 n1 2=n. Let\nk 0;1 n2 : arg inf k\nEf g\u0302 n2;4 k 2jxng:\nThen\nk0 n =f k 0;1 n1 2= k 0;1 n2 g?1 in probability:\nCorollary 3.2: Under the conditions of Theorem 3.5, k0 n ,\nk 0;1 n1 2 k 0;1 n2 V2 g b2 g; r V2 g b2 g; r 1 1\u00ff2r\n( ) ?1 in probability:\nCorollary 3.3: Suppose the conditions of Theorem 3.5 hold. De\u00aene\nk\u03020 n : k 0;1 n1 2 k 0;1 n2 V2 g\u0302n b2 g\u0302n; r\u0302n V2 g\u0302n b2 g\u0302n; r\u0302n 1 1\u00ff2r\u0302n\nwith k 0;1 n1 and k 0;1 n2 as de\u00aened in Theorem 3.4 and Theorem 3.5 respectively and with g\u0302n any consistent estimator of g ( for instance g\u0302n;2 k with k k n any sequence with k??; k=n?0), r\u0302n any consistent estimator for r , for instance\nr\u0302n : log k 0;1 n1\n\u00ff 2 log n1 2 log k 0;1 n1 :\nThen\nk\u03020 n =k0 n ?1 in probability;\nhence the asymptotic second moment of g\u0302n;2 k\u03020 n \u00ff g is asymptotically equal to the asymptotic second moment of g\u0302n;2 k0 n \u00ff g:"
        },
        {
            "heading": "4. Main results for Pickands' estimator",
            "text": "Throughout this section we assume that F is in the differentiable domain of attraction of Gg (notation: F [Ddif Gg ), i.e. F is differentiable in a left neighborhood of x? : supfx : F x 51g and there exist an40 and bn [R such that\nlim n?? q qx Fn anx bn G0g x 4:1\nlocally uniformly for all x [R. This is mainly done for convenience. In fact not much is lost of the general case and the computations are more simple. The differentiable domains of attraction were introduced by Pickands (1986). Clearly F [Ddif Gg implies F [D Gg for the same normalizing constants an and bn. De\u00aene U t : 1= 1\u00ff F \u00ff t . The following proposition characterizes the differentiable domain of attraction of Gg.\nProposition 1: F [Ddif Gg for some g [R if and only if U t is differentiable for all suf\u00aeciently large t and U0 t [RVg\u00ff1.\nProof: See Pickands (1986). &\nIn order to get the limit distribution function of estimator g\u0302n;y k we have to require some kind of second order condition. Because of Proposition 1 it is quite natural to assume that there is a positive function A t ?0 as t?? such that\nlim t??\nU0 tx U0 t \u00ff xg\u00ff1\nA t\nexists for every x40. In order to avoid trivialities we also assume that the limit function is not a multiple of xg\u00ff1. Then the limit function must be of the form c0xg\u00ff1 x\nr\u00ff1 r for constants\nr 0 and c0 6 0 (see Theorem 1.9 of Geluk and de Haan (1987) or Lemma 3.2.1 of Bingham et al. (1987); x0 \u00ff 1 =0 is de\u00aened as log x). We can and will subsume the constant c0 in the function A*. So suppose there is a function A with limt?? A t 0 and not changing sigh near in\u00aenity, such that\nlim t??\nU0 tx U0 t \u00ff xg\u00ff1\nA t x g\u00ff1 x r \u00ff 1 r\n4:2\nfor all x40. The function jAj is then regularly varying with index r notation : jAj [RVr . It can be proved (see Pereira, 1993, or de Haan and Stadtmu\u00c8ller (1996)) that (4.2) is equivalent to\nlim t?? U tx \u00ff U t \u00ff tU0 t xg\u00ff1g tU0 t A t hg;r x : 1 r xg r \u00ff 1 g r \u00ff xg \u00ff 1 g : 4:3\nFirst we determine the theoretically optimal value k0 n asymptotically.\nTheorem 4.1: Assume F [Ddif Gg and (4.3) holds for A t ctr with c 6 0 and r50. Determine k0 k0 n such that the asymptotic second moment of g\u0302n;y k \u00ff g is minimal. Then\nk0 n\n, y\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1\n\u00ff 2rc2 1\u00ffyrr 2 y\u00ffg\u00ffr\u00ff1 g r 2 y\u00ff2r\n0B@ 1CA 1 1\u00ff2r n \u00ff2r 1\u00ff2r 8><>: 9>=>;?1\nas n??:\nNext we compute the optimum with g replaced by g\u0302n;y ky2 :\nTheorem 4.2: Assume F [Ddif Gg and (4.3) holds for A t ctr with c 6 0 and r50. Determine k0 k0 n such that the asymptotic second moment of g\u0302n;y k \u00ff g\u0302n;y ky2 is minimal. Then\nk0 n\n, y\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1 1 y\u00ff2\n\u00ff 2rc2 1\u00ffyrr 2 y\u00ffg\u00ffr\u00ff1 g r 2 y\u00ff2r 1\u00ff y\u00ff2r 2\n0B@ 1CA 1 1\u00ff2r n \u00ff2r 1\u00ff2r 8><>>: 9>=>>;?1\nas n??:\nCorollary 4.1: Assume F [Ddif Gg and (4.3) holds for A t ctr with c 6 0 and r50. Determine k0 n such that the asymptotic second moment of g\u0302n;y k \u00ff g is minimal and k0 n such that the asymptotic second moment of g\u0302n;y k \u00ff g\u0302n;y ky2 is minimal. Then\nk0 n k0 n ? 1 y\u00ff2 1\u00ff y\u00ff2r 2 ! 1 1\u00ff2r\nas n??:\nIn order to show the convergence of mean squared error, we consider the following quantity\ngn;y k : g\u0302n;y k \u00ff g\u0302n;y2 ky2 1 jg\u0302n;y k \u00ff g\u0302n;y2 ky2 j kd\u00ff1=2 ;\nwhere d40. Then we have\nTheorem 4.3: Assume F [Ddif Gg and (4.3) holds for A t ctr with c 6 0 and r50. Determine k0;1 k0;1 n such that Ef gn;y k 2g is minimal. Then as n??\nk0;1 n = k0 n ?1:\nAs in Section 3, we draw resamples x n1 fX 1; . . . ;X n1g from xn fX1; . . . ;Xng with replacement. Let n15n and X\nn1;1 X n1;n1 denote the order statistics of x n1 and\nde\u00aene\ng\u0302 n1;y k1 : \u00ff log y \u00ff1\nlog X\nn1;n1\u00ff k1y2 \u00ff X n1;n1\u00ff k1y X n1;n1\u00ff k1y \u00ff X n1;n1\u00ffk1 :\nThen we propose to use the following bootstrap estimate of the mean square error\nEf g n1;y k1 2jxng:\nWe can prove\nTheorem 4.4: Assume F [Ddif Gg and (4.3) holds for A t ctr with c 6 0 and r50. Let n1 O n1\u00ffe for some e [ 0; 1 . Determine k 1;0 n1 such that Ef g n1;y k\n2jxng is minimal. Then\nk 1;0 n1\n, y\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1 1 y\u00ff2\n\u00ff 2rc2 1\u00ffyr r 2 y\u00ffg\u00ffr\u00ff1 g r 2 y\u00ff2r 1\u00ff y\u00ff2r 2\n0B@ 1CA 1 1\u00ff2r n \u00ff2r 1\u00ff2r 1 8>><>: 9>>=>;? p 1\nas n??:\nFinally we connect k0 n with k 1;0 and k 2;0 asymptotically.\nTheorem 4.5: Assume F [Ddif Gg and (4.3) holds for A t ctr r50 . Let n1 O n1\u00ffe for some e [ 0; 1=2 and n2 n1 2=n. Determine k i;0 ni such that Ef g ni;y ki 2jxng is minimal i 1; 2 . De\u00aene\nfy r 1 y\u00ff2 1\u00ff y\u00ff2r 2\n! 1 1\u00ff2r\n:\nThen\nk 1;0 2\nk 2;0 fy log k 1;0\n2 log k 1;0 \u00fflog n1 ,k0 n ?p 1 as n??:\nSo as before we get an estimator for k0 n which leads to an estimator for y which has asymptotically the lowest mean squared error."
        },
        {
            "heading": "5. Proofs",
            "text": "We shall give some lemmas \u00aerst.\nLemma 5.1: Let Y1; . . . ; Yn be i.i.d. random variables with common distribution function 1\u00ff x\u00ff1 x41 and Yn;1 Yn;n be the order statistics. Assume k??, k=n?0. Then\n(i) Yn;n\u00ffk= nk ?1 in probability\n(ii) De\u00aene\nPn : 1k Pk i 1 Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff\u00ff1 g\u00ff \u00ff 1 1\u00ffg\u00ff Qn : 1k Pk i 1 Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff\u00ff1 g\u00ff 2 \u00ff 2 1\u00ffg\u00ff 1\u00ff2g\u00ff Rn : 1k Pk i 1 Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff\u00ff1 g\u00ff 3 \u00ff 6 1\u00ffg\u00ff 1\u00ff2g\u00ff 1\u00ff3g\u00ff : 8>>>>>><>>>>>: We have k p Pn;Qn;Rn converges in distribution to P;Q;R , say, which is normally distributed with mean vector zero and covariance matrix\nE P2 1 1\u00ffg\u00ff 2 1\u00ff2g\u00ff E Q2 4 5\u00ff11g\u00ff 1\u00ffg\u00ff 2 1\u00ff2g\u00ff 2 1\u00ff3g\u00ff 1\u00ff4g\u00ff E R2 36 19\u00ff105g\u00ff 146g2\u00ff 1\u00ffg\u00ff 2 1\u00ff2g\u00ff 2 1\u00ff3g\u00ff 2 1\u00ff4g\u00ff 1\u00ff5g\u00ff 1\u00ff6g\u00ff E PQ 4 1\u00ffg\u00ff 2 1\u00ff2g\u00ff 1\u00ff3g\u00ff E PR 18 1\u00ffg\u00ff 2 1\u00ff2g\u00ff 1\u00ff3g\u00ff 1\u00ff4g\u00ff E QR 12 9\u00ff21g\u00ff 1\u00ffg\u00ff 2 1\u00ff2g\u00ff 2 1\u00ff3g\u00ff 1\u00ff4g\u00ff 1\u00ff5g\u00ff : 8>>>>>>>>><>>>>>>>>>: Moreover,\nk E P2n?E P 2\nk E Q2n?E Q 2\nk E R2n?E R 2: 8>><>: (iii) De\u00aene for j 1; 2; 3\nd j n : 1\nk Xk i 1 j H Yn;n\u00ffi 1=Yn;n\u00ffk Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff \u00ff 1 g\u00ff j\u00ff1\nThen by the law of large numbers\nd j n ? p dj Z ?\n1 j H y y g\u00ff \u00ff 1 g\u00ff\nj\u00ff1 dy\ny2 ; j 1; 2; 3\nor explicitly\nd1 1\n1\u00ff g\u00ff 1\u00ff r\u00ff g\u00ff ;\nd2 2 3\u00ff 2r\u00ff 4g\u00ff\n1\u00ff g\u00ff 1\u00ff 2g\u00ff 1\u00ff r\u00ff g\u00ff 1\u00ff r\u00ff 2g\u00ff\nand\nd3 6 18g2\u00ff \u00ff 22g\u00ff 15r g\u00ff 3r2 \u00ff 8r 6\n1\u00ff g\u00ff 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff 1\u00ff r\u00ff g\u00ff 1\u00ff r\u00ff 2g\u00ff 1\u00ff r\u00ff 3g\u00ff :\nProof: Similar to the proof of Theorem 3.4 of Dekkers et al. (1989) by writing Pn;Qn;Rn as a sum of i.i.d. random vectors. &\nThe following is an extension of a result by Drees (1998).\nLemma 5.2: Let f be a measurable function. Suppose there exist a real parameter a and functions a1 t 40 and A1 t ?0 such that for all x40\nlim t??\nf tx \u00fff t a1 t \u00ff x a\u00ff1 a\nA1 t H1 x\nwhere\nH1 x 1 b xa b \u00ff 1 a b \u00ff xa \u00ff 1 a\nb 0 :\nThen for any e40 there exists t040 such that for all t t0, tx t0,\nf tx \u00fff t a1 t \u00ff x a\u00ff1 a\nA1 t \u00ff H1 x\ne 1 xa 2xa beej log xj :\nProof: Suppose a 6 0. Then from relation (2.2) of Theorem 1 of de Haan and Stadmu\u00c8ller (1996), we have\ntx \u00ffaa1 tx \u00ff t\u00ffaa1 t t\u00ffaa1 t A1 t ? xb \u00ff 1 b :\nHence\nf tx \u00ff a1 tx =a\u00ff f t \u00ff a1 t =a a1 t A1 t =a\nf tx \u00ff f t \u00ff a1 t xa\u00ff1 a\na1 t A1 t =a \u00ff xa tx\n\u00ffaa1 tx \u00ff t\u00ffaa1 t t\u00ffaa1 t A1 t\n?aH1 x \u00ff xa xb\u00ff1b \u00ff x a b\u00ff1 a b :\nSimilar to the proof of Lemma 2.2 of de Haan and Peng (1997), we get xa tx \u00ffaa1 tx \u00ff t\u00ffaa1 t t\u00ffaa1 t A1 t \u00ff xa x b \u00ff 1 b\nxae 1 xbeej log xj\nand f tx \u00ff a1 tx =a\u00ff f t \u00ff a1 t =a a1 t A1 t =a x a b \u00ff 1 a b\ne 1 xa beej log xj :\nHence f tx \u00fff t\na1 t \u00ff x a\u00ff1 a\nA1 t \u00ff H1 x\nf tx \u00ff a1 tx =a\u00ff f t \u00ff a1 t =a a1 t A1 t x a b \u00ff 1 a a b\nxa tx \u00ffaa1 tx =a\u00ff t\u00ffaa1 t =a\nt\u00ffaa1 t A1 t \u00ff xa x b \u00ff 1 ab\nejaj 1 x a 2xa beej log xj :\nSuppose a 0 and b50. Then from the proof of Theorem 2 (iii) of de Haan and Stadtmu\u00c8ller (1996) we have a1 t ?c0 [ 0;? and c0\u00ffa1 t a1 t A1 t ?\u00ff 1=b. Hence\nf tx \u00ff c0 log tx \u00ff f t \u00ff c0 log t a1 t A t ? 1 b xb \u00ff 1 b :\nThe rest of proof for a 0 and b50 is similar to the case a 6 0. Suppose a b 0. Write\ng t : f t \u00ff 1 t Z t 0 f s ds\nwhich implies\nf t g t Z t\n0\ng s s ds\n(see Corollary 1.2.1 of de Haan, 1970). From Omey and Willekens (1988) we have\ng tx \u00ff g t a1 t A1 t ? log x:\nNote that\nf tx \u00ff f t \u00ff a1 t log x a1 t A1 t\ng tx \u00ff g t a1 t A1 t\nZ x\n1 g ts \u00ff a1 t sa1 t A1 t ds:\nHence\ng tx \u00ff a1 t a1 t A1 t ? log x\u00ff 1:\nFurthermore\ng t \u00ff a1 t a1 t A1 t ?\u00ff 1:\nUsing Proposition 1.19.4 of Geluk and de Haan (1987), we can easily see the lemma holds. Thus we complete the proof. &\nLet Fn denote the empirical distribution function of xn and Un 11\u00ffFn \u00ff .\nLemma 5.3: If (3.1) and (3.2) hold and n1 O n1\u00ffe0 for some e0 [ 0; 1 . Then for any 05e51 there exists t040 such that for all t0 t n1 log n1 2 and t0 tx n1 log n1 2\nlog Un tx \u00fflog Un t a t =U t \u00ff x g\u00ff\u00ff1 g\u00ff\nA t \u00ff H x\ntx p\nlog n n p e d g\u00ff; r xreej log xj\nt p\nlog n n p e d g\u00ff; r\ne 1 xg\u00ff 2xg\u00ff reej log xj\nd g\u00ff; r jA t j log n n p txp tp a:s: 5:1\nwhere d g\u00ff; r 40 is a constant which only depends on g\u00ff and r.\nProof: Let Gn denote the empirical distribution function of n independent, uniformly distributed random variables. As n is large enough and n1 O n1\u00ffe0 , we have\n1=2 sup t n1 log n1 2\njtG\u00ffn 1\nt\nj 2 a.s. 5:2\nand\nsup t 2\nt p Gn 1\nt\n\u00ff 1\nt log n np a.s. (see equations (10) and (17) of Chapter 10.5 of Shorack and Wellner (1986)). Hence\nsup 4 t n1 log n1 2\n1\nG\u00ffn 1t\ns Gn G \u00ff n 1\nt\n\u00ff G\u00ffn\n1\nt log n np a.s. Therefore for all 4 t n1 log n1 2\ntG\u00ffn 1\nt\n\u00ff 1\n2 tp log n np a.s. 5:3\nNow we use Lemma 5.2, (5.2), (5.3),\njyg \u00ff 1j jgj 2g\u00ff1V2\u00ffg 1 jy\u00ff 1j for 1=2 y 2 and\nUn t d U\nt\ntG\u00ffn 1t :\nIt follows that for any e [ 0; 1 there exists t044 such that for all t0 t n1 log n1 2 and t0 tx n1 log n1 2\nlog Un tx \u00fflog Un t a t =U t \u00ff x g\u00ff\u00ff1 g\u00ff\nA t \u00ff H x\nd log U tx txG\u00ffn 1tx\n\u00ff log U tx \u00ff a tx U tx txG\u00ffn 1tx \u00ffg\u00ff\u00ff1\ng\u00ff A tx a tx =U tx A tx a tx =U tx A t a t =U t\n\u00ff log U t tG\u00ffn 1t\n\u00ff log U t \u00ff a t U t tG\u00ffn 1t \u00ffg\u00ff\u00ff1\ng\u00ff\nA t a t =U t\nlog U tx \u00ff log U t \u00ff a t U t x g\u00ff\u00ff1 g\u00ff\nA t a t =U t \u00ff H x\na tx U tx txG\u00ffn 1tx \u00ffg\u00ff\u00ff1 g\u00ff\nA t a t =U t \u00ff tG\u00ffn 1t \u00ffg\u00ff \u00ff 1\ng\u00ffA t ( H 1txG\u00ffn 1tx ! e \" 1 txG\u00ffn 1 tx !\u00ffg\u00ff\n2 txG\u00ffn\n1\ntx\n!\u00ffg\u00ff\u00ffr eej log txG \u00ff n 1tx j #) 1 e xreej log xj\nH 1tG\u00ffn 1t ! e 1\ntG\u00ffn\n1\nt !\u00ffg\u00ff 2 tG\u00ffn 1\nt\n\u00ffg\u00ff\u00ffr eej log \u00ff tG\u00ffn \u00ff 1 t\ne 1 xg\u00ff 2xg\u00ff reej log xj\n1 e txG \u00ff n 1tx \u00ffg\u00ff \u00ff 1 g\u00ffA t\ntG\u00ffn 1t \u00ffg\u00ff \u00ff 1g\u00ffA t a.s.\nd1 g\u00ff; r tx p\nlog n n p ed2 g\u00ff; r xreejlog xj\nd1 g\u00ff; r t p\nlog n n p ed2 g\u00ff; r\ne 1 xg\u00ff 2xg\u00ff reej log xj d3 g\u00ff; r jA t j tx p log n n p d3 g\u00ff; r jA t j t p log n n p a.s.\nwhere dj g\u00ff; r 40; j 1; 2; 3 are constants only depending on g\u00ff and r. The lemma follows. &\nProof of Theorem 3.1: A full proof of a somewhat restricted case has been given in Dekkers and de Haan (1993). We shall give a sketch of the proof.\nBy Lemma 5.2, for any e40 there exists t040 such that for all t t0, tx t0\nlog U tx \u00fflog U t a t =U t \u00ff x g\u00ff\u00ff1 g\u00ff\nA t \u00ff H x e 1 xg\u00ff 2xg\u00ff reej log xj :\nApplying this relation with t replaced by Yn;n\u00ffk and x by Yn;n\u00ffi=Yn;n\u00ffk, adding the inequalities for i 0; 1; . . . ; k \u00ff 1 and dividing by k we get\nM 1 n k\na Yn;n\u00ffk =U Yn;n\u00ffk 1\n1\u00ff g\u00ff Pn A Yn;n\u00ffk\n1\nk Xk i 1 H Yn;n\u00ffi 1=Yn;n\u00ffk\neA Yn;n\u00ffk 1\nk Xk i 1 f1 Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff\n2 Yn;n\u00ffi 1=Yn;n\u00ffk g\u00ff reej log Yn;n\u00ffi 1=Yn;n\u00ffk jg:\nNote that fYn;n\u00ffi 1=Yn;n\u00ffkgki 1 d fY0igki 1 with Y 01; . . . ; Y0k i.i.d. with common distribution function 1\u00ff 1=x x41 . We apply the law of large numbers to the third and fourth terms. Also note that kn Yn;n\u00ffk?1 in probability, so that since jAj is regularly varying, we have A n=k \u00ff1A Yn;n\u00ffk ?1 in probability. As a result\nM 1 n k\na Yn;n\u00ffk =U Yn;n\u00ffk 1 1\u00ff g\u00ff Pn A n=k d1 op A n=k :\nHence\nM 1 n k 2 a2 Yn;n\u00ffk =U2 Yn;n\u00ffk\n1 1\u00ff g\u00ff 2 2Pn 1\u00ff g\u00ff 2A n=k d1 1\u00ff g\u00ff op A n=k :\nSimilarly\nM 2 n k\na2 Yn;n\u00ffk =U2 Yn;n\u00ffk\n2 1\u00ff g\u00ff 1\u00ff 2g\u00ff Qn A n=k d2 op A n=k :\nCombining these expansions we get\ng\u0302n;2 k \u00ff g\nM 1 n k \u00ff g M 2 n k \u00ff 2 M 1 n k 2\n2M 2 n k \u00ff 2 M 1 n k 2\n\u00ff g\u00ff\ng\na Yn;n\u00ffk U Yn;n\u00ffk \u00ff g\n1\n1\u00ff g\u00ff Pn d1A n=k\n\u00ff g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 2\nf 1\u00ff 2g\u00ff Qn \u00ff 4Pn d2 \u00ff 2g\u00ffd2 \u00ff 4d1 A n=k g op A n=k\na Yn;n\u00ffk U Yn;n\u00ffk\n\u00ff g ! 1\n1\u00ff g\u00ff a Yn;n\u00ffk U Yn;n\u00ffk \u00ff g\n! Pn d1A n=k\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 2\n2g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff \u00ff 4 Pn 1\u00ff 2g\u00ff Qn\nd2 \u00ff 2g\u00ffd2 \u00ff 4d1 2g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff d1\n! A n=k\nop A n=k :\nFrom the proof of Lemma 5.2 and Theorem 2, part (iii) of de Haan and Stadtmu\u00c8ller (1996) we can prove\na t U t \u00ff g\nA t\na t U t \u00ffg A t ? g r if g40\na t U t =jA t j?? if r5g 0 a t U t =A t ?0 if g5r. 8>>><>>: Consequently, by Lemma 5.1, we have that the asymptotic second moment of g\u0302n;2 k \u00ff g equals\nV2 g =k b2 g; r A20 n=k V2 g r=n b2 g; r A20 r with r : n=k. One obtains the minimum with respect to r by using (3.6) and equating the derivative to zero ( for details see Dekkers and de Haan, 1993). Since we have assumed in the derivation that k n is an intermediate sequence, we still have to show that the resulting k0 n is really the optimum. But it is easy to see that for any k n with k n =k0 n ?0 or ? the asymptotic second moment of g\u0302n;2 k \u00ff g is of large order as long as k n ?? and k n =n?0 (n??). In order to stay within these bounds, one can add the extra restriction log n k n n= log n in the optimization procedure. The theorem follows. &\nProof of Theorem 3.2: First we develop an asymptotic expansion for the alternative estimator g\u0302n;3 k . By the same arguments as in the proof of Theorem 3.1 we may show\nM 3 n k\na3 Yn;n\u00ffk =U3 Yn;n\u00ffk 6 1\u00ff g\u00ff 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff\nRn d3A n=k op A n=k\nand\nM 1 n k M 2 n k\na3 Yn;n\u00ffk =U3 Yn;n\u00ffk 2 1\u00ff g\u00ff 2 1\u00ff 2g\u00ff\n2Pn 1\u00ff g\u00ff 1\u00ff 2g\u00ff Qn\n1\u00ff g\u00ff 2d1 1\u00ff g\u00ff 1\u00ff 2g\u00ff d2 1\u00ff g\u00ff A n=k op A n=k :\nHence\ng\u0302n;3 k \u00ff g\nM 2 n k =2 q \u00ff g\nM 3 n k \u00ff 3M 1 n k M 2 n k\n3M 3 n k \u00ff 3M 1 n k M 2 n k\n\u00ff g\u00ff\na Yn;n\u00ffk U Yn;n\u00ffk\n1 1\u00ff g\u00ff 1\u00ff 2g\u00ff Qn 2 d2 2 A n=k\ns \u00ff g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff\n12\n\u00ff 6\n1\u00ff 2g\u00ff Pn \u00ff 3Qn 1\u00ff 3g\u00ff Rn\n1\u00ff 3g\u00ff d3 \u00ff\n6\n1\u00ff 2g\u00ff d1 \u00ff 3d2 A n=k\nop A n=k\ng\na Yn;n\u00ffk U Yn;n\u00ffk \u00ff g\n1 1\u00ff g\u00ff 1\u00ff 2g\u00ff p 1\u00ff g\u00ff 1\u00ff 2g\u00ff p 4 Qn\n1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n4 d2A n=k\n\u00ff g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff\n12\n\u00ff 6\n1\u00ff 2g\u00ff Pn \u00ff 3Qn 1\u00ff 3g\u00ff Rn\n1\u00ff 3g\u00ff d3 \u00ff\n6\n1\u00ff 2g\u00ff d1 \u00ff 3d2 A n=k\nop A n=k 10\na Yn;n\u00ffk U Yn;n\u00ffk\n\u00ff g !\n1 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\na Yn;n\u00ffk U Yn;n\u00ffk\n\u00ff g 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n4 Qn 1\u00ff g\u00ff 1\u00ff 2g\u00ff p 4 d2A n=k \" #\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff\n12\n\u00ff 6\n1\u00ff 2g\u00ff Pn 3g 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff \u00ff 3\n! Qn\n1\u00ff 3g\u00ff Rn 1\u00ff 3g\u00ff d3 \u00ff\n6\n1\u00ff 2g\u00ff d1 \u00ff 3d2\n3g 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff d2 A n=k\nop A n=k :\nCombining the above expansion with the expansion of g\u0302n;2 k \u00ff g in the proof of Theorem 3.1, we have\ng\u0302n;2 k \u00ff g\u0302n;3 k\na Yn;n\u00ffk U Yn;n\u00ffk\n\u00ff g !\n1 1\u00ff g\u00ff \u00ff 1 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n!\na Yn;n\u00ffk U Yn;n\u00ffk\n\u00ff g ! Pn d1A n=k \u00ff 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n4 Qn\n\u00ff 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n4 d2A n=k\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff 12\n6\n( 12g\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff \u00ff 24 1\u00ff 3g\u00ff 6 1\u00ff 2g\u00ff\n\" # Pn\n6 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff\n\u00ff 3g 1\u00ff g\u00ff 1\u00ff 2g\u00ff p\n1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff 3\n\" # Qn \u00ff 1\u00ff 3g\u00ff Rn\n6 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff d2 \u00ff 24 1\u00ff 3g\u00ff d1\n12g 1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff d1 \u00ff 1\u00ff 3g\u00ff d3\n6 1\u00ff 2g\u00ff d1 3d2 \u00ff 3g 1\u00ff g\u00ff 1\u00ff 2g\u00ff p 1\u00ff g\u00ff 2 1\u00ff 2g\u00ff 1\u00ff 3g\u00ff d2 A n=k ) op A n=k :\nThe rest of proof is similar to the proof of Theorem 3.1. &\nProof of Theorem 3.3: Let\nE 1 n : o : all of jPnj; kn Yn;n\u00ffk \u00ff 1 ; jd 1 n \u00ff d1j and\njD 1 n \u00ff D 1 j are less than or equal to kd0\u00ff1=2\nfor some d0 [ 0; 1=2 , where\nD 1 n : e 1\nk Xk\u00ff1 i 0 1 Yn;n\u00ffi=Yn;n\u00ffk g\u00ff 2 Yn;n\u00ffi=Yn;n\u00ffk g\u00ff r e\nand\nD 1 : e Z ?\n1\n1 xg\u00ff xg\u00ff r e x\u00ff2dx:\nNow take e and t0 as in the proof of Theorem 3.1. Then, provided n=k 1\u00ff kd0\u00ff1=2 t0, we have Yn;n\u00ffk t0 on E 1 n . Also, since A is regularly varying we have\njA Yn;n\u00ffk \u00ff A n=k j52eA n=k\non E 1 n . Using these two facts and the inequalities in the beginning of the proof of Theorem 3.1, we \u00aend\nM 1 n k a Yn;n\u00ffk =U Yn;n\u00ffk \u00ff 1 1\u00ff g\u00ff \u00ff Pn \u00ff A n=k d1 5eA n=k\non the set E 1 n : so we have o A n=k instead of op A n=k . De\u00aening sets E 2 n and E 3 n related to the behavior of M 2 n and M 3 n we get similar inequalities for those.\nDe\u00aene\nEn : E 1 n \\ E 2 n \\ E 3 n :\nUsing the mentioned inequalities and the fact that the conditions for the set En imply that Pn, Qn, Rn and A are surely small, we can also replace op A n=k by o A n=k in the expansions given for g\u0302n;2 k and g\u0302n;3 k in the proof of Theorems 3.1 and 3.2 as long as we stay inside En.\nMoreover the inequality j kn Yn;n\u00ffk \u00ff 1j kd0\u00ff1=2 guarantees that we can replace\na Yn;n\u00ffk U Yn;n\u00ffk \u00ff g by a n=k U n=k \u00ff g\n(cf. the limit relation for \u00ffg a t =U t in the proof of Theorem 3.1). Hence as in the proofs of Theorems 3.1 and 3.2 we \u00aend\nEfg\u03022n;41 En g V2 g =k b2 g; r A20 n=k ?1:\nNext we show that the contribution of the set Ecn to the expectation may be neglected. For example by the de\u00aenition of g\u0302n;4\nEfg\u03022n;41 jPnj4kd0\u00ff1=2 g k2d\u00ff1 PrfjPnj4kd0\u00ff1=2g\nand by Bennett's inequality (cf. Petrov, 1975, Ch. III, section 5) we can show\nPrfjPnj4kd0\u00ff1=2g k\u00ffb\neventually for any b40. Hence\nEfg\u03022n;41 jPnj4kd0\u00ff1=2 g V2 g =k b2 g; r A20 n=k ?0; n??:\nThe reasoning in case any of the other conditions of the set En is violated is exactly the same (the inequality Prfj kn Yn;n\u00ffk \u00ff 1j4kd0\u00ff1=2g k\u00ffb can be obtained by translating the inequality for kn Yn;n\u00ffk into one for its inverse 1 k Pn i 1 1fYi4 nk xg and then applying Bennets inequality). This completes the proof of Theorem 3.3. &\nProof of Theorem 3.4: Given xn : fX1; . . . ;Xng, we have\nM 1 n1 k1 d 1\nk1 Xk1 i 1 log Un Yn1;n1\u00ffi 1 \u00ff log Un Yn1;n1\u00ffk1\nwith fYn1;ig n1 i 1 the order statistics from a distribution function 1\u00ff 1=x x41 and independent of xn. By the same arguments as in the proof of Theorem 3.1 using Lemma 5.3 instead of Lemma 5.2 we get\nM 1 n1 k1\na Yn1;n1\u00ffk =U Yn1;n1\u00ffk1\n1 1\u00ff g\u00ff Pn1 A n1=k1 d1 op A n1=k1 Op\nn1=k1 p\nlog n n p :\nNote that\nn1=k1 p\nlog n n p o 1= k1p , so that the last term can be absorbed into the second one.\nThe expansion for M 1 n1 k1 is the same as for M 1 n1 k1 given xn. Similarly for M 2 n1 k1 and M 3 k1 n1 . So the result of Theorem 3.1 holds with k0 replaced by\nk 0;1 n1 : arg inf k\nas. Ef g\u0302 n1;2 k \u00ff g 2jxng\nand n by n1. A similar analogue holds to the results of Theorem 3.2. Finally in a way analoguous to what was done in Theorem 3.3 we can replace as. E by the non-asymptotic expectation. Hence the conclusion. &\nProof of Corollary 3.1: Note that limt?? t \u00ffga t =U t is a positive constant in the case r5g50 (see de Haan and Stadtmu\u00c8ller (1996)). Thus A0 t *c0tr which implies\ns\u00ff 1=t * \u00ff 2c20r 1 1\u00ff2r t 1 1\u00ff2r :\nThe Corollary easily follows from Theorems 3.4 and 3.5. &\nProof of Theorem 3.5: This follows by combining the results of Corollary 3.1 for k 0;1 n1 and k 0;1 n2 . &\nProof of Corollary 3.2: It easily follows from Theorems 3.1, 3.2 and 3.5. &\nProof of Corollary 3.3: We can use the result of Corollary 3.2 and we only have to prove that r\u0302n is a consistent estimator of r\n. By Theorem 3.4 the sequence k 0;1 n1 is asymptotic to\nc1n \u00ff2r 1\u00ff2r 1 :\nHence\nlog k 0;1 n1 = log n1? \u00ff2r\n1\u00ff 2r\nin probability. This gives the consistency. &\nLemma 5.4: If F [Ddif Gg , then (4.1) holds for an nU0 n and bn U n and for any k??, k=n?0 and y [ 0; 1 , the stochastic process\nWn;k y : k p Xn;n\u00ff ky \u00ff U nky\nn k U 0 nk\nconverges (in the sense of convergence of all \u00aenite marginal distributions) to a Gaussian process w y which has mean zero and covariance structure\nCov w y1 ;w y2 y\u00ffg1 y\u00ffg\u00ff12 ; 05y1 y2 1:\nProof: See Theorem 2.3 of Cooil (1985). &\nLemma 5.5: If (4.3) holds and n1 O n1\u00ffe0 for some e0 [ 0; 1 . Then for any 05e51 there exists t040 such that for all t0 t n1 log n1 2 and t0 tx n1 log n1 2\nUn tx \u00ffUn t a t \u00ff x g\u00ff1 g\nA t \u00ff hg;r x\ntx p\nlog n n p e D g; r xg reej log xj\nt p\nlog n n p e D g; r\ne 1 xg 2xg reej log xj D g; r jA t j t p log n n p xp 1 a:s:\nwhere D g; r 40 is a constant which only depends on g and r.\nProof: Similar to the proof of Lemma 5.3. &\nProof of Theorem 4.1: By Lemma 5.4 we have k p g\u0302n;y k \u00ff g\nk p 1 \u00ff log y log Xn; n\u00ff ky2 \u00ff Xn;n\u00ff ky Xn;n\u00ff ky \u00ff Xn;n\u00ffk \u00ff g !\nk p\n\u00ff log y log\n1 yg Xn; n\u00ff ky2 \u00ff Xn;n\u00ff ky Xn;n\u00ff ky \u00ff Xn;n\u00ffk \u00ff 1 !\nd k p\n\u00ff log y X n;n\u00ff ky2 \u00ff Xn;n\u00ff ky \u00ff y\u00ffg Xn;n\u00ff ky \u00ff Xn;n\u00ffk y\u00ffg Xn;n\u00ff ky \u00ff Xn;n\u00ffk 1 op 1\n\"\nk p\n\u00ff log y Xn;n\u00ff ky \u00ff U nky2 \u00ff 1 y \u00ffg Xn;n\u00ff ky \u00ff U nky y\u00ffg Xn;n\u00ffk \u00ff U nk y\u00ffg Xn;n\u00ff ky \u00ff Xn;n\u00ffk\nk p\n\u00ff log y U n ky2 \u00ff 1 y\u00ffg U nky y\u00ffgU nk y\u00ffg Xn;n\u00ff ky \u00ff Xn;n\u00ffk\n# 1 op 1\nnote Xn;n\u00ff ky \u00ff Xn;n\u00ffk\nn k U 0 nk\n? p y\u00ffg \u00ff 1\ng\n!\nd \" k p\n\u00ff log y Xn;n\u00ff ky2 \u00ff U nky2 \u00ff 1 y \u00ffg Xn;n\u00ff ky \u00ff U nky y\u00ffg Xn;n\u00ffk \u00ff U nk n k U 0 nk y\u00ffg y \u00ffg\u00ff1 g\nk p\n\u00ff log y U n\nky2 \u00ff 1 y\u00ffg U nky y\u00ffgU nk\nn k U 0 nk y\u00ffg y \u00ffg\u00ff1 g\n# 1 op 1\nd 1\u00ff log y 1 y\u00ffg y\u00ffg\u00ff1g w y2 \u00ff 1 y\u00ffg w y y\u00ffgw 1 op 1\nk p\n\u00ff log y U n\nky2 \u00ff 1 y\u00ffg U nky y\u00ffgU nk\nn k U 0 nk y\u00ffg y \u00ffg\u00ff1 g\n1 op 1 ;\nthus the asymptotic variance of k p g\u0302n;y k \u00ff g equals\ng2 y\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1 log y 2 y\u00ffg \u00ff 1 2\nand the asymptotic bias of k p g\u0302n;y k \u00ff g equals\nk p A n\nk\ny\u00ffr\n\u00ff log y g y\u00ffg \u00ff 1 1\u00ff yr r y\u00ffg\u00ffr \u00ff 1 g r :\nBy A t ct\u00ffr we get in a way similar to the proof of Theorem 3.1\nk0 n ,(\ny\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1 \u00ff2rc2 1\u00ffyrr 2 y\u00ffg\u00ffr\u00ff1 g r 2 y\u00ff2r\n! 1 1\u00ff2r\nn \u00ff2r 1\u00ff2r ) ?1: h\nProof of Theorem 4.2: By Lemma 5.4 we have k p g\u0302n;y k \u00ff g\u0302n;y ky2\nk p g\u0302n;y k \u00ff g \u00ff k p g\u0302n;y ky2 \u00ff g\nd \" k p\n\u00ff log y X\nn;n\u00ff ky2 \u00ff U n ky2 \u00ff 1 y\u00ffg Xn;n\u00ff ky \u00ff U n ky\ny\u00ffg Xn;n\u00ffk \u00ff U n k\ny\u00ffg Xn;\u00ff ky \u00ff Xn;n\u00ffk\nk p\nlog y\nU n ky2\n\u00ff 1 y\u00ffg U\nn ky\ny\u00ffgU\nn k\ny\u00ffg Xn;n\u00ff ky \u00ff Xn;n\u00ffk\n\u00ff k p\n\u00ff log y X\nn;n\u00ff ky4 \u00ff U n ky4 \u00ff 1 y\u00ffg X n;n\u00ff ky3 \u00ff U n ky3\ny\u00ffg X n;n\u00ff ky2 \u00ff U n ky2\ny\u00ffg X n;n\u00ff ky3 \u00ff Xn;n\u00ff ky2\n\u00ff\nk p\n\u00ff log y U n ky3\n\u00ff 1 y\u00ffg U n\nky2\ny\u00ffgU n\nky2\ny\u00ffg X n;n\u00ff ky3 \u00ff Xn;n\u00ff ky2\n# 1 op 1\nnote X n;n\u00ff ky3 \u00ff Xn;n\u00ff ky2 n k U 0 n k ?p y\u00ff2gy\u00ffg \u00ff 1 g\n!\nd 1\u00ff log y 1 y\u00ffg y\u00ffg\u00ff1g w y2 \u00ff 1 y\u00ffg w y y\u00ffgw 1 op 1\nk p\n\u00ff log y U n\nky2 \u00ff 1 y\u00ffg U n ky\ny\u00ffgU\nn k\nn k U 0 n k y\u00ffg y\u00ffg\u00ff1g\n1 op 1\n\u00ff 1\u00ff log y 1 y\u00ff3g y\u00ffg\u00ff1g w y4 \u00ff 1 y\u00ffg w y3 y\u00ffgw y2 op 1 \u00ff k p\n\u00ff log y U n ky4\n\u00ff 1 y\u00ffg U n\nky3\ny\u00ffgU n\nky2\nn k U 0 n k y\u00ff3g y\u00ffg\u00ff1g\n1 op 1 ;\nthus the asymptotic variance of k p g\u0302n;y k \u00ff g\u0302n;y ky2 equals\ng2 1 y\u00ff2g\u00ff1 y\u00ff1 \u00ff 1 1 y\u00ff2 log y 2 y\u00ffg \u00ff 1 2\nand the asymptotic bias of k p g\u0302n;y k \u00ff g\u0302n;y ky2 equals\nk p A n\nk\ny\u00ffr \u00ff log y 1\u00ff yr r y\u00ffg\u00ffr \u00ff 1 g r g y\u00ffg \u00ff 1 1\u00ff y \u00ff2r :\nBy A t ct\u00ffr we get in a way similar to the proof of Theorem 3.1\nk0 n ,(\ny\u00ff1 \u00ff 1 1 y\u00ff2g\u00ff1 1 y\u00ff2 \u00ff2rc2 1\u00ffyrr 2 y\u00ffg\u00ffr\u00ff1 g r 2 y\u00ff2r 1\u00ff y\u00ff2r 2\n! 1 1\u00ff2r\nn \u00ff2r 1\u00ff2r ) ?1: h\nProof of Theorem 4.3: Similar to the proof Theorem 3.3. &\nProof of Theorem 4.4: Similar to the proof of Theorem 3.4 by using Lemma 5.5 instead of Lemma 5.3. &\nProof of Theorem 4.5: Similar to the proof of Corollary 3.3. &"
        },
        {
            "heading": "6. Simulations",
            "text": "The bootstrap procedure was tested on various distribution functions in a small simulation study: 200 samples of size 10,000 are generated from each distribution function. To each sample the bootstrap method was applied, with e 0:05, that is n1 708 and n2 502, and 200 bootstrap samples. The distributions are Cauchy, generalized Pareto distribution (GPD) with g 1=4 and g \u00ff 1=4, generalized extreme value distribution (GEV) with g \u00ff 1=4 and g \u00ff 3=2 and \u00aenally the distribution with U t Hg;r t , equation (3.2) with g \u00ff 1=4, r \u00ff 1=10 and r \u00ff 1 (to have a distribution that allows a free choice of r ).\nFigure 1 illustrates the results for the Cauchy distribution: The bottom graph shows the observed and theoretical mean squared error of the g estimate as a function of k: the solid line represents the observed mean squared error (i.e. the sample mean of the estimated\ng\u0302n;2 k \u00ff g 2 of the individual simulated samples), and the dashed line the theoretical value calculated as V2 g =k A n=k b g; r 2. The vertical line indicates the sample mean of the k0 estimates. The two components of the MSE, bias and variance are illustrated in the top and middle graphs. Table 1 summarizes the simulation results. For each parameter the table reports\nthe theoretical value, statistics of the bootstrap estimates (sample mean, standard deviation and MSE of the estimates produced by the bootstrap procedure in the individual samples), observed optimal k, and the sample mean and MSE of the g estimate at this k.\nThe general conclusion is that the bootstrap procedure gives reasonable estimates for the sample fraction to be used. It is reasonable in terms of the MSE of the g estimate: for all but the last distribution the MSE of g\u0302n;2 k\u03020 is of the order of the MSE of the estimate at the observed optimal k.\nThe second order parameter r is only estimated correctly for the Cauchy distribution. The dif\u00aeculty of estimating r has also been reported by others (see Danielsson, 1996) and is subject for further study.\nFor three of the distributions no theoretical values for k0 has been given. These distributions have r g, a situation excluded in theorems (3.1) and (3.2). In this situation one cannot decide which part of the bias,\nis dominant. Most standard distributions with negative g turn out to fall in this category.\nClearly more work needs to be done: \u00aerst of all the performance of the r estimator and the g r situation need clari\u00aecation. And of course the effects of the number of bootstrap replications and the size of the bootstrap samples have to be studied."
        },
        {
            "heading": "7. Application",
            "text": "In the Neptune project (see de Haan and de Ronde (1997) for a review) we studied the joint distribution of extremes of wave-height, wave-period and sea levels. The project aimed at estimating failure probabilities of sea walls, based on the joint distribution of the extremes of the variables. A small dataset of 828 measurements covering 10 years at the Eierland station in the North sea was available. As is clear from Figure 2, the wave height data series does not behave very nicely, but it was what was available to us. The dif\u00aeculty in selecting a number of order statistics, makes the series a nice candidate for the bootstrap procedure (at the time we decided that only 27 order statistics should be used for estimation resulting in g\u0302n;2 0). We applied the bootstrap method in the following way: as in the simulation experiment we used 200 bootstrap samples, resulting in an estimate of the optimal k. In order to evaluate and improve the precision of this estimate, the procedure was repeated with again 200 bootstrap samples, and the estimates averaged, until the average had an estimated standard error less then 2.\nIn Figure 2 both the optimal k\u03020 259, estimated with e 0:1 and the corresponding g\u0302n;2 259 0:06 are indicated. This value is reasonably in line with g 0, the value used in the Neptune project.\nThe results for different values of e, determining the size of the bootstrap sample are shown in Table 2. The optimal k is not very sensitive to e, but it decreases with e only when e40:2, but those values correspond to a very small second bootstrap size.\nAppendix\nSecond order conditions\nThe second order relations in the Sections 3 and 4 are different. The reason for this stems from the expansion of the logarithms. Let us try to proceed from one to the other. The domain of attraction condition is\nlim t?? U tx \u00ff U t a t\nxg \u00ff 1 g for all x40: A.1\nIt follows, if U ? 40, that\nlim t?? log U tx \u00ff log U t a t =U t\nxg\u00ff \u00ff 1 g\u00ff : A.2\nSo far there are no complications. The natural second order condition related to (A.1) is\nlim t??\nU tx \u00ffU t a t \u00ff x g\u00ff1 g\nA t H x A.3\nfor some function A (positive or negative) with A t ?0 t?? . Now we try to work towards a second order condition for log U. Starting from (A.3)\nlog U tx \u00ff log U t\nlog 1\nU tx U t \u00ff 1\nU tx U t \u00ff 1\u00ff 1 2 U tx U t \u00ff 1\n2\nSo that (let us take g50 for example)\nlog U tx \u00ff log U t a t =U t \u00ff xg \u00ff 1 g\nU tx \u00ff U t a t \u00ff xg \u00ff 1 g \u00ff U t 2a t U tx U t \u00ff 1 2\nNow in some cases the \u00aerst term is dominant (the ``nice'' situation), but in other cases the second term is dominant. And sometimes there is no relative limit. The various cases are dealt with in the next Theorem and the Remark.\nTheorem A: Assume U ? 40 and there exist functions a t 40 and A t ?0 such that\nU tx \u00ffU t a t \u00ff x g\u00ff1 g\nA t ?Hg;r x\nwhere\nHg;r x 1 r xg r \u00ff 1 g r \u00ff xg \u00ff 1 g r 0 :\nSuppose that g 6 r. Then\nlim t??\na t U t \u00ff g\nA t c [ \u00ff?;?\nwhere\nc\n0 if g5r g\ng r if g4\u00ff r g\ng r if 05g5\u00ff r and limt?? U t \u00ff a t =g 0 +? if r5g 0 +? if 05g5\u00ff r and limt?? U t \u00ff a t =g 6 0 +? if g \u00ff r: 8>>>>>><>>>>>>: Furthermore\nlog U tx \u00fflog U t a t =U t \u00ff x g\u00ff\u00ff1 g\u00ff\n~A t ?Hg\u00ff;r0 x\nwhere\n~A t A t if c 0 g \u00ff a t U t if c +? rA t = g r if c g= g r ; 8><>>:\n~A t [RVr0 ,\nr0 \u00ffg if 05g \u00ffr g if r5g 0 r if g5r or g4\u00ff r: 8><>: Remark A: Hence r0 0 if g 0.\nProof: Suppose that g 6 0. Then from the proof of Lemma 5.2 we have\nU tx \u00ff a tx =g\u00ff U t \u00ff a t =g a t A t ?\u00ff 1 g xg r \u00ff 1 g r :\nIf g r40, then\nU t \u00ff a t =g a t A t ?\u00ff\n1\ng g r :\nHence\na t =U t \u00ff g A t a t g U t a t =g\u00ff U t a t A t ?g= g r :\nIf g r 0, i.e. g \u00ff r40, then\nU t \u00ff a t =g a t A t ?+?:\nHence\na t =U t \u00ff g A t a t g U t a t =g\u00ff U t a t A t ?+?:\nIf g r50, then\nU t \u00ff a t =g?c0 [ \u00ff?; 0 [ 0;? U t \u00ffa t =g\u00ffc0\na t A t ?\u00ff 1g g r :\n8<:\nFor g r50 and g40, i.e., 05g5\u00ff r, we have\na t =U t \u00ff g A t a t g\nU t a t =g\u00ff U t c0 a t A t \u00ff c0 a t A t\n? +? if c0 6 0 g= g r if c0 0:\nFor g r50 and g50, i.e., g50, we have ja t = U t A t j [RVg\u00ffr: Hence\na t = U t A t ? +? if g\u00ff r40 g50 0 if g\u00ff r50 g50:\nSuppose that g 0 and r50. Then from the proof of Lemma 5.2 a t ?c1 [ \u00ff?; 0 [ 0;? . Hence\na t = U t A t *c1= U ? A t ?+?:\nWe have now proved the \u00aerst part of the theorem. Note that a t =U t ?g : For g 0; we have\nlog U tx U t log 1 a t U t xg\u00ff \u00ff 1 g\u00ff A t Hg;r x o A t\na t U t\nxg\u00ff \u00ff 1\ng\u00ff A t Hg;r x o A t\n\u00ff 1\n2 a t U t 2 xg\u00ff \u00ff 1 g\u00ff A t Hg;r x o A t 2 o a t U t 2 ! ;\ni.e.,\nlog U tx \u00ff log U t a t =U t \u00ff xg\u00ff \u00ff 1 g\u00ff\nA t Hg;r x o A t\n\u00ff a t 2U t\nxg\u00ff \u00ff 1 g\u00ff 2 2x g\u00ff \u00ff 1 g\u00ff\nA t Hg;r x o A t \" #\no a t U t :\nFor g40, we have\nx\u00ffg U tx U t x\u00ffg a t\nU t 1\u00ff x\u00ffg g x\u00ffg a t U t A t Hg;r x o A t\n1 x\u00ffg \u00ff 1 1\u00ff a t gU t\nx\u00ffg a t\nU t A t Hg;r x o A t ;\ni.e.,\nlog U tx \u00ff log U t a t =U t \u00ff log x\n\u00ff log x x \u00ffg \u00ff 1 g U t a t a t =U t \u00ff g\nx\u00ffgA t Hg;r x o A t o a t =U t \u00ff g :\nSo the second part of the theorem follows easily. &\nRemark B: It is not true that a second order condition for U always implies a second order condition for log U: Let g r and de\u00aene\nU0 t tg\u00ff1 exp Z t\n1\nsg\u00ff1 2 sin log log s ds :\nFrom the representation (2.5) of de Haan and Resnick (1996) we \u00aend\nU tx \u00ffU t tU0 t \u00ff x g\u00ff1 g tg 2 sin log log t ? Z x\n1\nug\u00ff1 u\u00ffg \u00ff 1 \u00ffg du:\nHence\na t = U t A t\ntU 0 t\nU t tg 2 sin log log t * expfR? 1 sg\u00ff1 2 sin log log s dsg\nU ? 2 sin log log t\nwhich does not have a limit."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank two referees, the associate editor and the editor, who all put a lot of work into reviewing this paper, thereby greatly improving the mathematical content as well as the accessibility."
        }
    ],
    "title": "A Bootstrap-based Method to Achieve Optimality in Estimating the Extreme-value Index",
    "year": 2000
}