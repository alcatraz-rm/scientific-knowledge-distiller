{
    "abstractText": "A large part of the theory of extreme value index estimation is developed for positive extreme value indices. The best-known estimator of a positive extreme value index is probably the Hill estimator. This estimator belongs to the category of moment estimators, but can also be interpreted as a quasimaximum likelihood estimator. It has been generalized to a kernel-type estimator, but this kernel-type estimator can, similarly to the Hill estimator, only be used for the estimation of positive extreme value indices. In the present paper, we introduce kernel-type estimators which can be used for estimating the extreme value index over the whole (positive and negative) range. We present a number of results on their distributional behavior and compare their performance with the performance of other estimators, such as moment-type estimators for the whole range and the quasi-maximum likelihood estimator, based on the generalized Pareto distribution. We also discuss an automatic bandwidth selection method and introduce a kernel estimator for a second-order parameter, controlling the speed of convergence.",
    "authors": [
        {
            "affiliations": [],
            "name": "H. P. LOPUHA\u00c4"
        }
    ],
    "id": "SP:1d2281cdc069ae49ea7152bed08689ed233361e2",
    "references": [
        {
            "authors": [
                "J. BEIRLANT",
                "P. VYNCKIER",
                "J.L. TEUGELS"
            ],
            "title": "Excess functions and estimation of the extreme-value index",
            "venue": "Bernoulli",
            "year": 1996
        },
        {
            "authors": [
                "M. CS\u00d6RG\u0150",
                "S. CS\u00d6RG\u0150",
                "L. HORV\u00c1TH",
                "D. MASON"
            ],
            "title": "Weighted empirical and quantile processes",
            "venue": "Ann. Probab",
            "year": 1986
        },
        {
            "authors": [
                "S. CS\u00d6RG\u0150",
                "P. DEHEUVELS",
                "D. MASON"
            ],
            "title": "Kernel estimates of the tail index of a distribution",
            "venue": "Ann. Statist",
            "year": 1985
        },
        {
            "authors": [
                "S. CS\u00d6RG\u0150",
                "L. VIHAROS"
            ],
            "title": "Asymptotic normality of least-squares estimators of tail",
            "venue": "indices. Bernoulli",
            "year": 1997
        },
        {
            "authors": [
                "J. DANIELSSON",
                "L. DE HAAN",
                "L. PENG",
                "C.G. DE VRIES"
            ],
            "title": "Using a bootstrap method to choose the sample fraction in tail index estimation",
            "venue": "J. Multivariate Anal",
            "year": 2001
        },
        {
            "authors": [
                "A.L.M. DEKKERS",
                "L. DE HAAN"
            ],
            "title": "Optimal choice of sample fraction in extreme-value estimation",
            "venue": "J. Multivariate Anal",
            "year": 1993
        },
        {
            "authors": [
                "A.L.M. DEKKERS",
                "J.H.J. EINMAHL",
                "L. DE HAAN"
            ],
            "title": "A moment estimator for the index of an extreme-value distribution",
            "venue": "Ann. Statist",
            "year": 1989
        },
        {
            "authors": [
                "G. DRAISMA",
                "L. DE HAAN",
                "L. PENG",
                "T.T. PEREIRA"
            ],
            "title": "A bootstrap-based method to achieve optimality in estimating the extreme-value index",
            "venue": "Extremes",
            "year": 1999
        },
        {
            "authors": [
                "H. DREES"
            ],
            "title": "Refined Pickands estimators of the extreme value index",
            "venue": "Ann. Statist",
            "year": 1995
        },
        {
            "authors": [
                "H. DREES",
                "E. KAUFMANN"
            ],
            "title": "Selecting of the optimal sample fraction in univariate extreme-value estimation",
            "venue": "Stochastic Process. Appl",
            "year": 1998
        },
        {
            "authors": [
                "M.I. FRAGA ALVES",
                "L. DE HAAN",
                "T. LIN"
            ],
            "title": "Estimation of the parameter controlling the speed of convergence in extreme value theory",
            "venue": "Math. Methods Statist. To appear",
            "year": 2003
        },
        {
            "authors": [
                "J.L. GELUK",
                "L. DE HAAN"
            ],
            "title": "Regular Variation, Extensions and Tauberian Theorems",
            "venue": "Mathematical Centre Tracts",
            "year": 1987
        },
        {
            "authors": [
                "M.I. GOMES",
                "L. DE HAAN",
                "L. PENG"
            ],
            "title": "Semi-parametric estimation of the second order parameter\u2014asymptotic and finite sample behaviour. Extremes",
            "year": 2003
        },
        {
            "authors": [
                "L. DE HAAN",
                "T. PEREIRA"
            ],
            "title": "Estimating the index of a stable distribution",
            "venue": "Statist. Probab. Lett",
            "year": 1999
        },
        {
            "authors": [
                "P. HALL"
            ],
            "title": "Using the bootstrap to estimate mean squared error and select smoothing parameter in nonparametric problems",
            "venue": "J. Multivariate Anal",
            "year": 1990
        },
        {
            "authors": [
                "P. HALL",
                "A.H. WELSH"
            ],
            "title": "Best attainable rates of convergence for estimates of parameters of regular variation",
            "venue": "Ann. Statist",
            "year": 1984
        },
        {
            "authors": [
                "B.M. HILL"
            ],
            "title": "A simple general approach to inference about the tail of a distribution",
            "venue": "Ann. Statist",
            "year": 1975
        },
        {
            "authors": [
                "M. KRATZ",
                "S.I. RESNICK"
            ],
            "title": "The QQ-estimator and heavy tails",
            "venue": "Comm. Statist. Stochastic Models",
            "year": 1996
        },
        {
            "authors": [
                "D.M. MASON"
            ],
            "title": "Some characterizations of almost sure bounds for weighted multidimensional empirical distributions and a Glivenko\u2013Cantelli theorem for sample quantiles",
            "venue": "Z. Wahrsch. Verw. Gebiete",
            "year": 1982
        },
        {
            "authors": [
                "G. MATTHYS",
                "J. BEIRLANT"
            ],
            "title": "Adaptive threshold selection in tail index estimation",
            "venue": "In Extremes and Integrated Risk Management (P",
            "year": 2000
        },
        {
            "authors": [
                "E. OMEY",
                "E. WILLEKENS"
            ],
            "title": "\u03c0 -variation with remainder",
            "venue": "J. London Math. Soc. (2)",
            "year": 1987
        },
        {
            "authors": [
                "J. PICKANDS",
                "III"
            ],
            "title": "Statistical inference using extreme order statistics",
            "venue": "Ann. Statist",
            "year": 1975
        },
        {
            "authors": [
                "J. SCHULTZE",
                "J. STEINEBACH"
            ],
            "title": "On least squares estimates of an exponential tail coefficient",
            "venue": "Statist. Decisions",
            "year": 1996
        },
        {
            "authors": [
                "R.L. 1995 SMITH"
            ],
            "title": "Estimating tails of probability distributions",
            "venue": "Ann. Statist",
            "year": 1987
        }
    ],
    "sections": [
        {
            "text": "The Annals of Statistics 2003, Vol. 31, No. 6, 1956\u20131995 \u00a9 Institute of Mathematical Statistics, 2003\nKERNEL-TYPE ESTIMATORS FOR THE EXTREME VALUE INDEX"
        },
        {
            "heading": "BY P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "Delft University of Technology, Delft University of Technology and Statistics Netherlands\nA large part of the theory of extreme value index estimation is developed for positive extreme value indices. The best-known estimator of a positive extreme value index is probably the Hill estimator. This estimator belongs to the category of moment estimators, but can also be interpreted as a quasimaximum likelihood estimator. It has been generalized to a kernel-type estimator, but this kernel-type estimator can, similarly to the Hill estimator, only be used for the estimation of positive extreme value indices. In the present paper, we introduce kernel-type estimators which can be used for estimating the extreme value index over the whole (positive and negative) range. We present a number of results on their distributional behavior and compare their performance with the performance of other estimators, such as moment-type estimators for the whole range and the quasi-maximum likelihood estimator, based on the generalized Pareto distribution. We also discuss an automatic bandwidth selection method and introduce a kernel estimator for a second-order parameter, controlling the speed of convergence.\n1. Introduction. Let X1, . . . ,Xn denote a sample from a distribution function F , which is assumed to be in the domain of attraction of an extreme value distribution with extreme value index \u03b3 , denoted by F \u2208 D(G\u03b3 ). In the situation of estimating a positive extreme value index, one of the best-known estimators is the Hill estimator [Hill (1975)]. This estimator is consistent for all \u03b3 > 0, assuming only F \u2208 D(G\u03b3 ). In the case that the tail of the underlying distribution function is Pareto shaped, that is, 1 \u2212 F(x) = Cx\u22121/\u03b3 for all x \u2265 u with \u03b3 > 0, C > 0 and u > 0, the Hill estimator can be interpreted as a maximum likelihood estimator. This \u201cquasi\u201d likelihood approach was extended in Smith (1987), where a generalized Pareto distribution was assumed to hold for the tail of the underlying distribution function. The resulting estimator is consistent for \u03b3 > \u22121. Pickands (1975) proposed an estimator that is invariant under shift and scale transformations and that is consistent for all \u03b3 \u2208 R. However, it has poor efficiency.\nDekkers, Einmahl and de Haan (1989) extended the Hill estimator to an estimator that is consistent for all \u03b3 \u2208 R. The resulting estimator, also called the moment estimator, consists of two terms. The first term is the Hill estimator, which converges to \u03b3 \u22280. In order to have a consistent estimator for \u03b3 < 0, a second term was added that converges to \u03b3 \u2227 0. More recently, Beirlant, Vynckier and Teugels (1996) proposed an adaptive Hill estimator, which is also consistent for all \u03b3 \u2208 R.\nReceived April 2001; revised November 2002. AMS 2000 subject classifications. Primary 60G70, 62E20, 62G05; secondary 62G20, 62G30. Key words and phrases. Extreme value index, adaptive estimation, second order parameter\nestimation.\n1956\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1957\nDrees (1995) investigated a multistage procedure that results in a refinement of the Pickands estimator, which is consistent for all \u03b3 \u2208 R and improves the efficiency.\nAll the estimators mentioned above are based on the k largest observations. A major drawback of the estimators is the discrete character of the behavior of these estimators: adding a single large-order statistic in the calculation of the estimator, that is, increasing k by 1, can change the actual value of the estimate considerably. Plotting these estimators as a function of the order statistics used therefore often results in a zig-zag figure. In Cs\u00f6rgo\u030b, Deheuvels and Mason (1985), the Hill estimator is smoothed by a kernel. We call this estimator the CDM estimator. Incidentally, Hill\u2019s estimator reappears when substituting the uniform kernel in the CDM estimator. In the same paper, it was shown that it is possible to improve on the (asymptotic) variance of the estimator by choosing appropriate kernels. In this kernel-type estimator, the bandwidth h plays a similar role as the number of order statistics k in the aforementioned estimators: approximately nh order statistics will be used to calculate the estimate. Consequently, the estimator now depends in a continuous way on the fraction of order statistics used. Hence, plotting the estimator as a function of the bandwidth h then yields a smooth figure. Other attempts to construct smoothed versions of the Hill estimator can be found in Schultze and Steinebach (1996), Kratz and Resnick (1996) and Cs\u00f6rgo\u030b and Viharos (1997), which consider classical least squares estimators for the slope \u03b3 > 0 in a Pareto quantile plot.\nUnfortunately, the least squares estimators and the CDM kernel estimator are only valid for \u03b3 > 0. In the present paper, we introduce a new class of kerneltype estimators that is consistent for all \u03b3 \u2208 R. It should be emphasized that our estimator is not a smoothed version of the moment estimator, but is based on the von Mises conditions\nlim t\u2191x\u25e6F\n( d\ndt 1 \u2212 F(t) F \u2032(t)\n) = \u03b3,(1.1)\nwhere x\u25e6F = sup{x :F(x) < 1} \u2264 \u221e is the upper endpoint of F . These conditions are sufficient but not necessary for F \u2208 D(G\u03b3 ). Although this approach is different from the one that leads to the moment estimator, it will result in an estimator that also consists of two terms. We define the following estimator for \u03b3 \u2208 R:\n\u03b3\u0302 Kn,h = \u03b3\u0302 (pos)n,h \u2212 1 + q\u0302\n(2) n,h q\u0302 (1) n,h ,(1.2)\nwhere\n\u03b3\u0302 (pos) n,h = n\u22121\u2211 i=1 i n Kh ( i n )( logX(n\u2212i+1) \u2212 logX(n\u2212i)),\nq\u0302 (1) n,h = n\u22121\u2211 i=1 ( i n )\u03b1 Kh ( i n )( logX(n\u2212i+1) \u2212 logX(n\u2212i)),\n1958 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF\nq\u0302 (2) n,h = n\u22121\u2211 i=1 d du [u\u03b1+1Kh(u)]u=i/n(logX(n\u2212i+1) \u2212 logX(n\u2212i)),\nwith Kh(u) = K(u/h)/h and \u03b1 > 0. Note that all three quantities have an integral representation involving the empirical quantile function. This is explained in Section 2 [see (2.4), (2.6) and (2.8)], where we also give the motivation for this estimator and specify the conditions for the kernel K . An example of a suitable kernel is the biweight K(x) = 158 (1 \u2212 x2)2. The parameter \u03b1 is needed to prevent singularities near 0 and must be greater than 1/2 in order to have asymptotic normality. In our simulations, we took \u03b1 = 0.6. The first term in (1.2) is the kerneltype estimator of Cs\u00f6rgo\u030b, Deheuvels and Mason (1985) and is shown to converge to \u03b3 \u2228 0. Similarly to the moment estimator, the second term will compensate the behavior of the CDM kernel-type estimator for \u03b3 < 0 and is shown to converge to \u03b3 \u2227 0. The resulting estimator will inherit the smooth behavior of the CDM kernel-type estimator as well as the general applicability of the moment estimator.\nThe content of the paper is organized as follows. In Section 2, we explain how estimator (1.2) is motivated by (1.1). In Section 3, consistency of the estimator will be derived under the single condition that the underlying distribution function is in the domain of attraction of an extreme value distribution. Under additional assumptions on the underlying distribution, asymptotic normality will be derived in Section 4, and sufficient conditions are provided in Section 5, under which the asymptotic bias vanishes. In Section 6, we compare our estimator with other estimators, such as the moment estimator and the maximum likelihood estimator, and the more recent proposals by Beirlant, Vynckier and Teugels (1996) and Drees (1995). Finally, in Section 7, we discuss automatic bandwidth selection methods, in the course of which we also introduce a kernel estimate for an important secondorder parameter.\n2. Defining the estimator. Let X1, . . . ,Xn denote a sample from a distribution function F , with support on (0,\u221e). Suppose that F is in the domain of attraction of an extreme value distribution G\u03b3 for some \u03b3 \u2208 R, denoted by F \u2208 D(G\u03b3 ); that is, there exist {an} and {bn}, n \u2208 N, with an > 0 and bn \u2208 R, such that\nlim n\u2192\u221eF n(anx + bn) = G\u03b3 (x) = exp(\u2212(1 + \u03b3 x)\u22121/\u03b3 ) for all x with 1 + \u03b3 x > 0. We will use the convention that G0(x) = exp(\u2212e\u2212x) for x \u2208 R. Let Q denote the quantile function corresponding to F . By replacing t by Q(1 \u2212 s) in (1.1), the von Mises condition can be written as\nlim s\u21930\n( \u22121 \u2212 sF\n\u2032\u2032(Q(1 \u2212 s)) (F \u2032(Q(1 \u2212 s)))2\n) = \u03b3.\nIf logQ is well defined and differentiable, we can define the function \u03c6 by\n\u03c6(s) = \u2212s d ds logQ(1 \u2212 s).(2.1)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1959\nIn this case, the limit relation (1.1) can be translated into\nlim s\u21930\n( \u22121 + \u03c6(s) \u2212 s(d\n2/ds2) logQ(1 \u2212 s) (d/ds) logQ(1 \u2212 s)\n) = \u03b3.(2.2)\nThe construction of our estimator is based on this relation. Basically, we have to estimate the value of \u03c6, the numerator and denominator in (2.2) at 0. To get some intuition on how to construct the estimator, it is useful to consider the generalized Pareto distribution (GPD). For the GPD, the function \u03c6 is given by\n\u03c6GPD(s) =  \u03b3 1 \u2212 s\u03b3 , \u03b3 = 0, 1\nlog 1/s , \u03b3 = 0.\nClearly, for the GPD one has that\nlim s\u21930 \u03c6(s) = \u03b3 \u2228 0.(2.3)\nSuppose for the moment that \u03c6 in (2.1) exists and also satisfies (2.3). Let the empirical quantile function be defined by Qn(u) = inf{x :Fn(x) \u2265 u} and denote by X(1) \u2264 X(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 X(n) the order statistics corresponding to the sample X1,X2, . . . ,Xn. First, we estimate lims\u21930 \u03c6(s) by a kernel estimator\n\u03b3\u0302 (pos) n,h = \u2212 \u222b h 0\nuKh(u) d logQn(1 \u2212 u) (2.4)\n= n\u22121\u2211 i=1 i n Kh ( i n )( log X(n\u2212i+1) \u2212 log X(n\u2212i)),\nwhere Kh(u) = K(u/h)/h. Intuitively, using (2.3) and assuming that K integrates to 1, for h \u2193 0 this will behave as\n\u2212 \u222b h\n0 uKh(u) d logQ(1 \u2212 u) = \u222b 1 0 \u03c6(hu)K(u)du \u2192 (\u03b3 \u2228 0).\nThis is made rigorous for any F \u2208 D(G\u03b3 ) in Lemma 3.3, without assuming the differentiability of logQ. The numerator and the denominator on the left-hand side of (2.2) will be estimated separately at 0, using kernel-type estimators as well. In defining these estimators, we note that both numerator and denominator can be multiplied by any power of s, without changing the limit. Simulations show that this will lead to more stable estimators. For any \u03b1 > 0, we have that\nlim s\u21930 \u2212s(d2/ds2) logQ(1 \u2212 s) (d/ds) logQ(1 \u2212 s) = lims\u21930 s\u03b1+1(d2/ds2) logQ(1 \u2212 s) \u2212s\u03b1(d/ds) logQ(1 \u2212 s) .(2.5)"
        },
        {
            "heading": "1960 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "Note that, if (2.2) and (2.3) hold, the limit in (2.5) equals 1 + (\u03b3 \u2227 0). For the denominator on the right-hand side of (2.5), we estimate lims\u21930 \u2212s\u03b1(d/ds) \u00d7 log Q(1 \u2212 s) by a kernel estimator\nq\u0302 (1) n,h = \u2212 \u222b h 0\nu\u03b1Kh(u) d logQn(1 \u2212 u) (2.6)\n= n\u22121\u2211 i=1 ( i n )\u03b1 Kh ( i n )( logX(n\u2212i+1) \u2212 logX(n\u2212i)).\nIf we were to treat the numerator on the right-hand side of (2.5) similarly and treat Qn as if it were differentiable, we could estimate lims\u21930 s\u03b1+1(d2/ds2) \u00d7 log Q(1 \u2212 s) by \u222b h\n0 u\u03b1+1Kh(u)\nd2\ndu2 log Qn(1 \u2212 u).(2.7)\nTo overcome the difficulty that Qn is not differentiable, we use, as is customary in the literature on kernel estimation of derivatives of densities and regression functions, the derivative of the kernel instead of the derivative of a direct estimate of the unknown function. Hence, after using integration by parts in (2.7), we estimate lims\u21930 s\u03b1+1(d2/ds2) logQ(1 \u2212 s) by\nq\u0302 (2) n,h = \u2212 \u222b h 0 d du [u\u03b1+1Kh(u)]d logQn(1 \u2212 u)\n(2.8)\n= n\u22121\u2211 i=1 d du [u\u03b1+1Kh(u)]u=i/n(logX(n\u2212i+1) \u2212 log X(n\u2212i)).\nIntuitively, using (2.3), for h \u2193 0, the term q\u0302(1)n,h as defined in (2.6) will behave as\n\u2212 \u222b h\n0 u\u03b1Kh(u) d log Q(1 \u2212 u) = h\u03b1\u22121 \u222b 1 0 \u03c6(hu)u\u03b1\u22121K(u)du\n\u223c h\u03b1\u22121(\u03b3 \u2228 0) \u222b 1\n0 u\u03b1\u22121K(u)du.\nSimilarly, q\u0302(2)n,h as defined in (2.8) will behave as\n\u2212 \u222b h\n0\nd du [u\u03b1+1Kh(u)]d log Q(1 \u2212 u)\n\u223c h\u03b1\u22121 \u222b 1\n0 \u03c6(hu)u\u22121 d du [u\u03b1+1K(u)]du\n= h\u03b1\u22121(\u03b3 \u2228 0) \u222b 1\n0 u\u22121 d du [u\u03b1+1K(u)]du\n= h\u03b1\u22121(\u03b3 \u2228 0) \u222b 1\n0 u\u03b1\u22121K(u)du.\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1961\nIn the case \u03b3 > 0, this would immediately suggest that q\u0302(2)n,h/q\u0302 (1) n,h tends to 1. Without assuming differentiability, it is shown in Lemma 3.4, for any \u03b3 \u2208 R and for any F \u2208 D(G\u03b3 ), that q\u0302(2)n,h/q\u0302(1)n,h \u2192 1 + (\u03b3 \u2227 0).\nThe above discussion motivates the expression given in (1.2) as an estimator for \u03b3 \u2208 R. For the kernel K , we impose the following conditions. Let K : [0,1] \u2192 R+ be a fixed kernel function satisfying the following conditions:\n(CK1) K(x) = 0, whenever x /\u2208 [0,1) and K(x) \u2265 0, whenever x \u2208 [0,1); (CK2) K(1) = K \u2032(1) = 0; (CK3) \u222b 1 0 K(x)dx = 1; (CK4) K , K \u2032 and K \u2032\u2032 are bounded.\nIn the definition of \u03b3\u0302 Kn,h, the continuous parameter h is used. This bandwidth determines the number of order statistics that is used in the computation of the estimator. The continuous nature of the bandwidth ensures that the estimator is a smooth function of the fraction of order statistics used, as opposed to the more discrete nature of, for example, the moment estimator.\n3. Consistency. By rearranging terms and using that Qn(1 \u2212 u) = Xn\u2212k for k/n \u2264 u < (k + 1)/n, we can also write\n\u03b3\u0302 (pos) n,h = \u222b 1 0 logQn(1 \u2212 hu)d(uK(u)), q\u0302\n(1) n,h = h\u03b1\u22121 \u222b 1 0 log Qn(1 \u2212 hu)d(u\u03b1K(u)), q\u0302\n(2) n,h = h\u03b1\u22121 \u222b 1 0 log Qn(1 \u2212 hu)d ( d du [u1+\u03b1K(u)] ) .\nNote that Qn(s) D= Q( n(s)) and n(1 \u2212 s) D= 1 \u2212 n(s),(3.1)\nwhere n is the empirical quantile function of a uniform (0,1) sample U1, . . . ,Un. Since conditions (CK2) and (CK4) yield that \u222b d(uK(u)) = 0, we have that\n\u03b3\u0302 (pos) n,h\nD= \u222b 1\n0\n( logQ ( 1 \u2212 n(hu)) \u2212 logQ(1 \u2212 U(k+1)))d(uK(u)),\nwhere n is the empirical quantile function of a uniform (0,1) sample U1, . . . ,Un and k = nh . To avoid differentiability of the quantile function, we use the following lemma."
        },
        {
            "heading": "1962 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "LEMMA 3.1. Suppose F \u2208 D(G\u03b3 ) with x\u25e6F > 0. Denote the corresponding quantile function by Q(s) = F\u22121(s). Then, for some positive function a(\u00b7),\nlim s\u21930 log Q(1 \u2212 sy) \u2212 logQ(1 \u2212 s) a(s)/Q(1 \u2212 s) =  \u2212 log y, \u03b3 \u2265 0, y\u2212\u03b3 \u2212 1\n\u03b3 , \u03b3 < 0,\n(3.2)\nfor all y > 0. Moreover, for each \u03b5 > 0, there exists s0 such that for 0 < s \u2264 s0 and 0 < y \u2264 1,\n(1 \u2212 \u03b5)1 \u2212 y \u03b5 \u03b5 \u2212 \u03b5 < logQ(1 \u2212 sy) \u2212 log Q(1 \u2212 s)\na(s)/Q(1 \u2212 s) (3.3)\n< (1 + \u03b5)y \u2212\u03b5 \u2212 1\n\u03b5 + \u03b5\nprovided \u03b3 \u2265 0, and 1 \u2212 (1 + \u03b5)y\u2212\u03b3\u2212\u03b5 < logQ(1 \u2212 sy) \u2212 log Q(1 \u2212 s)\nlog Q(1) \u2212 logQ(1 \u2212 s) < 1 \u2212 (1 \u2212 \u03b5)y \u2212\u03b3+\u03b5(3.4)\nprovided \u03b3 < 0.\nPROOF. Rewrite Lemma 2.5 from Dekkers, Einmahl and de Haan (1989), using that Q(1 \u2212 s) = U(1/s), where U is the inverse of 1/(1 \u2212 F). Essentially, the inequalities are properties of regularly varying functions for \u03b3 < 0 and of -varying functions for \u03b3 \u2265 0.\nREMARK 3.1. From the properties of regularly varying functions, it follows that, in the case \u03b3 > 0, we can take a(s)/Q(1 \u2212 s) = \u03b3 in Lemma 3.1, whereas, in the case \u03b3 < 0, we can take a(s)/Q(1 \u2212 s) = \u2212\u03b3 (logQ(1) \u2212 logQ(1 \u2212 s)). Moreover, as a consequence of Lemma 3.1 and the properties of -varying functions, we have that, in the case \u03b3 = 0, a(s) = o(Q(1 \u2212 s)).\nThe idea now is to use (3.3) and (3.4) from Lemma 3.1 with y = n(hu)/U(k+1), where k = nh . Unfortunately, Lemma 3.1 cannot be applied directly. However, the next lemma shows that we may as well apply Lemma 3.1 with y equal to u instead of n(hu)/U(k+1).\nLEMMA 3.2. Let n(\u00b7) denote the empirical quantile function of U1, . . . ,Un with Ui i.i.d. U(0,1), h be a sequence of positive numbers with h = hn \u2192 0 and nhn \u2192 \u221e, as n \u2192 \u221e, and let L(\u00b7) be an integrable, bounded and positive function on (0,1). Define k = nh and \u03bb\u0304 = (\u03bb \u2227 0) for \u03bb > \u22121. Then, for each \u03b2 > (\u22121 \u2212 \u03bb\u0304),\u222b 1\n0\n[( n(hu)\nU(k+1)\n)\u03b2 \u2212 u\u03b2 ] u\u03bbL(u)du P\u21920 as n \u2192 \u221e.(3.5)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1963\nPROOF. The case \u03b2 = 0 is trivial; hence, we consider the case \u03b2 = 0. Write the left-hand side of (3.5) as\u222b 1\n0\n[( n(hu)\nU(k+1)\n)\u03b2 \u2212 ( n(ku/n)\nU(k+1)\n)\u03b2] u\u03bbL(u)du(3.6)\n+ \u222b 1\n0\n[( n(ku/n)\nU(k+1)\n)\u03b2 \u2212 u\u03b2 ] u\u03bbL(u)du.(3.7)\nFor (3.6), note that, for j = 1, . . . , k, by definition,\n( n(hu) )\u03b2 \u2212 ( n(ku/n))\u03b2 =  0, j \u2212 1 k < u \u2264 j nh ,\nU \u03b2 (j+1) \u2212 U\u03b2(j),\nj nh < u \u2264 j k .\nHence, (3.6) equals\nU \u2212\u03b2 (k+1) k\u2211 j=1 \u222b j/k j/nh ( U \u03b2 (j+1) \u2212 U\u03b2(j) ) u\u03bbL(u)du.\nLet \u2016L\u2016 = sups\u2208(0,1) |L(s)| and \u03bb\u0304 = \u03bb \u2227 0. Using that |x\u03bb+1 \u2212 y\u03bb+1| \u2264 (\u03bb + 1) \u00d7 (x \u2212 y)y\u03bb\u0304 for all 0 \u2264 y \u2264 x \u2264 1, and nh \u2212 k < 1, we get\u2223\u2223\u2223\u2223\u2223U\u2212\u03b2(k+1) k\u2211 j=1 \u222b j/k j/nh ( U \u03b2 (j+1) \u2212 U\u03b2(j) ) u\u03bbL(u)du\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u2016L\u2016\n\u03bb + 1U \u2212\u03b2 (k+1) k\u2211 j=1 \u2223\u2223U\u03b2(j+1) \u2212 U\u03b2(j)\u2223\u2223\u2223\u2223\u2223\u2223(jk )\u03bb+1 \u2212 ( j nh )\u03bb+1\u2223\u2223\u2223\u2223 < \u2016L\u2016U\u2212\u03b2(k+1)\nk\u2211 j=1 \u2223\u2223U\u03b2(j+1) \u2212 U\u03b2(j)\u2223\u2223 jknh ( 1 nh )\u03bb\u0304 .\nNote that the terms U\u03b2(j+1) \u2212 U\u03b2(j) are either all positive (in the case \u03b2 > 0) or all negative (in the case \u03b2 < 0), which implies that the right-hand side is equal to\n\u2016L\u2016 k(nh)1+\u03bb\u0304 U \u2212\u03b2 (k+1) \u2223\u2223\u2223\u2223\u2223 k\u2211 j=1 j ( U \u03b2 (j+1) \u2212 U\u03b2(j) )\u2223\u2223\u2223\u2223\u2223 = \u2016L\u2016\nk(nh)1+\u03bb\u0304 U\n\u2212\u03b2 (k+1) \u2223\u2223\u2223\u2223\u2223(k + 1)U\u03b2(k+1) \u2212 U\u03b2(1) \u2212 k\u2211\nj=1 U\n\u03b2 (j+1) \u2223\u2223\u2223\u2223\u2223 = \u2016L\u2016\nk(nh)1+\u03bb\u0304 \u2223\u2223\u2223\u2223\u2223 k+1\u2211 j=1 ( 1 \u2212 ( Uj U(k+1) )\u03b2)\u2223\u2223\u2223\u2223\u2223."
        },
        {
            "heading": "1964 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "Note that, for j = 1, . . . , k+1, U(k+1) \u2265 U\u03b2(j) \u2265 U\u03b2(1) if \u03b2 > 0, and U(k+1) \u2264 U\u03b2(j) \u2264 U\n\u03b2 (1) if \u03b2 < 0. This implies that, for all \u03b2 = 0, the last expression is bounded by\n\u2016L\u2016(k + 1) k(nh)1+\u03bb\u0304 \u2223\u2223\u2223\u22231 \u2212 ( U1U(k+1) )\u03b2 \u2223\u2223\u2223\u2223 \u2264 2\u2016L\u2016 (nh)1+\u03bb\u0304 \u2223\u2223\u2223\u22231 \u2212 ( U1U(k+1) )\u03b2 \u2223\u2223\u2223\u2223.(3.8)\nIn the case \u03b2 > 0, we know that, with probability 1, 1\u2212 (U(1)/U(k+1))\u03b2 is bounded between 0 and 1, hence, (3.8) tends to 0 as n \u2192 \u221e. In the case \u03b2 < 0, first observe that, for any integer 1 \u2264 k \u2264 n \u2212 1, we have that(\nU(1)\nU(k+1) , . . . ,\nU(k)\nU(k+1)\n) D= (V(1), . . . , V(k)),(3.9)\nwhere V(1), . . . , V(k) are the order statistics of k i.i.d. U(0,1) variables. Therefore, we have that U(1)/U(k+1) D= V(1), so that, for any \u03b4 > 0,\nP\n([( U(1)\nU(k+1)\n)\u03b2 \u2212 1 ] > \u03b4(nh)1+\u03bb\u0304 ) = 1 \u2212 (1 \u2212 (\u03b4(nh)1+\u03bb\u0304 + 1)1/\u03b2)k.(3.10)\nHowever, since \u03bb\u0304 > \u22121 and \u03b2 < 0, we have that k log ( 1 \u2212 (\u03b4(nh)1+\u03bb\u0304 + 1)1/\u03b2) = \u2212k(\u03b4(nh)1+\u03bb\u0304 + 1)1/\u03b2(1 + o(1)) as n \u2192 \u221e.\nUsing that k \u223c nh, we find that (3.10) tends to 0, whenever 1 + (1 + \u03bb\u0304)/\u03b2 < 0. Hence, (3.8) tends to 0 in probability as n \u2192 \u221e, whenever \u22121 \u2212 \u03bb\u0304 < \u03b2 < 0.\nFinally, consider the second term (3.7). Note that property (3.9) yields that all finite-dimensional projections of the process u \u2192 n(hu)/U(k+1) are equal in distribution to the finite-dimensional projections of the process u \u2192 k(u), where k(u) is the empirical quantile function of a U(0,1) sample V1, . . . , Vk . Hence, (3.7) is equal in distribution to\u222b 1\n0\n( k(u) \u03b2 \u2212 u\u03b2)u\u03bbL(u)du.(3.11) Moreover, for 0 < \u03bd1 < 1 + \u03bb and 0 < \u03bd2 < 1, we have\u2223\u2223\u2223\u2223 \u222b 1\n0\n( k(u) \u03b2 \u2212 u\u03b2)u\u03bbL(u)du\u2223\u2223\u2223\u2223 \u2264 sup\n0<u<1\n[ u\u03bd1(1 \u2212 u)\u03bd2| k(u)\u03b2 \u2212 u\u03b2 |]\u2016L\u2016\u222b 1\n0 u\u03bb\u2212\u03bd1(1 \u2212 u)\u2212\u03bd2 du.\nFor \u03b2 > 0, according to (3.1), the right-hand side has the same distribution as\nsup 0<u<1\n[ u\u03bd1(1 \u2212 u)\u03bd2|F\u22121\u03b2,k(u) \u2212 F\u22121\u03b2 (u)| ]\u2016L\u2016\u222b 1 0 u\u03bb\u2212\u03bd1(1 \u2212 u)\u2212\u03bd2 du,(3.12)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1965\nwhere F\u22121\u03b2 is the quantile function corresponding to the distribution function F\u03b2(x) = x1/\u03b2 for 0 < x < 1 and F\u22121\u03b2,k denotes the empirical quantile function of a sample Y1, . . . , Yk drawn from F\u03b2 . Note that, since 0 < |Y1| < 1, one has that E|Y1 \u2227 0|1/\u03bd1 = 0 and E(Y1 \u2228 0)1/\u03bd2 < \u221e for \u03bd2 > 0 and \u03b2 > 0. Theorem 3 in Mason (1982) then yields that the supremum in (3.12) tends to 0 with probability 1 as k \u2192 \u221e. Since \u03bd1 < (1 + \u03bb) and \u03bd2 < 1, the integral in (3.12) is finite. We conclude that, in the case \u03b2 > 0, (3.11) tends to 0 with probability 1 as k \u2192 \u221e. In the case \u03b2 < 0, again using (3.1), note that\nsup 0<u<1 u\u03bd1(1 \u2212 u)\u03bd2| k(u)\u03b2 \u2212 u\u03b2 | D= sup 0<u<1 (1 \u2212 u)\u03bd1u\u03bd2\u2223\u2223G\u22121\u03b2,k(u) \u2212 G\u22121\u03b2 (u)\u2223\u2223, with G\u22121\u03b2 the quantile function corresponding to the distribution function G\u03b2(x) = 1 \u2212 x1/\u03b2 for x \u2265 1 and G\u22121\u03b2,k denoting the empirical quantile function of a sample Z1, . . . ,Zk drawn from G\u03b2 . Again, use Theorem 3 in Mason (1982), with E |Z1 \u2227 0|1/\u03bd2 = 0, whenever \u03bd2 > 0 and \u03b2 < 0, and\nE(Z1 \u2228 0)1/\u03bd1 = \u2212 1 \u03b2 \u222b \u221e 1 z1/\u03bd1+1/\u03b2\u22121 dz < \u221e,\nwhenever \u03bd1 > \u2212\u03b2 . Hence, (3.11) tends to 0 almost surely as k tends to \u221e, taking \u2212\u03b2 < \u03bd1 < (1 + \u03bb) and 0 < \u03bd2 < 1.\nLEMMA 3.3. Assume that F \u2208 D(G\u03b3 ) for some \u03b3 \u2208 R. Let K be a kernel satisfying conditions (CK1)\u2013(CK4) and let \u03b3\u0302 Kn,h be defined by (1.2). If h = hn is such that h \u2193 0 and nh \u2192 \u221e as n \u2192 \u221e, then \u03b3\u0302 (pos)n,h \u2192 (\u03b3 \u2228 0) in probability.\nPROOF. First, observe that, according to (3.1) and conditions (CK2) and (CK4), we can write\n\u03b3\u0302 (pos) n,h\nD= \u222b 1\n0\n( logQ ( 1 \u2212 n(hu)) \u2212 logQ(1 \u2212 U(k+1)))d(uK(u)),\nwhere n is the empirical quantile function of a uniform (0,1) sample U1, . . . ,Un and k = nh . Consider the case \u03b3 > 0. By definition, U(k+1) \u2265 n(hu) with probability 1 for all u \u2208 (0,1), and U(k+1) \u2192 0 with probability 1 as h \u2193 0. We can therefore apply Lemma 3.1, with y = n(hu)/U(k+1), s = U(k+1) and a(s)/Q(1 \u2212 s) = \u03b3 (see Remark 3.1), to get that, with probability 1, for each \u03b5 > 0 there exists an n0 such that, for all n \u2265 n0,\n(1 \u2212 \u03b5)1 \u2212 ( n(hu)/U(k+1)) \u03b5 \u03b5 \u2212 \u03b5 < logQ(1 \u2212 n(hu)) \u2212 logQ(1 \u2212 U(k+1)) \u03b3\n< (1 + \u03b5)( n(hu)/U(k+1)) \u2212\u03b5 \u2212 1\n\u03b5 + \u03b5"
        },
        {
            "heading": "1966 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "for all u \u2208 (0,1). Defining L(u) = d(uK(u))/du, we get, by the boundedness of both K and K \u2032, that L(u) = L+(u) \u2212 L\u2212(u), where L\u00b1(u) are positive and bounded functions. Hence, for \u03b3 > 0,\n\u03b3\u0302 (pos) n,h < \u03b3 \u222b 1 0 [ (1 + \u03b5)( n(hu)/U(k+1)) \u2212\u03b5 \u2212 1 \u03b5 + \u03b5 ] L+(u) du\n\u2212 \u03b3 \u222b 1\n0\n[ (1 \u2212 \u03b5)1 \u2212 ( n(hu)/U(k+1)) \u03b5\n\u03b5 \u2212 \u03b5\n] L\u2212(u) du.\nApplying Lemma 3.2 twice (once with \u03b2 = \u2212\u03b5, \u03bb = 0 and L+ and once with \u03b2 = \u03b5, \u03bb = 0 and L\u2212) yields that, for any 0 < \u03b5 < 1, this upper bound tends to\n\u03b3 \u222b 1 0 [ (1 + \u03b5)u \u2212\u03b5 \u2212 1 \u03b5 + \u03b5 ] L+(u) du \u2212 \u03b3 \u222b 1 0 [ (1 \u2212 \u03b5)1 \u2212 u \u03b5 \u03b5 \u2212 \u03b5 ] L\u2212(u) du in probability as n \u2192 \u221e. Letting \u03b5 \u2193 0, by dominated convergence this tends to \u03b3\n\u222b 1 0 (\u2212 logu)L+(u) du \u2212 \u03b3 \u222b 1 0 (\u2212 logu)L\u2212(u) du\n= \u03b3 \u222b 1 0 (\u2212 log u)d((uK(u)) = \u03b3.\nSimilar arguments lead to a lower bound for \u03b3\u0302 (pos)n,h that tends to \u03b3 in probability as well. This proves the lemma for the case \u03b3 > 0.\nIn the case \u03b3 = 0, first note that, as a consequence of Lemma 3.1 together with the properties of -varying functions, one has that a(s) = o(Q(1 \u2212 s)) for s \u2193 0. Since U(k+1) \u2192 0 with probability 1, this means that a(U(k+1))/ Q(1 \u2212U(k+1)) \u2192 0 with probability 1. Similar to the case \u03b3 > 0, we can apply the inequalities of Lemma 3.1 to\nlogQ(1 \u2212 n(hu)) \u2212 log Q(1 \u2212 U(k+1)) a(U(k+1))/Q(1 \u2212 U(k+1)) .\nBy similar arguments as above, we conclude that \u03b3\u0302 (pos)n,h \u2192 0 in probability. Finally, consider the case \u03b3 < 0. Lemma 3.1 now yields the inequalities\n1 \u2212 (1 + \u03b5) ( n(hu)\nU(k+1)\n)\u2212\u03b3\u2212\u03b5 <\nlogQ(1 \u2212 n(hu)) \u2212 log Q(1 \u2212 U(k+1)) log Q(1) \u2212 logQ(1 \u2212 U(k+1))\n< 1 \u2212 (1 \u2212 \u03b5) ( n(hu)\nU(k+1)\n)\u2212\u03b3+\u03b5 .\nThus, with L\u00b1 as before, \u03b3\u0302\n(pos) n,h logQ(1) \u2212 log Q(1 \u2212 U(k+1)) < \u222b 1 0 [ 1 \u2212 (1 \u2212 \u03b5) ( n(hu) U(k+1) )\u2212\u03b3+\u03b5] L+(u) du\n\u2212 \u222b 1\n0\n[ 1 \u2212 (1 + \u03b5) ( n(hu)\nU(k+1)\n)\u2212\u03b3\u2212\u03b5] L\u2212(u) du.\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1967\nAgain, by two applications of Lemma 3.2 (once with \u03b2 = \u2212\u03b3 + \u03b5, \u03bb = 0 and L+ and once with \u03b2 = \u2212\u03b3 \u2212 \u03b5, \u03bb = 0 and L\u2212), we get that, for any 0 < \u03b5 < 1 \u2212 \u03b3 , the upper bound tends to\u222b 1\n0\n[ 1 \u2212 (1 \u2212 \u03b5)u\u2212\u03b3+\u03b5]L+(u) du \u2212 \u222b 1\n0\n[ 1 \u2212 (1 + \u03b5)u\u2212\u03b3\u2212\u03b5]L\u2212(u) du.\nSince both integrals are bounded for 0 < \u03b5 < 1 \u2212 \u03b3 and log Q(1) \u2212 log Q(1 \u2212 U(k+1)) \u2192 0 with probability 1, we get (with a similar lower bound) that \u03b3\u0302\n(pos) n,h \u2192 0.\nLEMMA 3.4. Assume that F \u2208 D(G\u03b3 ) for some \u03b3 \u2208 R. Let K be a kernel satisfying conditions (CK1)\u2013(CK4) and, for arbitrary \u03b1 > 0, let \u03b3\u0302 Kn,h be defined by (1.2). If h = hn is such that h \u2193 0 and nh \u2192 \u221e as n \u2192 \u221e, then q\u0302(2)n,h/q\u0302(1)n,h \u2192 1 + (\u03b3 \u2227 0) in probability.\nPROOF. Since we will consider q\u0302(2)n,h/q\u0302 (1) n,h, we can scale both numerator and denominator by the same factor, without changing the ratio. Moreover, by conditions (CK2) and (CK4), we have that, for any \u03b1 > 0,\u222b 1\n0 d ( u\u03b1K(u) ) = [u\u03b1K(u)]10 = 0 and \u222b 1\n0 d\n[ d\ndu u\u03b1+1K(u)\n] = [(\u03b1 + 1)u\u03b1K(u) + u\u03b1+1K \u2032(u)]10 = 0.\nFirst, consider \u03b3 \u2265 0. If we write d(u\u03b1K(u)) = u\u03b1\u22121L1(u) du, then, by the previous remarks, we have that\nh1\u2212\u03b1q\u0302(1)n,h a(U(k+1))/Q(1 \u2212 U(k+1))\nD= \u222b 1\n0 logQ(1 \u2212 n(hu)) \u2212 logQ(1 \u2212 U(k+1)) a(U(k+1))/Q(1 \u2212 U(k+1)) u \u03b1\u22121L1(u) du.\nSimilarly to the argument used in the proof of Lemma 3.3, we can first apply the inequalities from Lemma 3.1. Then, with \u03b5 > 0 fixed, let n \u2192 \u221e and apply Lemma 3.2 with \u03bb = \u03b1 \u2212 1, and finally let \u03b5 \u2193 0. We conclude that\nh1\u2212\u03b1q\u0302(1)n,h a(U(k+1))/Q(1 \u2212 U(k+1))\n(3.13)\n\u2192 \u222b 1 0 (\u2212 logu)d(u\u03b1K(u)) = \u222b 1 0 u\u03b1\u22121K(u)du"
        },
        {
            "heading": "1968 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "in probability. On the other hand, if we write\nd\n( d\ndu [u\u03b1+1K(u)]\n) = u\u03b1\u22121L2(u) du,\nwe have\nh1\u2212\u03b1q\u0302(2)n,h a(U(k+1))/Q(1 \u2212 U(k+1))\nD= \u222b 1\n0 logQ(1 \u2212 n(hu)) \u2212 log Q(1 \u2212 U(k+1)) a(U(k+1))/Q(1 \u2212 U(k+1)) u \u03b1\u22121L2(u) du.\nSimilarly, by an application of Lemmas 3.1 and 3.2, this tends in probability to\u222b 1 0 (\u2212 log u)d ( d du u\u03b1+1K(u) ) = \u222b 1 0 d du ( u\u03b1+1K(u) ) u\u22121 du = \u222b 1 0 u\u03b1\u22121K(u)du.\nCombining this with (3.13), we obtain that q\u0302(2)n,h/q\u0302 (1) n,h P\u21921, whenever \u03b3 \u2265 0. In the case \u03b3 < 0, similar arguments yield that\nh1\u2212\u03b1q\u0302(1)n,h logQ(1) \u2212 log Q(1 \u2212 U(k+1))\n\u2192 \u222b 1 0 (1 \u2212 u\u2212\u03b3 ) d(u\u03b1K(u)) = \u2212\u03b3 \u222b 1 0 u\u03b1\u2212\u03b3\u22121K(u)du\nin probability, and that\nh1\u2212\u03b1q\u0302(2)n,h logQ(1) \u2212 log Q(1 \u2212 U(k+1)) \u2192 \u222b 1 0 (1 \u2212 u\u2212\u03b3 ) d ( d du u\u03b1+1K(u) )\n= \u2212\u03b3 (1 + \u03b3 ) \u222b 1\n0 u\u03b1\u2212\u03b3\u22121K(u)du\nin probability. Hence, q\u0302(2)n,h/q\u0302 (1) n,h P\u21921 + \u03b3 as n \u2192 \u221e.\nThe following theorem is now a direct corollary of Lemmas 3.3 and 3.4.\nTHEOREM 3.1 (Consistency). Assume that F \u2208 D(G\u03b3 ) for some \u03b3 \u2208 R. Let K be a kernel satisfying conditions (CK1)\u2013(CK4) and, for arbitrary \u03b1 > 0, let \u03b3\u0302 Kn,h be defined by (1.2). If h = hn is such that h \u2193 0 and nh \u2192 \u221e as n \u2192 \u221e, then \u03b3\u0302 Kn,h \u2192 \u03b3 in probability as n \u2192 \u221e.\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1969\n4. Asymptotic normality. In order to obtain asymptotic normality, we need additional assumptions on F . Suppose that F \u2208 D(G\u03b3 ) for some \u03b3 \u2208 R and assume that \u03c6 from (2.1) exists and is well defined. Moreover, we assume that F satisfies the following conditions:\n(CP1) In the case \u03b3 \u2265 0, assume that \u03c6(s) \u2192 \u03b3 , as s \u2193 0. (CP2) In the case \u03b3 < 0, assume that, for some constant c > 0, s\u03b3 \u03c6(s) \u2192 \u2212c\u03b3 , as s \u2193 0. (CP3) In the case \u03b3 = 0, for all s > 0 assume that \u03c6(hs)/\u03c6(h) \u2192 1, as h \u2193 0. Consider the deterministic equivalent of \u03b3\u0302 Kn,h:\n\u03b3h = \u03b3 (pos)h + q\n(2) h q (1) h \u2212 1,(4.1)\nwith\n\u03b3 (pos) h = \u222b 1 0 log Q(1 \u2212 hu)d(uK(u)),(4.2) q\n(i) h = h\u03b1\u22121 \u222b 1 0\nlog Q(1 \u2212 hu)dK(i)(u), i = 1,2,(4.3) where K(1)(u) = u\u03b1K(u) and K(2)(u) = d(u\u03b1+1K(u))/du for a kernel K . Also write\nK (1) h (u) = u\u03b1Kh(u),(4.4)\nK (2) h (u) =\nd\ndu\n( u\u03b1+1Kh(u) ) .(4.5)\nLEMMA 4.1. Let X1, . . . ,Xn be a sample from F \u2208 D(G\u03b3 ) and suppose that F satisfies conditions (CP1)\u2013(CP3). Let K be a kernel satisfying conditions (CK1)\u2013(CK4) and let \u03b3\u0302 Kn,h be defined as in (1.2). Then, for any \u03b1 > 1 2 and h = hn, with h \u2193 0 and (nh)\u2212\u03b1 log n = O((nh)\u22121/2), as n \u2192 \u221e, we have, for i = 1,2, \u221a\nnhh1\u2212\u03b1 ( q\u0302\n(i) n,h \u2212 q(i)h ) D= \u222b 1 0 W(u) u \u03c6(hu)dK(i)(u) + oP (1)(4.6) as n \u2192 \u221e, where W denotes standard Brownian motion. PROOF. We will only present the proof for q\u0302(1)n,h, since the proof for q\u0302 (2) n,h is similar. The left-hand side of (4.6) can be decomposed into four parts:\nq\u0302 (1) n,h \u2212 q(1)h\n= \u222b 1/n\n0 log Qn(1 \u2212 u)dK(1)h (u) \u2212 \u222b 1/n 0 log Q(1 \u2212 u)dK(1)h (u)(4.7)\n+ \u222b bn\n1/n log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u) + \u222b h bn log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u),"
        },
        {
            "heading": "1970 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "where (bn) is a sequence of positive real numbers that satisfies 1/n < bn < h. For the first term of (4.7), note that Qn(1 \u2212 u) is constant for 0 \u2264 u < 1/n. Together with property (3.1), we get that\nh1\u2212\u03b1 \u222b 1/n\n0 logQn(1 \u2212 u)dK(1)h (u) D= logQ(1 \u2212 U(1)) \u222b 1/nh 0 dK(1)(u),\nwhere U(1) is the first order statistic from a sample U1, . . . ,Un from a uniform (0,1) distribution. Note that, from the properties of slowly varying functions, it follows that log Q(1 \u2212 s) \u2212 log s \u2192 0(4.8)\n[see de Wolf (1999) for a formal proof]. Therefore, since U(1) \u2192 0 almost surely, we have that\nh1\u2212\u03b1 \u222b 1/n\n0 logQn(1 \u2212 u)dK(1)h (u) D= oP (\u2212 log U(1) (nh)\u03b1 ) = oP ((nh)\u22121/2).(4.9)\nThe last equality follows from the fact that, for any \u03b5 > 0, P (\u2212(nh)1/2\u2212\u03b1 logU(1) \u2265 \u03b5)\n= 1 \u2212 (1 \u2212 exp(\u2212(nh)\u03b1\u22121/2\u03b5))n \u2264 n exp(\u2212(nh)\u03b1\u22121/2\u03b5), which tends to 0 according to the conditions on h. For the second part of (4.7), observe that, by integration by parts and application of (4.8),\nh1\u2212\u03b1 \u222b 1/n\n0 logQ(1 \u2212 u)dK(1)h (u)\n= h1\u2212\u03b1 logQ (\n1 \u2212 1 n\n) K\n(1) h\n( 1\nn\n) + \u222b 1/nh 0 \u03c6(hu)K(1)(u) du,\nwhere \u03c6 is defined in (2.1). Conditions (CP1)\u2013(CP3) yield that \u03c6(s) \u2192 (\u03b3 \u2228 0) as s \u2193 0. From the conditions on h, together with another application of (4.8), we conclude\nh1\u2212\u03b1 \u222b 1/n\n0 logQ(1 \u2212 u)dK(1)h (u)\n(4.10)\n= o ( logn\n(nh)\u03b1\n) + O((nh)\u22121) = o((nh)\u22121/2).\nFor the third part of (4.7), first observe that\u222b bn 1/n log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u)\nD= \u222b bn\n1/n\n[ log Q ( 1 \u2212 n(u)) \u2212 logQ(1 \u2212 u)]dK(1)h (u),\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1971\nwhere n is the empirical quantile function of a uniform (0,1) sample of size n. By the mean value theorem, we then get that\u222b bn\n1/n log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u) D= \u222b bn 1/n \u03c6(u + \u03ben,u) u + \u03ben,u ( u \u2212 n(u))dK(1)h (u),\nwith |\u03ben,u| \u2264 | n(u) \u2212 u|. We have that sup0<u<1 | n(u) \u2212 u| \u2192 0 with probability 1 as n \u2192 \u221e, and from Wellner (1978),\nsup 1/n\u2264u\u22641 \u2223\u2223\u2223\u2223 n(u)u \u2223\u2223\u2223\u2223 = OP (1) and sup 1/n\u2264u\u22641 \u2223\u2223\u2223\u2223 u n(u) \u2223\u2223\u2223\u2223 = OP (1).(4.11)\nFrom the conditions on F , it follows that \u03c6 is uniformly bounded in a neighborhood of 0. Furthermore, note that u/(u + \u03ben,u) lies between u/ n(u) and 1. Hence,\nsup 1/n\u2264u\u2264bn \u2223\u2223\u2223\u2223\u03c6(u + \u03ben,u) uu + \u03ben,u n(u) \u2212 uu \u2223\u2223\u2223\u2223 = OP (1).\nWriting dK(1)(u)/du = u\u03b1\u22121L1(u), we therefore obtain that\u2223\u2223\u2223\u2223 \u222b bn 1/n log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u) \u2223\u2223\u2223\u2223 \u2264 OP (1)\u222b bn 1/n \u2223\u2223\u2223\u2223dK(1)h (u)du \u2223\u2223\u2223\u2223du\n= h\u03b1\u22121OP (1) \u222b bn/h\n1/nh u\u03b1\u22121|L1(u)|du\n= h\u03b1\u22121OP ((bn/h)\u03b1). Taking bn = h(nh)\u2212(1/2+\u03bb)/\u03b1 for some 0 < \u03bb < \u03b1 \u2212 1/2, we get that\nh1\u2212\u03b1 \u222b bn\n1/n log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u) = oP ( (nh)\u22121/2 ) .(4.12)\nFinally, consider the fourth part of the decomposition (4.7). Following the same arguments as for the third part, we arrive at\u222b h\nbn\nlog (\nQn(1 \u2212 u) Q(1 \u2212 u)\n) dK\n(1) h (u) D= \u222b h bn \u03c6(u + \u03ben,u) u + \u03ben,u ( n(u) \u2212 u)dK(1)h (u)\nfor some |\u03ben,u| \u2264 | n(u) \u2212 u|. Since now bn \u2264 u \u2264 h, we have that\nsup bn\u2264u\u22641 \u2223\u2223\u2223\u2223 n(u) \u2212 uu \u2223\u2223\u2223\u2223 = oP (1)(4.13)\nfor any sequence (bn) of positive numbers satisfying nbn \u2192 \u221e as n \u2192 \u221e [see Wellner (1978)]. Condition (CP3) states that \u03c6 is slowly varying. This implies that \u03c6(hs)/\u03c6(h) \u2192 1 as h \u2193 0 uniformly for s \u2208 [a, b] for any 0 < a < b < \u221e. By"
        },
        {
            "heading": "1972 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "means of (4.13), we have that, for n sufficiently large, 1/2 < 1 + \u03ben,u/u < 3/2, which implies that\n\u03c6(u + \u03ben,u) \u03c6(u) = \u03c6(u(1 + \u03ben,u/u)) \u03c6(u)\n\u2192 1 uniformly for bn \u2264 u \u2264 h. It follows that, for all \u03b3 \u2208 R,\nsup bn\u2264u\u2264h \u2223\u2223\u2223\u2223\u03c6(u + \u03ben,u)\u03c6(u) uu + \u03ben,u \u2223\u2223\u2223\u2223 = 1 + oP (1).\nThis implies that\u222b h bn log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u) = ( 1 + oP (1)) \u222b h bn \u03c6(u) n(u) \u2212 u u dK (1) h (u). Note that, from Theorem 2.1 in Cs\u00f6rgo\u030b, Cs\u00f6rgo\u030b, Horv\u00e1th and Mason (1986), there exists a sequence (Bn) of Brownian bridges such that, for 0 \u2264 \u03bd < 1/2,\nsup 1/n\u2264u\u22641\u22121/n |\u221an( n(u) \u2212 u) \u2212 Bn(u)| u1/2\u2212\u03bd = OP (n\u2212\u03bd)(4.14) as n \u2192 \u221e, where n is the quantile function of U1, . . . ,Un. Applying (4.14), we get that\u222b h\nbn\n\u03c6(u) n(u) \u2212 u\nu dK\n(1) h (u) = n\u22121/2 \u222b h bn \u03c6(u) Bn(u) u dK (1) h (u) + Rn,h,\nwhere, for arbitrary 0 \u2264 \u03bd < 1/2,\n|Rn,h| \u2264 OP (n\u22121/2\u2212\u03bd) \u222b h bn u\u22121/2\u2212\u03bd|\u03c6(u)| \u2223\u2223\u2223\u2223dK(1)h (u)du \u2223\u2223\u2223\u2223du \u2264 h\u03b1\u22121OP ((nh)\u22121/2\u2212\u03bd) \u222b 1\nbn/h u\u22121/2\u2212\u03bd|\u03c6(hu)| \u2223\u2223\u2223\u2223dK(1)(u)du \u2223\u2223\u2223\u2223du\n= h\u03b1\u22121OP ((nh)\u22121/2\u2212\u03bd). Using that Bn(u)\nD= Wn(u) + \u03b6nu, where Wn is distributed as standard Brownian motion and \u03b6n is a standard normal variable, independent of Wn, we obtain, for h \u2193 0 and nh \u2192 \u221e,\u222b h\nbn\n\u03c6(u) Bn(u)\nu dK\n(1) h (u)\nD= \u222b h bn \u03c6(u) Wn(u) u dK (1) h (u) + \u03b6n \u222b h bn \u03c6(u) dK (1) h (u)\n= \u222b h bn \u03c6(u) Wn(u) u dK (1) h (u) + h\u03b1\u22121\u03b6n \u222b 1 bn/h \u03c6(hu)dK(1)(u)\nD= h\u03b1\u22121h\u22121/2 \u222b 1 bn/h \u03c6(hu) Wn(u) u dK(1)(u) + h\u03b1\u22121OP (1),\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1973\nwhere in the last equality we used that Wn(hu) D= \u221ahWn(u). Finally, since\nE |Wn(u)| \u2264 \u221a\nEWn(u)2 = \u221au, we find that\u222b bn/h 0 \u03c6(hu) W(u) u dK(1)(u) = OP ((bn/h)\u03b1\u22121/2) = oP (1).\nTherefore, by taking bn = h(nh)\u2212(1/2+\u03bb)/\u03b1 for some 0 < \u03bb < \u03b1 \u2212 1/2, we obtain that\nh1\u2212\u03b1 \u222b h bn log ( Qn(1 \u2212 u) Q(1 \u2212 u) ) dK (1) h (u)\nD= (nh)\u22121/2(1 + oP (1)) \u222b 1 0 \u03c6(hu) W(u) u dK(1)(u) + oP ((nh)\u22121/2)\n= (nh)\u22121/2 \u222b 1\n0 \u03c6(hu)\nW(u) u dK(1)(u) + oP ((nh)\u22121/2).\nTogether with decomposition (4.7), (4.9), (4.10) and (4.12), we obtain the assertion of the lemma for q(1)n,h. The argument for q (2) n,h runs similarly.\nTHEOREM 4.1 (Asymptotic normality). Let X1, . . . ,Xn be a sample from F with F satisfying (CP1)\u2013(CP3). Let K be a kernel satisfying conditions (CK1)\u2013(CK4) and let \u03b3\u0302 Kn,h be defined as in (1.2). Then, for any \u03b1 > 1/2 and h = hn with h \u2193 0 and (nh)\u2212\u03b1 logn = O((nh)\u22121/2) as n \u2192 \u221e,\n\u221a nh(\u03b3\u0302 Kn,h \u2212 \u03b3h) D\u2192N (0, \u03c3 2K),\nwhere \u03b3h is defined in (4.1) and \u03c3 2K = \u222b 1\n0\n( a0K\u0303(u) + a1K\u0303(2)(u) \u2212 a2K\u0303(1)(u))2 du,\nwith\nK\u0303(u) = \u222b 1 u x\u22121 d ( xK(x) ) , u \u2208 (0,1],\nK\u0303(i)(u) = \u222b 1 u\nx\u22121\u2212(\u03b3\u22270) dK(i)(x), u \u2208 (0,1], and\na0 = \u03b3 \u2228 0,\na1 = 1 /\u222b 1\n0 x\u22121\u2212(\u03b3\u22270)K(1)(x) dx,\na2 = (1 + (\u03b3 \u2227 0))a1."
        },
        {
            "heading": "1974 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "PROOF. First, note that by partial integration and application of (4.8), we have, for i = 1,2,\nh1\u2212\u03b1q(i)h = \u222b 1\n0\n\u03c6(hu)\nu K(i)(u) du.\nNote that, from the conditions on F , it follows that, in the case \u03b3 = 0, we have\nsup 0<u<1 \u2223\u2223\u2223\u2223\u03c6(hu)\u03c6(h) \u2212 u\u2212\u03b3\u0304 \u2223\u2223\u2223\u2223 \u2192 0(4.15)\nas h \u2193 0, where \u03b3\u0304 = \u03b3 \u2227 0. This implies that\u2223\u2223\u2223\u2223 \u222b 1 0 ( \u03c6(hu) \u03c6(h) \u2212 u\u2212\u03b3\u0304 ) K(i)(u) u du \u2223\u2223\u2223\u2223 = o(1).(4.16) In the case \u03b3 = 0, the function \u03c6 is slowly varying. This means we can apply the following inequality, taken from the proof of the proposition in the Appendix of de Haan and Pereira (1999): for each \u03b5, \u03b51 > 0, there exists an h0 such that, for all h \u2264 h0 and all hu \u2264 h0,\u2223\u2223\u2223\u2223\u03c6(hu)\u03c6(h) \u2212 1\n\u2223\u2223\u2223\u2223 \u2264 \u03b5e\u03b51| logu| = \u03b5u\u2212\u03b51,(4.17) where in the last equality we used that u \u2208 (0,1). This implies that (4.16) also holds in the case \u03b3 = 0. Hence, for all \u03b3 \u2208 R, we have, for i = 1,2,\nh1\u2212\u03b1q(i)h = \u03c6(h) [\u222b 1\n0 u\u22121\u2212\u03b3\u0304 K(i)(u) du + o(1)\n] .(4.18)\nSince this is O(1), we have from Lemma 4.1 that\nq\u0302 (2) n,h q\u0302 (1) n,h = q (2) h q (1) h + (nh) \u22121/2A(2)n h1\u2212\u03b1q(1)h \u2212 (nh) \u22121/2A(1)n h1\u2212\u03b1q(2)h (h1\u2212\u03b1q(1)h )2 + oP ((nh)\u22121/2),(4.19) where, for i = 1,2,\nA(i)n = \u222b 1\n0 \u03c6(hu)\nW(u)\nu dK(i)(u).(4.20)\nBecause \u03b3\u0302 (pos)n,h \u2212 \u03b3 (pos)h is a special case of q\u0302(1)n,h \u2212 q(1)h for \u03b1 = 1, another consequence of Lemma 4.1 is that\n\u221a nh ( \u03b3\u0302\n(pos) n,h \u2212 \u03b3 (pos)h ) = (\u03b3 \u2228 0)\u222b 1 0 W(u) u d ( uK(u) ) + oP (1) = \u2212a0 \u222b 1 0 W(u)dK\u0303(u) + oP (1).\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1975\nWe find that\n\u221a nh(\u03b3\u0302 Kn,h \u2212 \u03b3h) = \u221a nh ( \u03b3\u0302 (pos) n,h \u2212 \u03b3 (pos)h ) + \u221anh( q\u0302(2)n,h q\u0302\n(1) n,h\n\u2212 q (2) h\nq (1) h\n)\n= \u2212a0 \u222b 1\n0 W(u)dK\u0303(u) + A\n(2) n\nh1\u2212\u03b1q(1)h \u2212 A\n(1) n h 1\u2212\u03b1q(2)h (h1\u2212\u03b1q(1)h )2 + oP (1).\nTo deal with the A(i)n , i = 1,2, we again make use of (4.15) and (4.17). Together with E |W(u)| \u2264 \u221aEW(u)2 = \u221au, Markov\u2019s inequality, the conditions on K and the fact that \u03b1 > 1/2, this implies that, for \u03b3 \u2208 R, we have, for i = 1,2,\u2223\u2223\u2223\u2223 \u222b 1\n0\n( \u03c6(hu)\n\u03c6(h) \u2212 u\u03b3\u0304\n) W(u)\nu dK(i)(u) \u2223\u2223\u2223\u2223 = oP (1). Hence, for all \u03b3 \u2208 R, we have, for i = 1,2,\nA(i)n = \u03c6(h) [\u222b 1\n0 u\u22121\u2212\u03b3\u0304 W(u) dK(i)(u) + oP (1)\n] .(4.21)\nBy using (4.18) and (4.21), it follows that, for h \u2193 0, A (2) n\nh1\u2212\u03b1q(1)h =\n\u222b 1 0 u\n\u22121\u2212\u03b3\u0304 W(u) dK(2)(u)\u222b 1 0 u \u22121\u2212\u03b3\u0304 K(1)(u) du + oP (1)\n= \u2212a1 \u222b 1\n0 W(u)dK\u0303(2)(u) + oP (1)\nand\nA (1) n h 1\u2212\u03b1q(2)h (h1\u2212\u03b1q(1)h )2\n= \u222b 1 0 u \u22121\u2212\u03b3\u0304 W(u) dK(1)(u)\u222b 1\n0 u \u22121\u2212\u03b3\u0304 K(1)(u) du\n\u222b 1 0 u\n\u22121\u2212\u03b3\u0304 K(2)(u) du\u222b 1 0 u \u22121\u2212\u03b3\u0304 K(1)(u) du + oP (1)\n= a2 \u222b 1\n0 W(u)dK\u0303(1)(u) + oP (1),\nbecause\na2 = a21 \u222b 1 0 u\u22121\u2212\u03b3\u0304 K(2)(u) du = (1 + (\u03b3 \u2227 0))a1.\nHence, by integration by parts,\n\u221a nh(\u03b3\u0302 Kn,h \u2212 \u03b3h) D\u2192 \u222b 1 0 [ a0K\u0303(u) + a1K\u0303(2)(u) \u2212 a2K\u0303(1)(u)]dW(u).(4.22)\nThe assertion of the theorem follows.\nThe asymptotic variance depends on \u03b3 and the choice of the kernel K . We tried the following three different kernels: the biweight K(x) = 158 (1 \u2212 x2)2,"
        },
        {
            "heading": "1976 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "the triweight K(x) = 3516 (1 \u2212 x2)3 and the quadweight K(x) = 315128(1 \u2212 x2)4. The asymptotic variances of the corresponding estimators as a function of \u03b3 are displayed in Figure 1. It can be seen that one can reduce the variance for \u03b3 > 0 by taking higher powers of (1 \u2212 x2), but then the variance for \u03b3 < 0 increases. It seems that, among the above three estimators, the one constructed with the biweight kernel K(x) = 158 (1 \u2212 x2)2 has the best overall performance.\n5. Exploring the bias. The formulation of Theorem 4.1 implies that \u03b3\u0302 Kn,h might have asymptotic bias of the form \u221a nh(\u03b3h \u2212 \u03b3 ). In Dekkers and de Haan (1993), conditions are stated that cover all possible second-order behavior of quantile functions corresponding to distribution functions that are in the domain of attraction of an extreme value distribution. Under these additional conditions, we will derive asymptotic expressions for the bias. The conditions can be formulated in the following way:\n(RV1) In the case \u03b3 > 0, let U1(s) = log Q(1 \u2212 s) + \u03b3 log s \u2212 log c. Suppose that either U1 or \u2212U1 eventually remains positive, as s \u2193 0, and there exist \u03c1 > 0 and c > 0 such that, for all x > 0,\nlim s\u21930\nU1(sx)\nU1(s) = x\u03b3\u03c1.(5.1)\n(RV2) In the case \u03b3 < 0, let U2(s) = s\u03b3 (log Q(1) \u2212 logQ(1 \u2212 s)) \u2212 c/Q(1). Suppose that either U2 or \u2212U2 eventually remains positive, as s \u2193 0, and there exist \u03c1 > 0 and c > 0 such that, for all x > 0,\nlim s\u21930\nU2(sx) U2(s) = x\u2212\u03b3\u03c1.(5.2)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1977\nNote that condition (RV1) states that either U1 or \u2212U1 is regularly varying at 0 with index \u03b3\u03c1, whereas condition (RV2) states that either U2 or \u2212U2 is regularly varying at 0 with index \u2212\u03b3\u03c1. The generalized Pareto distribution satisfies conditions (RV1) and (RV2) for suitable choices of the parameters, and similarly this holds for the generalized extreme value distribution and the model considered in Hall and Welsh (1984). Other examples are the Cauchy distribution, which satisfies (RV1), and the uniform distribution, which satisfies (RV2).\nThe second set of conditions concerns the second-order -varying behavior of the quantile function.\n(PV1) In the case \u03b3 > 0, suppose there exists a positive function b1(\u00b7) such that, for all x > 0,\nlim s\u21930 V1(sx) \u2212 V1(s) a1(s) = \u2212 logx,(5.3) where V1(s) = \u00b1(log Q(1 \u2212 s)+ \u03b3 log s) and a1(s) = s\u2212\u03b3 b1(s)/Q(1 \u2212 s). (PV2) In the case \u03b3 = 0, suppose there exist positive functions b2(\u00b7) and b3(\u00b7),\nwith b2(s) \u2192 0, as s \u2193 0, such that for all x > 0,\nlim s\u21930 V2(sx) \u2212 V2(s) + b2(s) logx b3(s) = \u2212(log x) 2 2 ,\nwhere V2(s) = logQ(1 \u2212 s). (PV3) In the case \u03b3 < 0, suppose there exists a positive function b4(\u00b7) such that\nfor all x > 0,\nlim s\u21930 V3(sx) \u2212 V3(s) a3(s) = \u2212 logx,(5.4) where V3(s) = \u00b1s\u03b3 (logQ(1) \u2212 logQ(1 \u2212 s)) and a3(s) = b4(s)/Q(1). Note that condition (PV1) states that either logQ(1 \u2212 s)+ \u03b3 log s or \u2212(logQ(1 \u2212 s) + \u03b3 log s) is -varying at 0 with auxiliary function s\u2212\u03b3 b1(s)/Q(1 \u2212 s) and that condition (PV3) states either s\u03b3 (logQ(1)\u2212 log Q(1 \u2212 s)) or \u2212s\u03b3 (log Q(1)\u2212 logQ(1 \u2212 s)) is -varying at 0 with auxiliary function b4(s)/Q(1). The generalized Pareto distribution and the generalized extreme value distribution, both with \u03b3 = 0, are examples that satisfy condition (PV2).\nThe following lemmas are analogous to Lemma 3.1 and will be needed to apply dominated convergence to integrals such as \u222b Ui(su)/Ui(s) dK\n(j)(u) as s \u2193 0 for i = 1,2 and j = 1,2.\nLEMMA 5.1. Assume that conditions (RV1) and (RV2) hold. Then, for any \u03b5 > 0, there exists s0 > 0 such that for all 0 < s < s0 and 0 < y < 1, for \u03b3 > 0,\n(1 \u2212 \u03b5)y\u03b3\u03c1+\u03b5 < U1(sy) U1(s) < (1 + \u03b5)y\u03b3\u03c1\u2212\u03b5"
        },
        {
            "heading": "1978 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "and, for \u03b3 < 0,\n(1 \u2212 \u03b5)y\u2212\u03b3\u03c1+\u03b5 < U2(sy) U2(s) < (1 + \u03b5)y\u2212\u03b3\u03c1\u2212\u03b5, where U1 and U2 are defined in conditions (RV1) and (RV2).\nPROOF. The inequalities are the well-known inequalities of regularly varying functions [see, e.g., Geluk and de Haan (1987)].\nSimilar inequalities can be derived in the case of second-order -variation. They are stated in the next lemma, which is a reformulation of Lemma 3.5 in Dekkers, Einmahl and de Haan (1989) in terms of the quantile function.\nLEMMA 5.2. In the case \u03b3 > 0, assume that (5.3) holds for V1. Then, for any \u03b5 > 0, there exists s0 > 0 such that for all 0 < s < s0 and 0 < y < 1,\n(1 \u2212 \u03b5)1 \u2212 y \u03b5 \u03b5 \u2212 \u03b5 < V1(sy) \u2212 V1(y) a1(s) < (1 + \u03b5)y \u2212\u03b5 \u2212 1 \u03b5\n+ \u03b5."
        },
        {
            "heading": "In the case \u03b3 = 0, for any \u03b5 > 0, there exists s0 > 0 such that for all 0 < s < s0",
            "text": "and 0 < y < 1,\n(1 \u2212 \u03b5)2y\u03b5(logy)2 2 + 2\u03b5 log y \u2212 \u03b5 < V2(sy) \u2212 V2(s) + b2(s) logy b3(s)\n< (1 + \u03b5)2y\u2212\u03b5(logy)2\n2 \u2212 2\u03b5 logy + \u03b5.\nIn the case \u03b3 < 0, assume that (5.4) holds for V3. Then, for any \u03b5 > 0, there exists s0 > 0 such that for all 0 < s < s0 and 0 < y < 1,\n(1 \u2212 \u03b5)1 \u2212 y \u03b5 \u03b5 \u2212 \u03b5 < V3(sy) \u2212 V3(s) a3(s) < (1 + \u03b5)y \u2212\u03b5 \u2212 1 \u03b5 + \u03b5.\nPROOF. In the case \u03b3 = 0, the inequalities are just the well-known inequalities for -varying functions [see, e.g., Geluk and de Haan (1987), page 27]. In the case \u03b3 = 0, the inequalities follow using Omey and Willekens (1987) to obtain an asymptotic expression for b2(\u00b7) and applying the inequalities for -varying functions to that expression [see the proof of Lemma 3.5 in Dekkers, Einmahl and de Haan (1989)].\nDefining\n\u03bbst = \u222b 1\n0 us(log u)tK(u) du, s, t \u2265 0,(5.5)\nthe results concerning the asymptotic bias can be formulated in the following way.\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1979\nTHEOREM 5.1. Let \u03b3h be given by (4.1) for some \u03b1 > 0. Assume that K satisfies conditions (CK1)\u2013(CK4). Suppose that Q satisfies conditions (RV1) and (RV2) and that h = hn is such that hn \u2193 0 as n \u2192 \u221e. Then, as n \u2192 \u221e, in the case \u03b3 > 0,\n\u03b3h \u2212 \u03b3 = \u00b51U1(h) + \u00b52U1(h) + o(U1(h)) \u00b53 + \u00b54U1(h) + o(U1(h)) + o\n( U1(h) ) ,\nand, in the case \u03b3 < 0,\n\u03b3h \u2212 \u03b3 = \u00b55h\u2212\u03b3 U2(h) + \u00b56h\u2212\u03b3 + \u00b57U2(h) + o(U2(h)) \u00b58 + \u00b59U2(h) + o(U2(h)) + o\n( h\u2212\u03b3 U2(h) ) ,\nwhere the functions U1 and U2 are defined in (RV1) and (RV2) where, using the notation for the coefficients \u03bbst introduced in (5.5),\n\u00b51 = \u2212\u03b3\u03c1\u03bb\u03b3\u03c1,0, \u00b55 = \u2212\u03b3 (1 + \u03c1)\u03bb\u2212\u03b3 (1+\u03c1),0, \u00b52 = \u03b3\u03c12\u03bb\u03b3\u03c1+\u03b1\u22121,0, \u00b56 = \u2212\u03b3 c\u03bb\u2212\u03b3,0/Q(1), \u00b53 = \u03bb\u03b1\u22121,0, \u00b57 = \u03b3\u03c1(1 + \u03c1)\u03bb\u03b1\u2212\u03b3 (1+\u03c1)\u22121,0, \u00b54 = \u2212\u03c1\u03bb\u03b3\u03c1+\u03b1\u22121,0, \u00b58 = c\u03bb\u03b1\u2212\u03b3\u22121,0/Q(1),\n\u00b59 = (1 + \u03c1)\u03bb\u03b1\u2212\u03b3 (1+\u03c1)\u22121,0. Here c and \u03c1 are defined as in (RV1) and (RV2).\nPROOF. It is sufficient to consider only the case where U1 eventually remains positive and satisfies (5.1). For i = 1,2, consider\nh1\u2212\u03b1q(i)h = \u222b 1\n0 logQ(1 \u2212 hu)dK(i)(u)\n= U1(h) \u222b 1\n0\nU1(hu)\nU1(h) dK(i)(u) \u2212 \u222b 1 0 ( \u03b3 log(hu) \u2212 log c)dK(i)(u),\nwhere the function U(1) is defined in condition (RV1). For any \u03b1 > 0 and i = 1,2, we have that\nK(i)(0) = K(i)(1) = 0,(5.6) and, for any s, t \u2265 0 and i = 1,2, we have that\u222b 1\n0 us(logu)t dK(i)(u)\n= \u2212s\u03bbs+\u03b1\u22121,t \u2212 t\u03bbs+\u03b1\u22121,t\u22121(5.7) + (i \u2212 1){s2\u03bbs+\u03b1\u22121,t + 2st\u03bbs+\u03b1\u22121,t\u22121 + t (t \u2212 1)\u03bbs+\u03b1\u22121,t\u22122}.\nFor i = 1,2, write Li(u) = d(K(i)(u))/du. From condition (CK4), it follows that Li(u) = L+i (u) \u2212 L\u2212i (u), where L\u00b1i are positive and bounded. Hence, similar"
        },
        {
            "heading": "1980 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "to the proof of Lemma 3.3, using the inequalities of Lemma 5.1 and dominated convergence, condition (RV1) yields that\u222b 1\n0\nU1(hu)\nU1(h) dK(i)(u) \u2192 \u222b 1 0 u\u03b3\u03c1 dK(i)(u) for i = 1,2.(5.8) From (5.6)\u2013(5.8), it follows that, for i = 1,2,\nh1\u2212\u03b1q(i)h = \u03b3 \u03bb\u03b1\u22121,0 + U1(h) \u222b 1 0 u\u03b3\u03c1 dK(i)(u) + o(U1(h)).\nThe integral on the right-hand side can be evaluated by means of (5.7). Note that \u03b3\n(pos) h equals q (1) h with \u03b1 = 1. Putting things together proves the theorem for the case \u03b3 > 0. For the case \u03b3 < 0, it is sufficient to consider only the case where U2 eventually remains positive and satisfies (5.2). Similarly, using (RV2) and (5.6), for i = 1,2 we can write\nh1\u2212\u03b1q(i)h = \u2212h\u2212\u03b3 U2(h) \u222b 1\n0 u\u2212\u03b3 U2(hu) U2(h) dK(i)(u) \u2212 h \u2212\u03b3 c Q(1) \u222b 1 0 u\u2212\u03b3 dK(i)(u)\n= \u2212h\u2212\u03b3 U2(h) \u222b 1\n0 u\u2212\u03b3 (1+\u03c1) dK(i)(u) \u2212 h \u2212\u03b3 c Q(1) \u222b 1 0 u\u2212\u03b3 dK(i)(u)\n+ o(h\u2212\u03b3 U2(h)), where the function U2 is defined in condition (RV2) and where we have used the inequalities of Lemma 5.1, together with dominated convergence and condition (RV2). Again, the integrals on the right-hand side can be evaluated with (5.7). Hence, by putting things together this proves the theorem for the case \u03b3 < 0.\nREMARK 5.1. According to condition (RV1), |U1| is regularly varying with index \u03b3\u03c1 > 0, so that, by Proposition 1.7.1 in Geluk and de Haan (1987), it follows that U1(s) \u2192 0 and, similarly, U2(s) \u2192 0. This means that, for the case \u03b3 > 0, one can write\n\u03b3h \u2212 \u03b3 = c1U1(h) + o(U1(h)), where c1 = (\u00b51\u00b53 + \u00b52)/\u00b53, and for the case \u03b3 < 0,\n\u03b3h \u2212 \u03b3 = c2U2(h) + \u00b56h\u2212\u03b3 + O(h\u2212\u03b3 U2(h)) + o(U2(h)), where c2 = \u00b57/\u00b58.\nCOROLLARY 5.1. Assume the conditions of Theorem 4.1 and suppose that conditions (RV1) and (RV2) are satisfied. Suppose that h = hn is such that, as n \u2192 \u221e, h \u2193 0 and:\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1981\n(i) in the case \u03b3 > 0, nhU1(h)2 \u2192 0; (ii) in the case \u03b3 < 0, nhU2(h)2 \u2192 0 and nh1\u22122\u03b3 \u2192 0.\nThen, as n \u2192 \u221e, \u221a nh(\u03b3\u0302 Kn,h \u2212 \u03b3 ) D\u2192N (0, \u03c3 2K), with \u03c3 2K as defined in Theorem 4.1.\nIn the derivation of the asymptotic expansion of the bias under condition (PV1), we have to distinguish between the case that either V1(s) = logQ(1 \u2212 s) + log s or V1(s) = \u2212(logQ(1 \u2212 s) + log s) satisfies (5.3), and similarly for asymptotic expansion of the bias under condition (PV3).\nTHEOREM 5.2. Let \u03b3h be given by (4.1) for some \u03b1 > 0. Assume that K satisfies conditions (CK1) and (CK2) and that Q satisfies conditions (PV1)\u2013(PV3). Suppose that h = hn is such that, when n \u2192 \u221e, h \u2193 0. Then, in the case \u03b3 > 0,\n\u03b3h \u2212 \u03b3 = \u00b1a1(h) + o(a1(h)) \u03bd1(\u03b3 \u00b1 a1(h)) + o(a1(h)) + o\n( a1(h) ) ,\nwhere one should read a1 (or \u2212a1) whenever V1(s) = logQ(1 \u2212 s) + log s [or V1(s) = \u2212(log Q(1 \u2212 s) + log s)] satisfies (5.3). In the case \u03b3 = 0,\n\u03b3h = b2(h) + \u03bd2b3(h) + \u03bd3b3(h) + o(b3(h)) \u03bd4b3(h) + \u03bd1b2(h) + o(b3(h)) + o\n( b3(h) ) ."
        },
        {
            "heading": "In the case \u03b3 < 0,",
            "text": "\u03b3h \u2212 \u03b3 = \u00b1\u03bd5h\u2212\u03b3 a3(h) + \u03bd6h\u2212\u03b3 V3(h) + \u00b1\u03bd7a3(h) + o(a3(h))\u00b1\u03bd8a3(h) + \u03bd7V3(h) + o(a3(h)) + o ( h\u2212\u03b3 a3(h) ) ,\nwhere one should read a3 (or \u2212a3) whenever V3(s) = s\u03b3 (log Q(1) \u2212 logQ(1 \u2212 s)) [or V3(s) = \u2212s\u03b3 (log Q(1)\u2212 logQ(1 \u2212 s))] satisfies (5.4). The functions a1, b2, b3, a3 and V3 are defined in (PV1)\u2013(PV3) and, using the notation for the coefficients \u03bbst introduced in (5.5),\n\u03bd1 = \u03bb\u03b1\u22121,0, \u03bd5 = \u03b3 \u03bb\u2212\u03b3,1 \u2212 \u03bb\u2212\u03b3,0, \u03bd2 = \u03bb0,1, \u03bd6 = \u2212\u03b3 \u03bb\u2212\u03b3,0, \u03bd3 = \u2212\u03bb\u03b1\u22121,0, \u03bd7 = \u2212\u03b3 \u03bb\u03b1\u2212\u03b3\u22121,0, \u03bd4 = \u03bb\u03b1\u22121,1, \u03bd8 = \u03b3 \u03bb\u03b1\u2212\u03b3\u22121,1 \u2212 \u03bb\u03b1\u2212\u03b3\u22121,0.\nPROOF. For the case \u03b3 > 0, we only consider the case where V1(s) = logQ(1 \u2212 s) + log s satisfies (5.3). The case V1(s) = \u2212(logQ(1 \u2212 s) + log s)"
        },
        {
            "heading": "1982 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "can be handled by a similar argument. The proof is similar to that of Theorem 5.1. Using (5.6) and (5.7), we have, for i = 1,2,\nh1\u2212\u03b1q(i)h = a1(h) \u222b 1\n0 V1(hu) \u2212 V1(h) a1(h) dK(i)(u) + \u03b3 \u03bb\u03b1\u22121,0,\nwhere the function a1 is defined in condition (PV1). Again, writing d(K(i)(u))/ du = Li(u) = L+i (u) \u2212 L\u2212i (u), with L\u00b1i positive and bounded, we use similar arguments as in the proofs of Lemma 3.3 and Theorem 5.1. Using the inequalities of Lemma 5.2 and dominated convergence, from condition (PV1) it follows that, for i = 1,2,\u222b 1\n0 V1(hu) \u2212 V1(h) a1(h)\ndK(i)(u) \u2192 \u222b 1\n0 (\u2212 log u)dK(i)(u) = \u03bb\u03b1\u22121,0\nas h \u2193 0, where we again used (5.7). Combining things proves the theorem for \u03b3 > 0.\nIn the case \u03b3 = 0, using (5.6), for i = 1,2 we can write\nh1\u2212\u03b1q(i)h = b3(h) \u222b 1\n0 V2(hu) \u2212 V2(h) + b2(h) logu b3(h) dK(i)(u)\n\u2212 b2(h) \u222b 1\n0 log udK(i)(u),\nwhere the functions V2, b2 and b3 are defined in condition (PV2). By a similar argument, using the inequalities of Lemma 5.2 and dominated convergence, we have from condition (PV2) that, for i = 1,2,\u222b 1\n0 V2(hu) \u2212 V2(h) + b2(h) logu b3(h) dK(i)(u) \u2192 \u22121 2 \u222b 1 0 (log u)2 dK(i)(u).\nBy means of (5.7), we find that, for i = 1,2, h1\u2212\u03b1q(i)h = b3(h) { \u03bb\u03b1\u22121,1 \u2212 (i \u2212 1)\u03bb\u03b1\u22121,0} + b2(h)\u03bb\u03b1\u22121,0 + o(b3(h)). Putting things together proves the theorem for \u03b3 = 0. For the case \u03b3 < 0, we only consider the case where V3(s) = s\u03b3 (logQ(1) \u2212 log Q(1 \u2212 s)) satisfies (5.4). The case V3(s) = \u2212s\u03b3 (log Q(1) \u2212 logQ(1 \u2212 s)) can be handled by a similar argument. Using (5.6), for i = 1,2, write\nh1\u2212\u03b1q(i)h = \u2212h\u2212\u03b3 a3(h) \u222b 1 0 u\u2212\u03b3 V3(hu) \u2212 V3(h) a3(h) dK(i)(u)\n\u2212 h\u2212\u03b3 V3(h) \u222b 1\n0 u\u2212\u03b3 dK(i)(u),\nwhere the function a3 is defined in condition (PV3). As before, using the inequalities of Lemma 5.2 together with dominated convergence, from condition (PV3)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1983\nwe obtain\u222b 1 0 u\u2212\u03b3 V3(hu) \u2212 V3(h) a3(h) dK(i)(u) \u2192 \u2212 \u222b 1 0 u\u2212\u03b3 logudK(i)(u) as h \u2193 0. If we evaluate the integrals by means of (5.7), we find that, for i = 1,2, h1\u2212\u03b1q(i)h = h\u2212\u03b3 a3(h)\n{ \u03b3 \u03bb\u03b1\u2212\u03b3\u22121,1 \u2212 \u03bb\u03b1\u2212\u03b3\u22121,0 + (i \u2212 1)(\u03b3 2\u03bb\u03b1\u2212\u03b3\u22121,1 \u2212 2\u03b3 \u03bb\u03b1\u2212\u03b3\u22121,0)}\n\u2212 h\u2212\u03b3 V3(h){\u03b3 \u03bb\u03b1\u2212\u03b3\u22121,0 + (i \u2212 1)\u03b3 2\u03bb\u03b1\u2212\u03b3\u22121,0} + o(h\u2212\u03b3 a3(h)). Putting things together proves the theorem for \u03b3 < 0.\nCOROLLARY 5.2. Assume the conditions of Theorem 5.2 and suppose that (PV1)\u2013(PV3) are satisfied. Suppose that h = hn is such that, as n \u2192 \u221e, h \u2193 0 and in the case \u03b3 > 0,\nnha1(h) 2 \u2192 0,\nin the case \u03b3 = 0,\nnhb2(h) 2 \u2192 0 and nh\n( b3(h)\nb2(h)\n)2 \u2192 0,\nand in the case \u03b3 < 0,\nnh1\u22122\u03b3 V3(h)2 \u2192 0 and nh ( a3(h)\nV3(h)\n)2 \u2192 0.\nThen \u221a nh(\u03b3h \u2212 \u03b3 ) \u2192 0 as n \u2192 \u221e.\nNote that the condition for the case \u03b3 > 0 and the second condition for the case \u03b3 < 0 resemble the conditions on the parameter k in the case of the moment estimator as defined in Dekkers, Einmahl and de Haan (1989).\n6. Comparison with other estimators. To illustrate the finite-sample behavior of our estimator, we present some results from a small simulation study. We will compare our estimator to the moment estimator of Dekkers, Einmahl and de Haan (1989), the (quasi) MLE of Smith (1987) and the more recent proposals of Beirlant, Vynckier and Teugels (1996) and Drees (1995). For easy reference, we restate their definitions. The moment estimator is given by\n\u03b3\u0302 Mn,k = M(1)n,k + 1 \u2212 1\n2\n( 1 \u2212 (M (1) n,k) 2\nM (2) n,k\n)\u22121 ,"
        },
        {
            "heading": "1984 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "where, for r = 1,2,\nM (r) n,k =\n1\nk k\u2211 i=1 ( log X(n\u2212i+1) \u2212 log X(n\u2212k))r .\nNote that k is the number of largest order statistics from the sample used to calculate the moment estimator. The (quasi) maximum likelihood estimator \u03b3\u0302 MLn,k is defined using the excesses Yi = Xj \u2212 un, where Xj is the ith exceedance over the threshold un tending the upper endpoint of the distribution that generated the sample. Assuming that these excesses are distributed as a sample of a generalized Pareto distribution with parameters \u03b3 and \u03c3(un), the estimator is defined by maximizing the likelihood of Y1, . . . , YN , where N is the number of excesses over the threshold un. In our simulations, we took un = X(n\u2212k). Note that, again, k is the number of upper order statistics used to calculate the estimator. The adjusted Hill estimator from Beirlant, Vynckier and Teugels (1996) is defined as\n\u03b3\u0302 AHn,h = 1\nk k\u2211 i=1 log UHi,k \u2212 log UHk+1,n,\nwhere\nUHl,n = X(n\u2212l) ( 1\nl l\u2211 j=1\nlogX(n\u2212j+1) \u2212 log X(n\u2212l) ) .\nFor the multistage procedure that leads to the refined Pickands estimator \u03b3\u0302 RPn,h, we refer to Drees (1995). For our kernel estimator, we took \u03b1 = 0.6 (to ensure asymptotic normality) and the biweight kernel K defined by K(x) = 158 (1 \u2212 x2)2 for 0 \u2264 x \u2264 1.\nWe start by presenting a plot of the above methods used to estimate the extreme value index of a real-life data set. The data concerned were obtained from Lobith, the village where the first inhabitants of the Netherlands (the \u201cBataviers\u201d) are supposed to have entered on rafts along the Rhine River. They represent the peaks in the water discharges at that particular place along the Rhine. During the period 1901\u20131991, the maximum water discharge was measured on a daily basis. These maxima were plotted against time and only those maxima above a certain threshold and at least a fortnight apart were recorded. Whenever several values appeared above the threshold but within a fortnight of each other, the maximum of these values was recorded. This resulted in a data set of 155 measurements. To be able to compare the estimators, we will plot each estimator as a function of the fraction of order statistics used to calculate the estimator. That is, we will use k = nh and plot each estimator as a function of h \u2208 (0,1). The plots are given in Figure 2. All estimators have a kind of dip near 0.15. This is caused by a gap between the largest order statistics and the other sample values. The refined Pickands estimator \u03b3\u0302 RPn,k reduces the jumpy behavior of the original Pickands estimator, but is still\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1985\nless stable than all other methods. One striking feature of the kernel estimator is its smoothness: whereas the other estimators behave rather erratically as a function of h, the kernel estimator behaves very smoothly. A major advantage of this feature is that the exact choice of the bandwidth h to be used is not as crucial as the exact choice of the k in the other estimators: increasing k by 1 can seriously change the value of the estimator. Changing h by 1/n, however, does not change the kernel estimator too much. Indeed, only an approximately optimal bandwidth would produce an estimate almost as good as the estimate using the exact optimal bandwidth.\nWe also compared the stability of the estimators as a function of h for a single sample of size n = 100 from three distributions corresponding to \u03b3 negative, zero and positive: a uniform distribution on the interval (2,5), the exponential distribution with mean 1 and a distribution derived from the Hall model with extreme value index \u03b3 = 1/3 [see, e.g., Hall and Welsh (1984)]. The Hall model that we use corresponds to the distribution function\nF(x) = 1 \u2212 2 x3\n( 1 \u2212 1\n2x3\n) , x \u2265 1.\nThe results are shown in Figure 3. First of all, each estimator is quite close to the true value of the extreme value index, considering the small sample size. The behavior of the estimators is similar to that in Figure 2.\nFinally, we compared the simulated mean squared error of the \u03b3\u0302 AHn,k , \u03b3\u0302 M n,k and \u03b3\u0302 MLn,k with our kernel estimator, for a sample of size n = 100 from the same three distributions mentioned above. The results of 1000 samples of size n = 100 are displayed in Figure 4. The kernel estimator and the adjusted Hill estimator outperform the other estimators for \u03b3 < 0. For \u03b3 > 0, all estimators, except the refined Pickands estimator, behave similarly. For \u03b3 = 0, the kernel"
        },
        {
            "heading": "1986 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "KERNEL ESTIMATORS FOR \u03b3 \u2208 R 1987\nestimator behaves similarly to \u03b3\u0302 MLn,k . Apart from \u03b3\u0302 RP n,k , all estimators reach the similar minimum values for the mean squared error. We conclude that the kernel estimator behaves more smoothly as a function of the bandwidth than the moment-type estimators and quasi-maximum likelihood estimator as a function of the fraction of order statistics and that its mean squared error attains values of the same order as the mean squared error of the other estimators.\n7. Automatic bandwidth choice. One of the things that remains to be discussed is the (automatic) choice of the bandwidth. This topic is the subject of a manuscript in preparation. But because of the importance of this issue, we still want to discuss the matter here.\nA bootstrap-based approach to the choice of number of largest order statistics in moment-type estimators is presented in, for example, Draisma, de Haan, Peng and Pereira (1999) and Danielsson, de Haan, Peng and de Vries (2001). The basic difficulty in a bootstrap-based approach is the fact that in the empirical (nonparametric) bootstrap the bias is not adequately estimated in the evaluation of the bootstrap mean squared error, unless one performs bootstrapping with vanishing sample fractions. This fact has been clearly pointed out in, for example, Hall (1990), where the idea of bootstrapping with vanishing sample fractions was introduced.\nNow, with our kernel-type estimators, we can follow a similar approach as in Draisma, de Haan, Peng and Pereira (1999) and Danielsson, de Haan, Peng and de Vries (2001). In these papers, the difference of two moment-type estimators is used for dealing with the difficulty of estimating the bias. Instead, we can use two estimators \u03b3\u0302 K1n,h and \u03b3\u0302 K2 n,h based on two different kernels, say the biweight kernel\nK1(u) = 158 (1 \u2212 u2)21[0,1](u), and the triweight kernel\nK2(u) = 3516(1 \u2212 u2)31[0,1](u). We first present the method, outlined in Draisma, de Haan, Peng and Pereira [(1999, page 368], as it would apply to our kernel-type estimators. Let X1, . . . ,Xn be a sample from a distribution for which we want to estimate the extreme value index.\nSTEP 1. For a sample size n1 n, select a bootstrap sample X\u22171 , . . . ,X\u2217n1 from the original sample and compute the estimates (\u03b3\u0302 K1n1,h) \u2217 and (\u03b3\u0302 K2n1,h) \u2217 defined as in (1.2), with the order statistics X(i) replaced by the order statistics X\u2217(i) of the bootstrap sample. Next, compute\n\u03b4\u2217n1,h = ( \u03b3\u0302 K1 n1,h )\u2217 \u2212 (\u03b3\u0302 K2n1,h)\u2217.(7.1)"
        },
        {
            "heading": "1988 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "STEP 2. Repeat this procedure r times independently, yielding a sequence \u03b4\u2217n1,h,1, . . . , \u03b4 \u2217 n1,h,r . Then compute\nM\u0302SE \u2217( \u03b4\u2217n1,h ) = 1\nr r\u2211 i=1 ( \u03b4\u2217n1,h,i )2 ,\nwhich is an estimate of the bootstrap mean squared error of \u03b4\u2217n1,h.\nSTEP 3. Compute\nh\u2217(n1) def= arg min h\nM\u0302SE \u2217( \u03b4\u2217n1,h ) .(7.2)\nIn practice, one would compute M\u0302SE \u2217 (\u03b4\u2217n1,h) on a grid of values of hi , say, with distance 0.01 between successive values (the exact distance might be chosen to be dependent on the sample size n1), and then take for h\u2217(n1) the minimizer of the values M\u0302SE\n\u2217 (\u03b4\u2217n1,hi ).\nSTEP 4. Repeat steps 1 to 3 independently with n1 replaced by n2 = n21/n . This yields a value h\u2217(n2), defined by\nh\u2217(n2) def= arg min h\nM\u0302SE \u2217( \u03b4\u2217n2,h ) .\nSTEP 5. Estimate the optimal bandwidth h\u0302n,opt by\nh\u0302n,opt = c(h\u2217(n1), h\u2217(n2))h\u2217(n1)2 h\u2217(n2) ,(7.3)\nwhere c(h1, h2) is a function of h1 and h2, depending on the kernels K1 and K2 and the sample sizes n1 and n2.\nNext, we discuss why this procedure would \u201cwork\u201d for our kernel-type estimator, for example, under the second-order condition used in Danielsson, de Haan, Peng and de Vries (2001). Note that this is our (RV1) condition of Section 5. If n1 = O(n1\u2212\u03b5), for some \u03b5 \u2208 (0,1), then, using Theorem 4.1, we have\n\u03b3\u0302 K1 n1,h \u2212 \u03b3\u0302 K2n1,h = \u03b3 K1 h \u2212 \u03b3 K2h + Dn1,h\u221a n1h ,(7.4)\nwhere Dn1,h has a limiting normal distribution with mean 0, and where, according to (4.1)\u2013(4.3), for i = 1,2,\n\u03b3 Ki h = \u222b 1 0 log Q(1 \u2212 hu)d(uKi(u)) + \u222b 10 logQ(1 \u2212 hu)dK(2)i (u)\u222b 1 0 logQ(1 \u2212 hu)dK(1)i (u) \u2212 1.\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1989\nThis means that the random variable \u03b4\u2217n1,h, defined by (7.1), has an expansion of the form\n\u03b4\u2217n1,h = \u03b3\u0302 K1n,h \u2212 \u03b3\u0302 K2n,h + D\u2217n1,h\u221a\nn1h + Op ( 1\u221a nh ) ,(7.5) where the conditional distribution of D\u2217n1,h, given the original sample X1, . . . ,Xn, is again asymptotically normal as n1 \u2192 \u221e under the conditions on n1 and h given in Theorem 4.1 (with n replaced by n1).\nNow, as an example, consider the model in Hall and Welsh (1984) with \u03b3 > 0. This corresponds to a function \u03c6, as defined in (2.1), with expansion\n\u03c6(s) = \u03b3 + cs\u03c4 + o(s\u03c4 ), s \u2193 0,(7.6) for some \u03c4 > 0. Then we get\n\u03b3 K1 h \u2212 \u03b3 K2h = cK1,K2h\u03c4 + o(h\u03c4 ),(7.7)\nwhere cK1,K2 only depends on the kernels K1 and K2, the constant c in (7.6) and possibly the parameters \u03b3 and \u03c4 . So we get the following expansion for \u03b4\u2217n1,h in (7.5):\n\u03b4\u2217n1,h = cK1,K2h\u03c4 + D\u2217n1,h\u221a n1h + o(h\u03c4 ) + Op(1/\u221anh )\n(7.8)\n= cK1,K2h\u03c4 + D\u2217n1,h\u221a n1h + o(h\u03c4 ) + op(1/\u221an1h ),\nusing n1/n \u2192 0 in the last step. Comparing (7.8) with (7.4) and (7.7) means that the bootstrap mean squared error of \u03b4\u2217n1,h has the same asymptotic behavior as the real mean squared error MSE(\u03b3\u0302 K1n1,h \u2212 \u03b3\u0302 K2 n1,h\n), implying that the minimizer h\u2217(n1), as defined in (7.2), will (in probability) be asymptotically equivalent to the minimizer hK1\u2212K2n1,opt of MSE(\u03b3\u0302 K1 n1,h\n\u2212 \u03b3\u0302 K2n1,h). To illustrate the procedure for finding the optimal h in the model (7.6), we present only the computations for the positive part \u03b3\u0302 (pos)n,h of our estimator which is the CDM estimator proposed in Cs\u00f6rgo\u030b, Deheuvels and Mason (1985). The procedure for the full estimator is similar, but just involves more constants. It turns out that in the model (7.6) we only have to perform the bootstrap samples of size n1 and we do not need to perform the second experiment with the smaller sample size n2 = n21/n .\nIf we write \u03c4 = \u03b3\u03c1, then, for the model (7.6), similar to the expressions obtained in Theorems 4.1 and 5.1, the asymptotic bias of \u03b3\u0302 (pos)n,h is given by \u00b51(\u03c4 )ch \u03c4 , where\n\u00b51(\u03c4 ) = \u2212\u03c4\u03bb\u03c4,0 = \u2212\u03c4 \u222b 1\n0 u\u03c4K(u)du,"
        },
        {
            "heading": "1990 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "and the limiting variance of \u03b3\u0302 (pos)n,h is given by\n\u03c3 2 K\u0303\n= \u03b3 2 \u222b 1\n0 K\u0303(u)2 du,\nusing the notation introduced in Theorems 4.1 and 5.1. Minimizing the expression\nMSE(\u03b3\u0302 Kn,h) = \u03c3 2 K\u0303\nnh + \u00b51(\u03c4 )2c2h2\u03c4\nas a function of h yields the theoretically (asymptotically) optimal h for sample size n,\nhKn,opt = {\n\u03c3 2 K\u0303\n2c2\u03c4 \u00b51(\u03c4 )2n\n}1/(1+2\u03c4) .(7.9)\nFor the biweight kernel K1, we get \u03c3 2K\u03031 = 10\u03b3 2/7 and \u00b51(\u03c4 ) = \u221215\u03c4/((1+\u03c4 )(3+ \u03c4 )(5 + \u03c4 )), so that (7.9) becomes\nh K1 n,opt =\n{ \u03b3 2(1 + \u03c4 )2(3 + \u03c4 )2(5 + \u03c4 )2\n315c2\u03c4 3n\n}1/(1+2\u03c4) .(7.10)\nNow, if we do the same computation for the difference of two kernels K1 and K2, minimizing MSE(\u03b3\u0302 K1n1,h \u2212 \u03b3\u0302 K2 n1,h ) as a function of h, we get, for the asymptotically optimal hK1\u2212K2n,opt ,\nh K1\u2212K2 n,opt =\n{ \u03c3 2 K\u03031\u2212K\u03032\n2c2\u03c4\u00b51(\u03c4 )2n\n}1/(1+2\u03c4) ,\nwhere\n\u03c3 2 K\u03031\u2212K\u03032 = \u03b3\n2 \u222b 1\n0\n{ K\u03031(u) \u2212 K\u03032(u)}2 du\nand\n\u00b51(\u03c4 ) = \u2212\u03c4 \u222b 1\n0 u\u03c4 {K1(u) \u2212 K2(u)}du.\nFor the biweight kernel K1 and the triweight kernel K2, we get \u03c3 2K\u03031\u2212K\u03032 = 30\u03b3 2/1001 and \u00b51(\u03c4 ) = \u221215\u03c4 2/((1 + \u03c4 )(3 + \u03c4 )(5 + \u03c4 )(7 + \u03c4 )), implying\nh K1\u2212K2 n,opt =\n{ \u03b3 2(1 + \u03c4 )2(3 + \u03c4 )2(5 + \u03c4 )2(7 + \u03c4 )2\n15015c2\u03c4 5n\n}1/(1+2\u03c4) .(7.11)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1991\nCombining (7.10) and (7.11) yields\nh K1 n,opt =\n{ 143\u03c4 2 3(7 + \u03c4 )2 }1/(1+2\u03c4) h K1\u2212K2 n,opt .(7.12)\nApplying (7.11) to sample sizes n and n1 gives\nh K1\u2212K2 n,opt =\n( n1\nn\n)1/(1+2\u03c4) h\nK1\u2212K2 n1,opt .(7.13)\nCombining this with (7.12), we find\nh K1 n,opt =\n{ 143n1\u03c4 2 3n(7 + \u03c4 )2 }1/(1+2\u03c4) h K1\u2212K2 n1,opt .(7.14)\nHence, if we have a bootstrap estimate of hK1\u2212K2n1,opt , the last step is the estimation of \u03c4 . Draisma, de Haan, Peng and Pereira (1999) propose the following estimator (here interpreted for our situation):\n\u03c4\u0302 = \u2212 logn1 + logh \u2217(n1)\n2 logh\u2217(n1) ,(7.15)\nwhere h\u2217(n1) is the bootstrap estimate of hK1\u2212K2n1,opt , as defined in (7.2). Since, indeed,\nlogn1 + log h\u2217(n1) \u22122 logh\u2217(n1) = logn1 \u2212 {1 + 2\u03c4 }\u22121 log n1 + Op(1) 2{1 + 2\u03c4 }\u22121 logn1 + Op(1)\n= \u03c4 + Op ( 1\nlogn1\n) ,\nthis is also a consistent estimate of \u03c4 in our situation. According to (7.14), in the model (7.6) we only need a bootstrap estimate for h K1\u2212K2 n1,opt . Nevertheless, if we apply (7.11) to sample sizes n1 and n2 = n21/n, similar to (7.13) we find that( n1\nn\n)1/(1+2\u03c4) = ( n2\nn1\n)1/(1+2\u03c4) = h K1\u2212K2 n1,opt\nh K1\u2212K2 n2,opt\n,\nso that from (7.14) we find\nh K1 n,opt =\n{ 143\u03c4 2 3(7 + \u03c4 )2 }1/(1+2\u03c4) {hK1\u2212K2n1,opt }2\nh K1\u2212K2 n2,opt\n."
        },
        {
            "heading": "1992 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": "Plugging in (7.15) results in the following expression for the bootstrap estimate for hK1n,opt: { 143{logn1 + log h\u2217(n1)}2 3{logn1 \u2212 13 logh\u2217(n1)}2 }\u2212 logh\u2217(n1)/ logn1 (h\u2217(n1))2 h\u2217(n2) ,(7.16)\nwhich is of the form (7.3) and is similar to the expressions in Draisma, de Haan, Peng and Pereira (1999) and Danielsson, de Haan, Peng and de Vries (2001).\nNote, however, that if we assume the model (7.6), we do not have to do the second bootstrap experiment with bootstrap sample size n2, and that, using (7.14), we can estimate the asymptotically optimal bandwidth by\n{ 143n1{logn1 + log h\u2217(n1)}2 3n{logn1 \u2212 13 logh\u2217(n1)}2 }\u2212 logh\u2217(n1)/ logn1 h\u2217(n1),(7.17)\nonly involving the bandwidth h\u2217(n1). Our simulation experiments showed that the bootstrap method worked rather well for determining the asymptotically optimal bandwidth hK1\u2212K2n1,opt and that the bottleneck of the whole procedure is the estimation of \u03c4 (which is also the case for the approach using moment estimators, although our impression is that there the bootstrap method seems to work somewhat less well, possibly as a result of the nonsmooth dependence on the sample fraction). The estimator (7.15) may have a large bias, because the estimate can be rather far from its target value, even when evaluated at the theoretically optimal bandwidth. Moreover, it only converges at logarithmic speed.\nWe therefore propose another estimate, which is more in line with the methods of the present paper. An estimate of the parameter \u03c4 can be based on the following relation which holds, at least in a (Schwarz) distributional sense, in the model (7.6):\n\u03c4 = 1 + lim s\u21930 s1+\u03b1\u03c6\u2032\u2032(s) s\u03b1\u03c6\u2032(s) , \u03b1 \u2265 0.(7.18)\nHere we introduce only the differentiability of \u03c6 for the motivation of our estimator; the proof of its consistency does not require the differentiability of \u03c6, just as in our estimate of \u03b3 . So we can estimate \u03c4 in a similar way as we estimated the possibly negative \u03b3 in the general case, that is, by a ratio of kernel estimators. The proposed estimator for \u03c4 is\n\u03c4\u0302n,h = 1 + p\u0302\n(2) n,h p\u0302 (1) n,h ,(7.19)\nKERNEL ESTIMATORS FOR \u03b3 \u2208 R 1993\nwhere p\u0302(i)n,h, i = 1,2, are defined for some \u03b1 > 0 by\np\u0302 (1) n,h = \u222b h 0 u d du ( u\u03b1Kh(u) ) d logQn(1 \u2212 u)\n(7.20)\n= \u2212 n\u22121\u2211 i=1 [ u d du ( u\u03b1Kh(u) )] u=i/n { log X(n\u2212i+1) \u2212 logX(n\u2212i)}\nand\np\u0302 (2) n,h = \u2212 \u222b h 0 u d2 du2 ( u1+\u03b1Kh(u) ) d logQn(1 \u2212 u)\n(7.21)\n= n\u22121\u2211 i=1 [ u d2 du2 ( u1+\u03b1Kh(u) )] u=i/n { logX(n\u2212i+1) \u2212 logX(n\u2212i)},\nrespectively. Note the similarity to the definitions of q\u0302(1)n,h and q\u0302 (2) n,h by (2.6) and (2.8), but also note that we have to take one extra derivative to get hold of the second-order parameter \u03c4 . As in the definition of q\u0302(i)n,h, we have some freedom in the choice of the parameter \u03b1 in these expressions.\nEstimator (7.19) will be asymptotically normal and will have polynomial rate of convergence under conditions that are similar to conditions proposed in the recent literature on moment estimators of \u03c4 in, for example, Gomes, de Haan and Peng (2003) and Fraga Alves, de Haan and Lin (2003) (our \u03c4 is \u2212\u03c1 in their notation), in contrast with the estimator (7.15), which only has a logarithmic speed of convergence. Simulations show that the difference in smoothness of the dependence on the bandwidth of the estimator (7.19) with respect to the dependence on the sample fraction of the moment estimators, proposed in these papers, is even more striking than the corresponding difference in smoothness in the estimation of \u03b3 between moment estimators and the kernel estimators, discussed above. Since the estimator (7.19) in a sense deals with a third derivative of the logarithm of the quantile function (although, as noted above, we do not need to assume differentiability), it comes as no surprise that the optimal bandwidths for \u03c4\u0302n,h are larger than those for \u03b3\u0302n,h. This is in accordance with the findings reported in Gomes, de Haan and Peng (2003) and Fraga Alves, de Haan and Lin (2003) for their moment estimators of \u03c4 .\nAs mentioned in the beginning of this section, a more detailed treatment of the automatic bandwidth choice for the full kernel estimators, introduced in the present paper, will be given in a sequel to the present paper. The research on automatic selection of sample fractions for moment-type estimators is rather intensive at present. We have followed the bootstrap approach for our kernel estimators, but we should mention that for moment-type estimators other (more or less) automatic methods have also been suggested [see, e.g., Drees and Kaufmann (1998) and Matthys and Beirlant (2000)]."
        },
        {
            "heading": "1994 P. GROENEBOOM, H. P. LOPUHA\u00c4 AND P. P. DE WOLF",
            "text": ""
        },
        {
            "heading": "P. GROENEBOOM",
            "text": "H. P. LOPUHA\u00c4 DEPARTMENT OF MATHEMATICS FACULTY OF INFORMATION TECHNOLOGY\nAND SYSTEMS DELFT UNIVERSITY OF TECHNOLOGY MEKELWEG 4 2628 CD DELFT THE NETHERLANDS E-MAIL: p.groeneboom@its.tudelft.nl\nh.p.lopuhaa@its.tudelft.nl\nP. P. DE WOLF STATISTICS NETHERLANDS THE NETHERLANDS E-MAIL: pwof@cbs.nl"
        }
    ],
    "title": "KERNEL-TYPE ESTIMATORS FOR THE EXTREME VALUE INDEX BY P. GROENEBOOM,",
    "year": 2004
}