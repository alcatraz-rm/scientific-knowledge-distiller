[
    {
        "authors": [
            "Jooyoung Kim",
            "Sojung Go",
            "Kyoung Jin Noh",
            "Sang Jun Park",
            "Soochahn Lee"
        ],
        "title": "Fully Leveraging Deep Learning Methods for Constructing Retinal Fundus Photomontages",
        "publication_date": "2021-02-16 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "MDPI AG",
        "volume": "",
        "doi": "10.3390/app11041754",
        "urls": [
            "https://web.archive.org/web/20210429010227/https://res.mdpi.com/d_attachment/applsci/applsci-11-01754/article_deploy/applsci-11-01754-v2.pdf"
        ],
        "id": "id4743187891299626722",
        "abstract": "Retinal photomontages, which are constructed by aligning and integrating multiple fundus images, are useful in diagnosing retinal diseases affecting peripheral retina. We present a novel framework for constructing retinal photomontages that fully leverage recent deep learning methods. Deep learning based object detection is used to define the order of image registration and blending. Deep learning based vessel segmentation is used to enhance image texture to improve registration performance within a two step image registration framework comprising rigid and non-rigid registration. Experimental evaluation demonstrates the robustness of our montage construction method with an increased amount of successfully integrated images as well as reduction of image artifacts.",
        "versions": [],
        "rank": 0
    },
    {
        "authors": [
            "M. Surucu",
            "A. Woerner",
            "J. Roeske"
        ],
        "title": "TU-H-CAMPUS-JeP1-05: Dose Deformation Error Associated with Deformable Image Registration Pathways.",
        "publication_date": "2016-06-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Medical physics",
        "volume": "43 6",
        "doi": "10.1118/1.4957673",
        "urls": [
            "https://www.semanticscholar.org/paper/01f8f8e7fdcd298e0c77f7f7053747c7857a4248"
        ],
        "id": "id-8431052845029952508",
        "abstract": "PURPOSE\nTo evaluate errors associated with using different deformable image registration (DIR) pathways to deform dose from planning CT (pCT) to cone-beam CT (CBCT).\n\n\nMETHODS\nDeforming dose is controversial because of the lack of quality assurance tools. We previously proposed a novel metric to evaluate dose deformation error (DDE) by warping dose information using two methods, via dose and contour deformation. First, isodose lines of the pCT were converted into structures and then deformed to the CBCT using an image based deformation map (dose/structure/deform). Alternatively, the dose matrix from the pCT was deformed to CBCT using the same deformation map, and then the same isodose lines of the deformed dose were converted into structures (dose/deform/structure). The doses corresponding to each structure were queried from the deformed dose and full-width-half-maximums were used to evaluate the dose dispersion. The difference between the FWHM of each isodose level structure is defined as the DDE. Three head-and-neck cancer patients were identified. For each patient, two DIRs were performed between the pCT and CBCT, either deforming pCT-to-CBCT or CBCT-to-pCT. We evaluated the errors associated by using either of these pathways to deform dose. A commercially available, Demons based DIR was used for this study, and 10 isodose levels (20% to 105%) were used to evaluate the errors in various dose levels.\n\n\nRESULTS\nThe prescription dose for all patients was 70 Gy. The mean DDE for CT-to-CBCT deformation was 1.0 Gy (range: 0.3-2.0 Gy) and this was increased to 4.3 Gy (range: 1.5-6.4 Gy) for CBCT-to-CT deformation. The mean increase in DDE between the two deformations was 3.3 Gy (range: 1.0-5.4 Gy).\n\n\nCONCLUSION\nThe proposed DDF was used to quantitatively estimate dose deformation errors caused by different pathways to perform DIR. Deforming dose using CBCT-to-CT deformation produced greater error than CT-to-CBCT deformation.",
        "versions": [
            {
                "year": 2016,
                "source": "SupportedSources.CROSSREF",
                "title": "TU-H-CAMPUS-JeP1-05: Dose Deformation Error Associated with Deformable Image Registration Pathways",
                "journal": "",
                "urls": [
                    "https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1118%2F1.4957673",
                    "https://onlinelibrary.wiley.com/doi/full/10.1118/1.4957673",
                    "http://dx.doi.org/10.1118/1.4957673"
                ],
                "doi": "10.1118/1.4957673",
                "publication_date": "2016-01-01 00:00:00"
            }
        ],
        "rank": 1
    },
    {
        "authors": [
            "Gholipour, Ali",
            "Karimi, Davood",
            "Ouaalam, Abdelhakim",
            "Rollins, Caitlin K.",
            "Velasco-Annis, Clemente"
        ],
        "title": "Learning to segment fetal brain tissue from noisy annotations",
        "publication_date": "2022-03-25 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "http://arxiv.org/abs/2203.14962"
        ],
        "id": "id8312439568927007117",
        "abstract": "Automatic fetal brain tissue segmentation can enhance the quantitative\nassessment of brain development at this critical stage. Deep learning methods\nrepresent the state of the art in medical image segmentation and have also\nachieved impressive results in brain segmentation. However, effective training\nof a deep learning model to perform this task requires a large number of\ntraining images to represent the rapid development of the transient fetal brain\nstructures. On the other hand, manual multi-label segmentation of a large\nnumber of 3D images is prohibitive. To address this challenge, we segmented 272\ntraining images, covering 19-39 gestational weeks, using an automatic\nmulti-atlas segmentation strategy based on deformable registration and\nprobabilistic atlas fusion, and manually corrected large errors in those\nsegmentations. Since this process generated a large training dataset with noisy\nsegmentations, we developed a novel label smoothing procedure and a loss\nfunction to train a deep learning model with smoothed noisy segmentations. Our\nproposed methods properly account for the uncertainty in tissue boundaries. We\nevaluated our method on 23 manually-segmented test images of a separate set of\nfetuses. Results show that our method achieves an average Dice similarity\ncoefficient of 0.893 and 0.916 for the transient structures of younger and\nolder fetuses, respectively. Our method generated results that were\nsignificantly more accurate than several state-of-the-art methods including\nnnU-Net that achieved the closest results to our method. Our trained model can\nserve as a valuable tool to enhance the accuracy and reproducibility of fetal\nbrain analysis in MRI",
        "versions": [
            {
                "year": 2022,
                "source": "SupportedSources.ARXIV",
                "title": "Learning to segment fetal brain tissue from noisy annotations",
                "journal": null,
                "urls": [
                    "http://arxiv.org/pdf/2203.14962v2",
                    "http://arxiv.org/abs/2203.14962v2",
                    "http://arxiv.org/pdf/2203.14962v2"
                ],
                "doi": "",
                "publication_date": "2022-03-25 21:22:24+00:00"
            }
        ],
        "rank": 2
    },
    {
        "authors": [
            "Wangbin Ding",
            "Lei Li",
            "Xiahai Zhuang",
            "Liqin Huang"
        ],
        "title": "Unsupervised Multi-Modality Registration Network based on Spatially Encoded Gradient Information",
        "publication_date": "2021-08-29 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://web.archive.org/web/20210902002445/https://arxiv.org/pdf/2105.07392v3.pdf"
        ],
        "id": "id-8707707340401877043",
        "abstract": "Multi-modality medical images can provide relevant or complementary information for a target (organ, tumor or tissue). Registering multi-modality images to a common space can fuse these comprehensive information, and bring convenience for clinical application. Recently, neural networks have been widely investigated to boost registration methods. However, it is still challenging to develop a multi-modality registration network due to the lack of robust criteria for network training. In this work, we propose a multi-modality registration network (MMRegNet), which can perform registration between multi-modality images. Meanwhile, we present spatially encoded gradient information to train MMRegNet in an unsupervised manner. The proposed network was evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising performance for left ventricle cardiac registration tasks. Meanwhile, to demonstrate the versatility of MMRegNet, we further evaluate the method with a liver dataset from CHAOS 2019. Source code will be released publicly[https://github.com/NanYoMy/mmregnet] once the manuscript is accepted.",
        "versions": [
            {
                "year": 2021,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Unsupervised Multi-modality Registration Network Based on Spatially Encoded Gradient Information",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/ff4fcb9af7a3a2aa843d883475802df236c854f5",
                    "http://arxiv.org/pdf/2105.07392"
                ],
                "doi": "10.1007/978-3-030-93722-5_17",
                "publication_date": "2021-05-16 00:00:00"
            },
            {
                "year": 2021,
                "source": "SupportedSources.ARXIV",
                "title": "Unsupervised Multi-Modality Registration Network based on Spatially Encoded Gradient Information",
                "journal": null,
                "urls": [
                    "http://arxiv.org/pdf/2105.07392v3",
                    "http://arxiv.org/abs/2105.07392v3",
                    "http://arxiv.org/pdf/2105.07392v3"
                ],
                "doi": "",
                "publication_date": "2021-05-16 09:47:42+00:00"
            }
        ],
        "rank": 3
    },
    {
        "authors": [
            "Jiyun Li",
            "Xiaomeng Wang",
            "Chen Qian"
        ],
        "title": "Mammography Registration for Unsupervised Learning Based on CC and MLO Views",
        "publication_date": "2020-06-26 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1145/3430199.3430238",
        "urls": [
            "https://www.semanticscholar.org/paper/48f988df41b905a7ebf78f65a517017bfe4218c9"
        ],
        "id": "id-1760058201089218827",
        "abstract": "Mammography image usually contains two views in different orientations---Cranial Caudal (CC) and Mediolateral Oblique (MLO). In clinical decision making, the location of the lesions on the CC and MLO views are usually different. And the shape of breast varies greatly among patients, therefore, two views are necessary for evaluating the information in a comprehensively manner. In this paper, we propose an unsupervised registration algorithm based on CC and MLO views of mammography, which learns the deformation function through a Convolutional Neural Network (CNN). This function maps the input image to the corresponding deformation field and generates an image with the same shape as the template image after deformation, so that the doctor can better observe the two views. According to the radiologist's assessment, our work can contribute to medical image analysis and processing while providing novel guidance in learning-based registration and its applications.",
        "versions": [],
        "rank": 4
    },
    {
        "authors": [
            "Fierrez, Julian",
            "Loyola-Gonz\u00e1lez, Octavio",
            "Medina-P\u00e9rez, Miguel Angel",
            "Mehnert, Emilio Francisco Ferreira",
            "Monroy, Ra\u00fal",
            "Morales, Aythami"
        ],
        "title": "Impact of minutiae errors in latent fingerprint identification: assessment and prediction",
        "publication_date": "2021-05-04 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "Applied Sciences",
        "volume": "",
        "doi": "10.3390/app11094187",
        "urls": [
            "https://repositorio.uam.es/bitstream/10486/699600/1/impact_loyola_applsci_2021.pdf"
        ],
        "id": "id6972775931705572541",
        "abstract": "We study the impact of minutiae errors in the performance of latent fingerprint identification systems. We perform several experiments in which we remove ground-truth minutiae from latent fingerprints and evaluate the effects on matching score and rank-n identification using two different matchers and the popular NIST SD27 dataset. We observe how missing even one minutia from a fingerprint can have a significant negative impact on the identification performance. Our experimental results show that a fingerprint which has a top rank can be demoted to a bottom rank when two or more minutiae are missed. From our experimental results, we have noticed that some minutiae are more critical than others to correctly identify a latent fingerprint. Based on this finding, we have created a dataset to train several machine learning models trying to predict the impact of each minutia in the matching score of a fingerprint identification system. Finally, our best-trained model can successfully predict if a minutia will increase or decrease the matching score of a latent fingerprintThis research was partly supported by the National Council of Science and Technology of\r\nMexico under the scholarship grants 717345 and 005438. Authors J.F. and A.M. are funded by project\r\nBIBECA (RTI2018-101248-B-I00 MINECO/FEDER) and TRESPASS-ETN (MSCA-ITN-2019-860813",
        "versions": [],
        "rank": 5
    },
    {
        "authors": [
            "R. Camassa",
            "Dongyang Kuang",
            "Long Lee"
        ],
        "title": "A Geodesic Landmark Shooting Algorithm for Template Matching and Its Applications",
        "publication_date": "2017-03-15 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "SIAM J. Imaging Sci.",
        "volume": "10",
        "doi": "10.1137/15M104373X",
        "urls": [
            "https://www.semanticscholar.org/paper/1421e50b30cf0f904e9617793c3798867a6a38f9"
        ],
        "id": "id-7290725869676823064",
        "abstract": "We present an efficient landmark shooting algorithm for template matching and its applications. The novelties of the algorithm include the use of a constant matrix to update the search direction of the geodesic shooting, instead of the traditional methods of forward-backward integration for updating the gradient or Newton's optimization, and the use of a nonsmooth conic kernel for the particle system that accelerates the convergence of matching. To investigate the usage of the output quantities computed along the warping algorithm, such as the Hamiltonian metric and the momentum field, we introduce a multiscale decomposition method that separates the scales/components of the momentum and the Hamiltonian metric associated with the deformation. We numerically explore the potential of using the decomposed Hamiltonian metric and momentum vectors as input feature vectors into neural networks for clustering/classification analysis. The results of our numerical experiments are encouraging.",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.OPENALEX",
                "title": "A Geodesic Landmark Shooting Algorithm for Template Matching and Its Applications",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2595196388",
                    "https://doi.org/10.1137/15m104373x"
                ],
                "doi": "10.1137/15m104373x",
                "publication_date": "2017-03-15 00:00:00"
            }
        ],
        "rank": 6
    },
    {
        "authors": [
            "I. Grigorescu",
            "L. Cordero-Grande",
            "A. Edwards",
            "J. Hajnal",
            "M. Modat",
            "M. Deprez"
        ],
        "title": "Investigating Image Registration Impact on Preterm Birth Classification: An Interpretable Deep Learning Approach",
        "publication_date": "2019-10-17 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-3-030-32875-7_12",
        "urls": [
            "https://www.semanticscholar.org/paper/38436b62bffa2f384095d0fbf151511deaff0b11"
        ],
        "id": "id-2407810805497638140",
        "abstract": null,
        "versions": [],
        "rank": 7
    },
    {
        "authors": [
            "Bauer, Stefan",
            "Iftekharuddin, Khan M.",
            "Jakab, Andras",
            "Kalpathy-Cramer, Jayashree",
            "Menze, Bjoern H.",
            "Reza, Syed M.S."
        ],
        "title": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)",
        "publication_date": "2015-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/217291418.pdf"
        ],
        "id": "id5029651591022265294",
        "abstract": "In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low-and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource",
        "versions": [],
        "rank": 8
    },
    {
        "authors": [
            "Zachary Burns",
            "Zhaowei Liu"
        ],
        "title": "Untrained, physics-informed neural networks for structured illumination microscopy",
        "publication_date": "2022-07-15 19:02:07+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "10.1364/OE.476781",
        "urls": [
            "http://arxiv.org/pdf/2207.07705v1",
            "http://dx.doi.org/10.1364/OE.476781",
            "http://arxiv.org/abs/2207.07705v1",
            "http://arxiv.org/pdf/2207.07705v1"
        ],
        "id": "id-7915373871109500632",
        "abstract": "In recent years there has been great interest in using deep neural networks\n(DNN) for super-resolution image reconstruction including for structured\nillumination microscopy (SIM). While these methods have shown very promising\nresults, they all rely on data-driven, supervised training strategies that need\na large number of ground truth images, which is experimentally difficult to\nrealize. For SIM imaging, there exists a need for a flexible, general, and\nopen-source reconstruction method that can be readily adapted to different\nforms of structured illumination. We demonstrate that we can combine a deep\nneural network with the forward model of the structured illumination process to\nreconstruct sub-diffraction images without training data. The resulting\nphysics-informed neural network (PINN) can be optimized on a single set of\ndiffraction limited sub-images and thus doesn't require any training set. We\nshow with simulated and experimental data that this PINN can be applied to a\nwide variety of SIM methods by simply changing the known illumination patterns\nused in the loss function and can achieve resolution improvements that match\nwell with theoretical expectations.",
        "versions": [],
        "rank": 9
    },
    {
        "authors": [
            "Brandon Chan",
            "J. Rudan",
            "P. Mousavi",
            "M. Kunz"
        ],
        "title": "Intraoperative integration of structured light scanning for automatic tissue classification: a feasibility study",
        "publication_date": "2020-03-06 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "International Journal of Computer Assisted Radiology and Surgery",
        "volume": "15",
        "doi": "10.1007/s11548-020-02129-8",
        "urls": [
            "https://www.semanticscholar.org/paper/b2d8210f53387a08dc64987358204a8a3bc419ad"
        ],
        "id": "id-7872617380983224616",
        "abstract": null,
        "versions": [],
        "rank": 10
    },
    {
        "authors": [
            "Paarth Bir",
            "V. Balas"
        ],
        "title": "A Review on Medical Image Analysis with Convolutional Neural Networks",
        "publication_date": "2020-10-02 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/GUCON48875.2020.9231203",
        "urls": [
            "https://www.semanticscholar.org/paper/5e8274faf73411d53d7078432dd6522c9066af51"
        ],
        "id": "id-1699943528392340471",
        "abstract": "Over the last few years, deep learning has grown rapidly from a promising to a viable option to analyze medical images. With an increase in use of medical imaging for diagnosis and treatment, the field offers a significant potential for research. A major advantage offered by deep learning is using large amounts of data to avoid tedious hand-crafting of features which requires extensive domain knowledge. This review introduces a few popular algorithms using Convolutional Neural Networks(CNNs) being used in the field along with their applications: Classification, Detection, Segmentation, Registration and Image Enhancement. The paper further provides some useful resources on some of the most promising anatomical areas of application in medical image analysis with Convolutional Neural Networks: brain, breast, chest, eye and skin.",
        "versions": [],
        "rank": 11
    },
    {
        "authors": [
            "Kaiyue Shen",
            "Chen Guo",
            "Manuel Kaufmann",
            "Juan Jose Zarate",
            "Julien Valentin",
            "Jie Song",
            "Otmar Hilliges"
        ],
        "title": "X-Avatar: Expressive Human Avatars",
        "publication_date": "2023-03-09 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://web.archive.org/web/20230311165042/https://arxiv.org/pdf/2303.04805v2.pdf"
        ],
        "id": "id1680388001837477578",
        "abstract": "We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and appearance in a holistic fashion and can be learned from either full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allowing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we propose novel part-aware sampling and initialization strategies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training despite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, geometry and the normals of the deformed surface. We show experimentally that our method outperforms strong baselines in both data domains both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, containing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames.",
        "versions": [],
        "rank": 12
    },
    {
        "authors": [
            "R. Srikanchana",
            "K. Woods",
            "J. Xuan",
            "C. Nguyen",
            "Y. Wang"
        ],
        "title": "Non-rigid image registration by neural computations",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/NNSP.2001.943145",
        "urls": [
            "https://www.semanticscholar.org/paper/5445ebff40012cf0f825338d7c6eeff71a91b09a"
        ],
        "id": "id-1204832246619975985",
        "abstract": "This paper describes a neural computation based nonrigid registration methodology using multiple rigid transforms, in a piece-wise fashion, to model the registration process between images in a sequence. The registration methodology is a hybrid approach that combines registration without exact point correspondence via multi-object principal axes, and registration with point correspondence via polynomial transform. Neural computation is used to combine the derived individual principal axes solutions for each object in a committee machine formulation and to obtain the polynomial transform based on extracted control points using a multi layer perceptrons (MLP). Three examples are presented to demonstrate the techniques involved in the process. The first example uses four Gaussian clusters and focuses on the combination of the multiple transforms into a composite transform using finite mixture modeling techniques. The next examples present the complete process for prostate cancer registration and breast sequence analysis respectively. To verify performance, the results are compared to non-neural based implementations and other existing registration methods.",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.CROSSREF",
                "title": "Non-rigid image registration by neural computations",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx5/7505/20420/00943145.pdf?arnumber=943145",
                    "http://dx.doi.org/10.1109/nnsp.2001.943145"
                ],
                "doi": "10.1109/nnsp.2001.943145",
                "publication_date": "None"
            }
        ],
        "rank": 13
    },
    {
        "authors": [
            "N. Maffei",
            "G. Guidi",
            "C. Vecchi",
            "G. Baldazzi",
            "T. Costi"
        ],
        "title": "SU-E-J-96: Predictive Neural Network for Parotid Glands Deformation Using IGRT and Dose Warping Systems.",
        "publication_date": "2014-06-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Medical physics",
        "volume": "41 6",
        "doi": "10.1118/1.4888148",
        "urls": [
            "https://www.semanticscholar.org/paper/f6a78899b6882ca6ab7917a42fbb7c7e833bf802"
        ],
        "id": "id-5964797113255827173",
        "abstract": "PURPOSE\nTumor shrinkage and modification of patient anatomy may occur within the weeks of therapy. Dose warping algorithm and IGRT can usefully determinate morphological variations occurring during treatments and predict possible challenge or requirement of re-planning. Using metaanalysis and developing Predictive Neural Network (PNN) is possible to collect information for perspective treatment evaluation of inappropriate dose delivery or to re-planning care course.\n\n\nMETHODS\n23 H' N patients, treated using Tomotherapy Unit, were post-processed using deformable hybrid algorithms to generate contours evolution throughout the course. 750 MVCT were elaborated focusing on parotids. For each deformable image registration were generated deformable ROIs and re-mapped to dose grid to accumulate dose. A nomogram was developed to evaluate Volume (V) and Dose (D) variations. Using Moving Average (MA) function and PNN were identified the weeks when re-plan should be predictable.\n\n\nRESULTS\nThe weekly nomogram provided the percentage of patients affected by V and D deviation respect to the first day of treatment. During 1st week of treatment 95% of patients have a \u0394V=10% and \u0394D=2%. In the 6th week, only 70% of cases remain inside that range. The PNN using Cluster Analysis and Support Vector Machines, highlighted the requirement of warping methods in clinical practice. Validated by 8 patients test cases, PNN allow to classify 2 statistical cluster related to original plan and re-plan requirements. MA with period of 3 and 5 days, predict days of treatment where to locate physics replan. Best statistical benefits can be achieved with evaluation in the 4th week.\n\n\nCONCLUSION\nBody morphing and organ motion during therapy can affect to the dose distribution and induce unexpected or late toxicities. Hybrid deformable registration allows to study the time to re-plan or to adapt treatments. Monitoring large patient's database during treatments, thereby rising accuracy in nomograms, PNN and reproducibility of treatments. The research is partially co-funded by the MoH (GR-2010-2318757) and Tecnologie Avanzate S.r.1.(Italy).",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.OPENALEX",
                "title": "SU-E-J-96: Predictive Neural Network for Parotid Glands Deformation Using IGRT and Dose Warping Systems",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2052696148",
                    "https://doi.org/10.1118/1.4888148"
                ],
                "doi": "10.1118/1.4888148",
                "publication_date": "2014-06-01 00:00:00"
            }
        ],
        "rank": 14
    },
    {
        "authors": [
            "Sahar Abdolmaleki",
            "M. S. Abadeh"
        ],
        "title": "Brain MR Image Classification for ADHD Diagnosis Using Deep Neural Networks",
        "publication_date": "2020-02-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/MVIP49855.2020.9116877",
        "urls": [
            "https://www.semanticscholar.org/paper/6d522690e1efa486d151aa4c788775cd1858d476"
        ],
        "id": "id-8731331347890871557",
        "abstract": "Attention-deficit/hyperactivity disorder (ADHD) is one of the most prevalent neurodevelopmental disorder in childhood and adolescence. ADHD diagnosis currently includes psychological tests and depends on ratings of behavioral symptoms, which can be unreliable. Thus, an objective diagnostic tool based on non-invasive imaging can improve the understanding and diagnosis of ADHD. The purpose of this study is classifying brain images by using Artificial Intelligence methods such as clinical decision support system for the diagnosis of ADHD. For this purpose and according to a medical imaging classification system, firstly, image pre-processing is done. Then, a deep multi-modal 3D CNN is trained on GM from structural and fALFF from functional MRI using ADHD-200 training dataset. Finally, with the intention of classifying the extracted features, early and late fusion schemes are employed, and the output scores are classified with the SVM, KNN and LDA algorithms. The evaluation of the proposed approach on the ADHD-200 testing dataset revealed that the presence of personal characteristics alone increased the classification accuracy by 3.79%. In addition, using a combination of early, late fusion and personal characteristics together improved the accuracy of the classification by 5.84%. Among the three classifiers LDA showed better results and achieved a classification accuracy of 74.93%. The comparison of results showed that the combination of early and late fusion as well as considering personal characteristics has a significant effect on enhancing classification accuracy. As a result of this, the reliability of this medical decision support system is increased.",
        "versions": [
            {
                "year": 2020,
                "source": "SupportedSources.CROSSREF",
                "title": "Brain MR Image Classification for ADHD Diagnosis Using Deep Neural Networks",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx7/9113418/9116868/09116877.pdf?arnumber=9116877",
                    "http://dx.doi.org/10.1109/mvip49855.2020.9116877"
                ],
                "doi": "10.1109/mvip49855.2020.9116877",
                "publication_date": "2020-01-01 00:00:00"
            }
        ],
        "rank": 15
    },
    {
        "authors": [
            "Dingna Duan",
            "Shunren Xia",
            "Yu Song Meng",
            "Li Wang",
            "Weili Lin",
            "John H. Gilmore",
            "Dinggang Shen",
            "G. Li"
        ],
        "title": "Exploring Gyral Patterns of Infant Cortical Folding Based on Multi-view Curvature Information",
        "publication_date": "2017-09-10 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-3-319-66182-7_2",
        "urls": [
            "https://openalex.org/W2752199675",
            "https://doi.org/10.1007/978-3-319-66182-7_2",
            "https://europepmc.org/articles/pmc5674991?pdf=render"
        ],
        "id": "id1102904987501457738",
        "abstract": "",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Exploring Gyral Patterns of Infant Cortical Folding Based on Multi-view Curvature Information",
                "journal": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "urls": [
                    "https://www.semanticscholar.org/paper/2e220ff60ae77533cdcc41dc5351741ffb221759",
                    "https://europepmc.org/articles/pmc5674991?pdf=render"
                ],
                "doi": "10.1007/978-3-319-66182-7_2",
                "publication_date": "2017-09-10 00:00:00"
            }
        ],
        "rank": 16
    },
    {
        "authors": [
            "Ravi Bansal",
            "Xuejun Hao",
            "Bradley S. Peterson"
        ],
        "title": "Morphological covariance in anatomical MRI scans can identify discrete neural pathways in the brain and their disturbances in persons with neuropsychiatric disorders",
        "publication_date": "2015-05-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "",
        "doi": "10.1016/j.neuroimage.2015.02.022",
        "urls": [
            "https://openalex.org/W2076182591",
            "https://doi.org/10.1016/j.neuroimage.2015.02.022"
        ],
        "id": "id8709648913581760442",
        "abstract": "",
        "versions": [
            {
                "year": 2015,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Morphological covariance in anatomical MRI scans can identify discrete neural pathways in the brain and their disturbances in persons with neuropsychiatric disorders",
                "journal": "NeuroImage",
                "urls": [
                    "https://www.semanticscholar.org/paper/9b017a06f4a557570acbf77709df877b20e4d5ce"
                ],
                "doi": "10.1016/j.neuroimage.2015.02.022",
                "publication_date": "2015-05-01 00:00:00"
            }
        ],
        "rank": 17
    },
    {
        "authors": [
            "Benjamin Chidester",
            "Minh N. Do",
            "Jian Ma"
        ],
        "title": "Rotation Equivariance and Invariance in Convolutional Neural Networks",
        "publication_date": "2018-05-31 03:13:41+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "",
        "urls": [
            "http://arxiv.org/pdf/1805.12301v1",
            "http://arxiv.org/abs/1805.12301v1",
            "http://arxiv.org/pdf/1805.12301v1"
        ],
        "id": "id-2961376229739765890",
        "abstract": "Performance of neural networks can be significantly improved by encoding\nknown invariance for particular tasks. Many image classification tasks, such as\nthose related to cellular imaging, exhibit invariance to rotation. We present a\nnovel scheme using the magnitude response of the 2D-discrete-Fourier transform\n(2D-DFT) to encode rotational invariance in neural networks, along with a new,\nefficient convolutional scheme for encoding rotational equivariance throughout\nconvolutional layers. We implemented this scheme for several image\nclassification tasks and demonstrated improved performance, in terms of\nclassification accuracy, time required to train the model, and robustness to\nhyperparameter selection, over a standard CNN and another state-of-the-art\nmethod.",
        "versions": [],
        "rank": 18
    },
    {
        "authors": [
            "Haji, Tomoki",
            "Katsuyama, Narumi",
            "Kim, Yuri",
            "Matsumoto, Kenji",
            "Miyazaki, Atsushi",
            "Nakamura, Katsuki",
            "Taira, Masato",
            "Usui, Nobuo"
        ],
        "title": "Cortical Regions Encoding Hardness Perception Modulated by Visual Information Identified by Functional Magnetic Resonance Imaging With Multivoxel Pattern Analysis",
        "publication_date": "2019-10-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "Frontiers in Systems Neuroscience",
        "volume": "",
        "doi": "10.3389/fnsys.2019.00052",
        "urls": [
            "https://core.ac.uk/download/343492801.pdf"
        ],
        "id": "id-5214224219458698830",
        "abstract": "Recent studies have revealed that hardness perception is determined by visual information along with the haptic input. This study investigated the cortical regions involved in hardness perception modulated by visual information using functional magnetic resonance imaging (fMRI) and multivoxel pattern analysis (MVPA). Twenty-two healthy participants were enrolled. They were required to place their left and right hands at the front and back, respectively, of a mirror attached to a platform placed above them while lying in a magnetic resonance scanner. In conditions SFT, MED, and HRD, one of three polyurethane foam pads of varying hardness (soft, medium, and hard, respectively) was presented to the left hand in a given trial, while only the medium pad was presented to the right hand in all trials. MED was defined as the control condition, because the visual and haptic information was congruent. During the scan, the participants were required to push the pad with the both hands while observing the reflection of the left hand and estimate the hardness of the pad perceived by the right (hidden) hand based on magnitude estimation. Behavioral results showed that the perceived hardness was significantly biased toward softer or harder in >73% of the trials in conditions SFT and HRD; we designated these trials as visually modulated (SFTvm and HRDvm, respectively). The accuracy map was calculated individually for each of the pair-wise comparisons of (SFTvm vs. MED), (HRDvm vs. MED), and (SFTvm vs. HRDvm) by a searchlight MVPA, and the cortical regions encoding the perceived hardness with visual modulation were identified by conjunction of the three accuracy maps in group analysis. The cluster was observed in the right sensory motor cortex, left anterior intraparietal sulcus (aIPS), bilateral parietal operculum (PO), and occipito-temporal cortex (OTC). Together with previous findings on such cortical regions, we conclude that the visual information of finger movements processed in the OTC may be integrated with haptic input in the left aIPS, and the subjective hardness perceived by the right hand with visual modulation may be processed in the cortical network between the left PO and aIPS",
        "versions": [],
        "rank": 19
    },
    {
        "authors": [
            "Daniel Hofman",
            "Mario Kova\u010d",
            "Martin \u017dagar"
        ],
        "title": "Framework for 4D medical data compression",
        "publication_date": "2012-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/14446613.pdf"
        ],
        "id": "id5925028578607112661",
        "abstract": "U ovom radu predlo\u017een je novi programski okvir za kompresiju \u010detvero-dimenzionalnih (4D) medicinskih podataka. Arhitektura ovog programskog okvira temelji se na razli\u010ditim procedurama i algoritmima koji detektiraju vremenske i prostorne zalihosti u ulaznim 4D medicinskim podacima. Pokret kroz vrijeme analizira se pomo\u0107u vektora pomaka koji predstavljaju ulazne parametre za neuronske mre\u017ee koje se koriste za procjenu pokreta. Kombinacijom segmentacije, pronala\u017eenja odgovaraju\u0107ih blokova i predikcijom vektora pomaka, zajedno s ekspertnim znanjem mogu\u0107e je optimirati performanse sustava. Frekvencijska svojstva se analiziraju pro\u0161irenjem wavelet transformacije na tri dimenzije. Za mirne volumetrijske objekte, mogu\u0107e je konstruirati razli\u010dite wavelet pakete s razli\u010ditim filtrima koji omogu\u0107avaju \u0161irok raspon analiza frekvencijskih zalihosti. Kombinacijom uklanjanja vremenskih i prostornih zalihosti mogu\u0107e je posti\u0107i vrlo visoke omjere kompresije.This work presents a novel framework for four-dimensional (4D) medical data compression architecture. This framework is based on different procedures and algorithms that detect time and spatial (frequency) redundancy in recorded 4D medical data. Motion in time is analyzed through the motion fields that produce input parameters for the neural network used for motion estimation. Combination of segmentation, block matching and motion field prediction along with expert knowledge are incorporated to achieve better performance. Frequency analysis is done through an extension of one dimensional wavelet transformation to three dimensions. For still volume objects different wavelet packets with different filter banks can be constructed, providing a wide range of frequency analysis. With combination of removing temporal and spatial redundancies, very high compression ratio is achieved",
        "versions": [],
        "rank": 20
    },
    {
        "authors": [
            "Nastaran Nourbakhsh Kaashki",
            "P. Hu",
            "A. Munteanu"
        ],
        "title": "Deep Learning-Based Automated Extraction of Anthropometric Measurements From a Single 3-D Scan",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Transactions on Instrumentation and Measurement",
        "volume": "70",
        "doi": "10.1109/tim.2021.3106126",
        "urls": [
            "https://www.semanticscholar.org/paper/3d7631e93d285302f02bfa819bbc6061a60d8e60"
        ],
        "id": "id4901859663741584769",
        "abstract": "The appearance of 3-D scanners, generating point clouds, has revolutionized anthropometric data collection systems and their applications. Anthropometric data are of paramount importance in several applications, including fashion design, medical diagnosis, and virtual character modeling, all of which require a fully automatic anthropometric measurement extraction method. 3-D-based methods for anthropometric measurement extraction becomes more and more popular due to their improved accuracy compared to classical image-based approaches. Existing 3-D methods can be mainly classified into two categories: landmark and template-based methods. The former is highly dependent on the estimated landmarks which are highly sensitive to noise in the input or missing data. The latter has to iteratively solve an objective function to deform a body template to fit the scan, which is time-consuming while being also sensitive to noise and missing data. In this study, we propose the first approach for automatic contact-less anthropometric measurements extraction based on deep-learning (AM-DL). A novel module dubbed multiscale EdgeConv is proposed to learn local features from point clouds at multiple scales. Multiscale EdgeConv can be directly integrated with other neural networks for various tasks, e.g., classification of point clouds. We exploit this module to design an encoder\u2013decoder architecture that learns to deform a template model to fit a given scan. The measurement values are then calculated on the deformed template model. To evaluate the proposed method, 27 female and 25 male subjects were scanned using a photogrametry-based scanner and measured by an experienced tailor. Experimental results on the synthetic ModelNet40 dataset and on the real scans demonstrate that the proposed method outperforms state-of-the-art methods, and performs sufficiently close to a professional tailor.",
        "versions": [],
        "rank": 21
    },
    {
        "authors": [
            "Mendoza Bobadilla, Julio Cesar, 1990-"
        ],
        "title": "Classifica\u00e7\u00e3o de n\u00f3dulos pulmonares baseada em redes neurais convolucionais profundas em radiografias",
        "publication_date": "2018-09-03 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/296892516.pdf"
        ],
        "id": "id-2578590482901610221",
        "abstract": "Orientador: H\u00e9lio PedriniDisserta\u00e7\u00e3o (mestrado) - Universidade Estadual de Campinas, Instituto de Computa\u00e7\u00e3oResumo: O c\u00e2ncer de pulm\u00e3o, que se caracteriza pela presen\u00e7a de n\u00f3dulos, \u00e9 o tipo mais comum de c\u00e2ncer em todo o mundo, al\u00e9m de ser um dos mais agressivos e fatais, com 20% da mortalidade total por c\u00e2ncer. A triagem do c\u00e2ncer de pulm\u00e3o pode ser realizada por radiologistas que analisam imagens de raios-X de t\u00f3rax (CXR). No entanto, a detec\u00e7\u00e3o de n\u00f3dulos pulmonares \u00e9 uma tarefa dif\u00edcil devido a sua grande variabilidade, limita\u00e7\u00f5es humanas de mem\u00f3ria, distra\u00e7\u00e3o e fadiga, entre outros fatores. Essas dificuldades motivam o desenvolvimento de sistemas de diagn\u00f3stico por computador (CAD) para apoiar radiologistas na detec\u00e7\u00e3o de n\u00f3dulos pulmonares. A classifica\u00e7\u00e3o do n\u00f3dulo do pulm\u00e3o \u00e9 um dos principais t\u00f3picos relacionados aos sistemas de CAD. Embora as redes neurais convolucionais (CNN) tenham demonstrado um bom desempenho em muitas tarefas, h\u00e1 poucas explora\u00e7\u00f5es de seu uso para classificar n\u00f3dulos pulmonares em imagens CXR. Neste trabalho, propusemos e analisamos um arcabou\u00e7o para a detec\u00e7\u00e3o de n\u00f3dulos pulmonares em imagens de CXR que inclui segmenta\u00e7\u00e3o da \u00e1rea pulmonar, localiza\u00e7\u00e3o de n\u00f3dulos e classifica\u00e7\u00e3o de n\u00f3dulos candidatos. Apresentamos um m\u00e9todo para classifica\u00e7\u00e3o de n\u00f3dulos candidatos com CNNs treinadas a partir do zero. A efic\u00e1cia do nosso m\u00e9todo baseia-se na sele\u00e7\u00e3o de par\u00e2metros de aumento de dados, no projeto de uma arquitetura CNN especializada, no uso da regulariza\u00e7\u00e3o de dropout na rede, inclusive em camadas convolucionais, e no tratamento da falta de amostras de n\u00f3dulos em compara\u00e7\u00e3o com amostras de fundo, balanceando mini-lotes em cada itera\u00e7\u00e3o da descida do gradiente estoc\u00e1stico. Todas as decis\u00f5es de sele\u00e7\u00e3o do modelo foram tomadas usando-se um subconjunto de imagens CXR da base Lung Image Database Consortium and Image Database Resource Initiative (LIDC/IDRI) separadamente. Ent\u00e3o, utilizamos todas as imagens com n\u00f3dulos no conjunto de dados da Japanese Society of Radiological Technology (JSRT) para avalia\u00e7\u00e3o. Nossos experimentos mostraram que as CNNs foram capazes de alcan\u00e7ar resultados competitivos quando comparados com m\u00e9todos da literatura. Nossa proposta obteve uma curva de opera\u00e7\u00e3o (AUC) de 7.51 considerando 10 falsos positivos por imagem (FPPI) e uma sensibilidade de 71.4% e 81.0% com 2 e 5 FPPI, respectivamenteAbstract: Lung cancer, which is characterized by the presence of nodules, is the most common type of cancer around the world, as well as one of the most aggressive and deadliest cancer, with 20% of total cancer mortality. Lung cancer screening can be performed by radiologists analyzing chest X-ray (CXR) images. However, the detection of lung nodules is a difficult task due to their wide variability, human limitations of memory, distraction and fatigue, among other factors. These difficulties motivate the development of computer-aided diagnosis (CAD) systems for supporting radiologists in detecting lung nodules. Lung nodule classification is one of the main topics related to CAD systems. Although convolutional neural networks (CNN) have been demonstrated to perform well on many tasks, there are few explorations of their use for classifying lung nodules in CXR images. In this work, we proposed and analyzed a pipeline for detecting lung nodules in CXR images that includes lung area segmentation, potential nodule localization, and nodule candidate classification. We presented a method for classifying nodule candidates with a CNN trained from the scratch. The effectiveness of our method relies on the selection of data augmentation parameters, the design of a specialized CNN architecture, the use of dropout regularization on the network, inclusive in convolutional layers, and addressing the lack of nodule samples compared to background samples balancing mini-batches on each stochastic gradient descent iteration. All model selection decisions were taken using a CXR subset of the Lung Image Database Consortium and Image Database Resource Initiative (LIDC/IDRI) dataset separately. Thus, we used all images with nodules in the Japanese Society of Radiological Technology (JSRT) dataset for evaluation. Our experiments showed that CNNs were capable of achieving competitive results when compared to state-of-the-art methods. Our proposal obtained an area under the free-response receiver operating characteristic (AUC) curve of 7.51 considering 10 false positives per image (FPPI), and a sensitivity of 71.4% and 81.0% with 2 and 5 FPPI, respectivelyMestradoCi\u00eancia da Computa\u00e7\u00e3oMestre em Ci\u00eancia da Computa\u00e7\u00e3oCAPE",
        "versions": [],
        "rank": 22
    },
    {
        "authors": [
            "Dakai Jin",
            "Dazhou Guo",
            "Tsung-Ying Ho",
            "Adam P. Harrison",
            "Jing Xiao, Chen-kan Tseng",
            "Le Lu"
        ],
        "title": "Accurate Esophageal Gross Tumor Volume Segmentation in PET/CT using Two-Stream Chained 3D Deep Network Fusion",
        "publication_date": "2019-09-06 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://web.archive.org/web/20200930074206/https://arxiv.org/pdf/1909.01524v2.pdf"
        ],
        "id": "id-4524715733224438034",
        "abstract": "Gross tumor volume (GTV) segmentation is a critical step in esophageal cancer radiotherapy treatment planning. Inconsistencies across oncologists and prohibitive labor costs motivate automated approaches for this task. However, leading approaches are only applied to radiotherapy computed tomography (RTCT) images taken prior to treatment. This limits the performance as RTCT suffers from low contrast between the esophagus, tumor, and surrounding tissues. In this paper, we aim to exploit both RTCT and positron emission tomography (PET) imaging modalities to facilitate more accurate GTV segmentation. By utilizing PET, we emulate medical professionals who frequently delineate GTV boundaries through observation of the RTCT images obtained after prescribing radiotherapy and PET/CT images acquired earlier for cancer staging. To take advantage of both modalities, we present a two-stream chained segmentation approach that effectively fuses the CT and PET modalities via early and late 3D deep-network-based fusion. Furthermore, to effect the fusion and segmentation we propose a simple yet effective progressive semantically nested network (PSNN) model that outperforms more complicated models. Extensive 5-fold cross-validation on 110 esophageal cancer patients, the largest analysis to date, demonstrates that both the proposed two-stream chained segmentation pipeline and the PSNN model can significantly improve the quantitative performance over the previous state-of-the-art work by 11% in absolute Dice score (DSC) (from 0.654 to 0.764) and, at the same time, reducing the Hausdorff distance from 129 mm to 47 mm.",
        "versions": [],
        "rank": 23
    },
    {
        "authors": [
            "Jordina Torrents-Barrena",
            "Gemma Piella",
            "Narc\u00eds Masoller",
            "Eduard Gratac\u00f3s",
            "Elisenda Eixarch",
            "Mario Ceresa",
            "Miguel \u00c1ngel Gonz\u00e1lez Ballester"
        ],
        "title": "Segmentation and classification in MRI and US fetal imaging: Recent trends and future prospects",
        "publication_date": "2019-01-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "51",
        "doi": "10.1016/j.media.2018.10.003",
        "urls": [
            "https://openalex.org/W2898050043",
            "https://doi.org/10.1016/j.media.2018.10.003"
        ],
        "id": "id-916068727792213508",
        "abstract": "",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Segmentation and classification in MRI and US fetal imaging: Recent trends and future prospects\u2606",
                "journal": "Medical Image Analysis",
                "urls": [
                    "https://www.semanticscholar.org/paper/3ddf1f467e4beaeda6c81504a774a90c7d0e6788"
                ],
                "doi": "10.1016/j.media.2018.10.003",
                "publication_date": "None"
            }
        ],
        "rank": 24
    },
    {
        "authors": [
            "Hanchuan Peng",
            "Phuong Chung",
            "Fuhui Long",
            "Lei Qu",
            "Arnim Jenett",
            "A. Seeds",
            "E. Myers",
            "J. Simpson"
        ],
        "title": "BrainAligner: 3D Registration Atlases of Drosophila Brains",
        "publication_date": "2011-04-19 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Nature methods",
        "volume": "8",
        "doi": "10.1038/nmeth.1602",
        "urls": [
            "https://www.semanticscholar.org/paper/99c8ada76d6891259d1b73dbc2cd9021e00d3ca4",
            "https://europepmc.org/articles/pmc3104101?pdf=render"
        ],
        "id": "id-7501283921715394722",
        "abstract": null,
        "versions": [],
        "rank": 25
    },
    {
        "authors": [
            "David Fuentes-Jim\u00e9nez",
            "D. Pizarro",
            "D. Casillas-P\u00e9rez",
            "T. Collins",
            "A. Bartoli"
        ],
        "title": "Texture-Generic Deep Shape-From-Template",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Access",
        "volume": "9",
        "doi": "10.1109/ACCESS.2021.3082011",
        "urls": [
            "https://www.semanticscholar.org/paper/504e6c69363b1e60cad9622b15e1e9017ae6a4a7",
            "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09435325.pdf"
        ],
        "id": "id-781550740756846819",
        "abstract": "Shape-from-Template (SfT) solves the registration and 3D reconstruction of a deformable 3D object, represented by the template, from a single image. Recently, methods based on deep learning have been able to solve SfT for the wide-baseline case in real-time, clearly surpassing classical methods. However, the main limitation of current methods is the need for fine tuning of the neural models to a specific geometry and appearance represented by the template texture map. We propose the first texture-generic deep learning SfT method which adapts to new texture maps at run-time, without the need for texture specific fine tuning. We achieve this by dividing the problem into a segmentation step and a registration and reconstruction step, both solved with deep learning. We include the template texture map as one of the neural inputs in both steps, training our models to adapt to different ones. We show that our method obtains comparable or better results to previous deep learning models, which are texture specific. It works in challenging imaging conditions, including complex deformations, occlusions, motion blur and poor textures. Our implementation runs in real-time, with a low-cost GPU and CPU.",
        "versions": [
            {
                "year": 2021,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Texture-Generic Deep Shape-from-Template",
                "journal": "Institute of Electrical and Electronics Engineers (IEEE)",
                "urls": [
                    "https://web.archive.org/web/20210717114943/https://ieeexplore.ieee.org/ielx7/6287639/6514899/09435325.pdf"
                ],
                "doi": "10.1109/access.2021.3082011",
                "publication_date": "2021-01-01 00:00:00"
            }
        ],
        "rank": 26
    },
    {
        "authors": [
            "Lina Olandersson",
            "Susanna Larsson",
            "Johan Karlsson"
        ],
        "title": "Detection of Cardiomyocyte Proliferation using Convolutional Neural Networks Master\u2019s thesis in Biomedical Engineering",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://www.semanticscholar.org/paper/196600db7545818f98a33ffd0091691a37f58a6b"
        ],
        "id": "id8940857726007904525",
        "abstract": "Cardiovascular diseases are one of the major health issues of today. A myocardial infarction can cause heart muscle cells to die due to oxygen deficiency. The heart cells have no capability of regeneration, which means that the dead cells will not be replaced by new ones. The pharmaceutical company AstraZeneca is searching for a compound that can induce cell divisions of heart cells. The aim of this thesis is to investigate a method of measuring cell division rate. Time sequences of 2D microscope images are analyzed to detect cell divisions using a convolutional neural network. First, all sequences were processed by image registration and normalization. Then, a training set was generated by manual annotation. Data augmentation was used to increase the number of training examples of cell divisions, due to that cell divisions are rare. Mitosis takes several hours and is hard to detect on a single image. To make use of the temporal information, three images from the time sequence were used as input on multiple channels. The convolutional neural network learned to distinguish between two classes, cell division and background, after training on patches of size 60x60 pixels and three time steps. A sliding window was used to classify a full time sequence. The model learned to detect cell division in a fluorescent channel with a recall of 83% and in a bright field channel with a recall of 48%. A problem with the model detecting false positives was encountered. It was to some extent solved by active learning and using ensembles of models. To be able to measure the true cell division rate accurately, the number of false positives should be lower than the number of true positives. The developed model cannot be applied to count cell divisions to a high enough accuracy. Despite that, the thesis shows that a convolutional neural network can detect cell divisions in bright field images, which are difficult to detect by a human eye. Furthermore, the use of ensembles and active learning to improve the results showed promising result and could be further developed to create a model that can be used in pharmaceutical research.",
        "versions": [],
        "rank": 27
    },
    {
        "authors": [
            "Seyoun Park",
            "W. Plishker",
            "R. Shekhar",
            "H. Quon",
            "J. Wong",
            "Junghoon Lee"
        ],
        "title": "Deformable registration of CT and cone-beam CT by local CBCT intensity correction",
        "publication_date": "2015-03-20 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1117/12.2082485",
        "urls": [
            "https://www.semanticscholar.org/paper/aeb57c15de82e73ccb5f4ae0f9c25f985171c7c7"
        ],
        "id": "id-1181756869639452839",
        "abstract": "In this paper, we propose a method to accurately register CT to cone-beam CT (CBCT) by iteratively correcting local CBCT intensity. CBCT is a widely used intra-operative imaging modality in image-guided radiotherapy and surgery. A short scan followed by a filtered-backprojection is typically used for CBCT reconstruction. While data on the mid-plane (plane of source-detector rotation) is complete, off-mid-planes undergo different information deficiency and the computed reconstructions are approximate. This causes different reconstruction artifacts at off-mid-planes depending on slice locations, and therefore impedes accurate registration between CT and CBCT. To address this issue, we correct CBCT intensities by matching local intensity histograms slice by slice in conjunction with intensity-based deformable registration. This correction-registration step is repeated until the result image converges. We tested the proposed method on eight head-and-neck cancer cases and compared its performance with state-of-the-art registration methods, Bspline, demons, and optical flow, which are widely used for CT-CBCT registration. Normalized mutual-information (NMI), normalized cross-correlation (NCC), and structural similarity (SSIM) were computed as similarity measures for the performance evaluation. Our method produced overall NMI of 0.59, NCC of 0.96, and SSIM of 0.93, outperforming existing methods by 3.6%, 2.4%, and 2.8% in terms of NMI, NCC, and SSIM scores, respectively. Experimental results show that our method is more consistent and roust than existing algorithms, and also computationally efficient with faster convergence.",
        "versions": [
            {
                "year": 2015,
                "source": "SupportedSources.CROSSREF",
                "title": "Deformable registration of CT and cone-beam CT by local CBCT intensity correction",
                "journal": "",
                "urls": [
                    "http://dx.doi.org/10.1117/12.2082485"
                ],
                "doi": "10.1117/12.2082485",
                "publication_date": "2015-03-20 00:00:00"
            }
        ],
        "rank": 28
    },
    {
        "authors": [
            "Hern\u00e1ndez Gim\u00e9nez, M\u00f3nica",
            "Mayordomo C\u00e1mara, Elvira",
            "Ramon Julvez, Ubaldo"
        ],
        "title": "LDDMM Meets GANs: Generative Adversarial Networks for Diffeomorphic Registration",
        "publication_date": "2021-11-12 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": "10.26754/jjii3a.20216002",
        "urls": [
            "https://core.ac.uk/download/525009290.pdf"
        ],
        "id": "id706523872110911895",
        "abstract": "In this work, we propose an unsupervised adversarial learning LDDMM method for 3D mono-modal images based on Generative Adversarial Networks. We have successfully implemented two models with stationary and EPDiff constrained non-stationary parameterizations of diffeomorphisms. Our approach has shown a competitive performance with respect to benchmark supervised and model-based methods.In this work, we propose an unsupervised adversarial learning LDDMM method for 3D mono-modal images based on Generative Adversarial Networks. We have successfully implemented two models with stationary and EPDiff constrained non-stationary parameterizations of diffeomorphisms. Our approach has shown a competitive performance with respect to benchmark supervised and model-based methods",
        "versions": [
            {
                "year": 2021,
                "source": "SupportedSources.ARXIV",
                "title": "LDDMM meets GANs: Generative Adversarial Networks for diffeomorphic registration",
                "journal": null,
                "urls": [
                    "http://arxiv.org/pdf/2111.12544v1",
                    "http://arxiv.org/abs/2111.12544v1",
                    "http://arxiv.org/pdf/2111.12544v1"
                ],
                "doi": "",
                "publication_date": "2021-11-24 15:26:16+00:00"
            },
            {
                "year": 2022,
                "source": "SupportedSources.CROSSREF",
                "title": "LDDMM Meets GANs: Generative Adversarial Networks for\u00a0Diffeomorphic Registration",
                "journal": "",
                "urls": [
                    "https://link.springer.com/content/pdf/10.1007/978-3-031-11203-4_3",
                    "http://dx.doi.org/10.1007/978-3-031-11203-4_3"
                ],
                "doi": "10.1007/978-3-031-11203-4_3",
                "publication_date": "2022-01-01 00:00:00"
            },
            {
                "year": 2021,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "LDDMM meets GANs: Generative Adversarial Networks for diffeomorphic registration",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/9bb74608180e97815a93567bad9c7d21c87b6876",
                    "http://arxiv.org/pdf/2111.12544"
                ],
                "doi": "10.1007/978-3-031-11203-4_3",
                "publication_date": "2021-11-12 00:00:00"
            },
            {
                "year": 2021,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "LDDMM Meets GANs: Generative Adversarial Networks for Diffeomorphic Registration",
                "journal": "Universidad de Zaragoza",
                "urls": [
                    "https://web.archive.org/web/20220106213732/https://papiro.unizar.es/ojs/index.php/jji3a/article/download/6002/4993"
                ],
                "doi": "10.26754/jjii3a.20216002",
                "publication_date": "2021-11-12 00:00:00"
            }
        ],
        "rank": 29
    },
    {
        "authors": [
            "Leila C. C. Bergamasco",
            "F\u00e1tima L. S. Nunes"
        ],
        "title": "Intelligent retrieval and classification in three-dimensional biomedical images \u2014 A systematic mapping",
        "publication_date": "2019-02-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "31",
        "doi": "10.1016/j.cosrev.2018.10.003",
        "urls": [
            "https://openalex.org/W2901162702",
            "https://doi.org/10.1016/j.cosrev.2018.10.003"
        ],
        "id": "id-3835578080381074034",
        "abstract": "",
        "versions": [
            {
                "year": 2019,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Intelligent retrieval and classification in three-dimensional biomedical images - A systematic mapping",
                "journal": "Comput. Sci. Rev.",
                "urls": [
                    "https://www.semanticscholar.org/paper/ca91ab0b3979da41e30415771d5c86e494609a3d"
                ],
                "doi": "10.1016/J.COSREV.2018.10.003",
                "publication_date": "2019-02-01 00:00:00"
            }
        ],
        "rank": 30
    },
    {
        "authors": [
            "Xiao Chen",
            "Hongyan Liang",
            "Yimin Li",
            "Deyong Chen",
            "Junbo Wang",
            "Jian Chen"
        ],
        "title": "Development of an Imaging and Impedance Flow Cytometer Based on a Constriction Microchannel and Deep Neural Pattern Recognition",
        "publication_date": "2022-11-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Transactions on Electron Devices",
        "volume": "69",
        "doi": "10.1109/TED.2022.3206278",
        "urls": [
            "https://www.semanticscholar.org/paper/ef34ef4ef845ff2c3eda3c24b9bed2ea6df6e184"
        ],
        "id": "id5180769609287921660",
        "abstract": "This article presents an imaging and impedance flow cytometer based on a constriction microchannel with corresponding cell-type classification based on deep neural pattern recognition. When an incoming cell reached the entrance of the constriction microchannel, the image of the nucleus labeled with fluorescence was captured by a high-speed camera without the concern of losing focus, whereas when the cell deformed through the constriction microchannel by effectively blocking electrical lines, large impedance variations were sampled by an impedance analyzer. Six key biostructural and bioelectrical parameters (e.g., cell diameter, nuclear roundness, and cytoplasmic conductivity) from thousands of single cells were extracted, producing a successful rate of 88.3% in classifying A549 versus Jurkat versus K562 cells based on the feedforward neural network. In addition, multilayer neural networks of deep learning (e.g., VGG16 of CNN and LSTM of RNN) were also used to process fluorescent images and impedance profiles, producing an almost 100% successful rate in cell-type classification. In summary, the microfluidic flow cytometer reported in this article could characterize single-cell biostructural and bioelectrical properties in a high throughput manner, realizing high successful rates of cell-type classification.",
        "versions": [],
        "rank": 31
    },
    {
        "authors": [
            "Fei Song",
            "Mengya Li",
            "Yang Yang",
            "Kun Yang",
            "Xueyan Gao",
            "Tingting Dan"
        ],
        "title": "Small UAV based multi-viewpoint image registration for monitoring cultivated land changes in mountainous terrain",
        "publication_date": "2018-11-02 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "International Journal of Remote Sensing",
        "volume": "39",
        "doi": "10.1080/01431161.2018.1516051",
        "urls": [
            "https://www.semanticscholar.org/paper/566cf3fcadfc3688faf19daffa886300740f6e4a"
        ],
        "id": "id-8959767914365283961",
        "abstract": "ABSTRACT Land degradation, soil erosion, and illegal occupation in the mountainous terrain of southern China have severely reduced the amount of cultivatable land. The use of small unmanned aerial vehicles (UAVs, aka drones) equipped with various types of cameras is considered to be a flexible and low-cost platform for monitoring cultivated land changes. However, image pairs of the same scene taken from different viewpoints often contain discontinuous rotated images with illuminated variations. To address these problems, a novel small UAV based multi-viewpoint image registration method for monitoring cultivated land changes in mountainous terrain is proposed. First, a mixed feature descriptor (MFD) is defined for measuring global and local discrepancies between two datapoint sets, and a deterministic annealing scheme is employed to control the balance of the MFD. Second, the mixed feature finite mixture model (MFMM) is formulated to be the estimation of mixture densities. Finally, the double geometric constraints for -minimizing estimate based energy optimization is formulated in order to calculate a reasonable position in a reproducing kernel Hilbert space. Extensive experiments on UAV images with different viewpoints are conducted. Experimental results show that our method provides better performances in most cases after comparing with six state-of-the-art methods.",
        "versions": [
            {
                "year": 2018,
                "source": "SupportedSources.OPENALEX",
                "title": "Small UAV based multi-viewpoint image registration for monitoring cultivated land changes in mountainous terrain",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2899338172",
                    "https://doi.org/10.1080/01431161.2018.1516051"
                ],
                "doi": "10.1080/01431161.2018.1516051",
                "publication_date": "2018-11-02 00:00:00"
            }
        ],
        "rank": 32
    },
    {
        "authors": [
            "E. Falconer",
            "Adrian R. Allen",
            "K. Felmingham",
            "L. Williams",
            "R. Bryant"
        ],
        "title": "Inhibitory neural activity predicts response to cognitive-behavioral therapy for posttraumatic stress disorder.",
        "publication_date": "2013-09-15 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "The Journal of clinical psychiatry",
        "volume": "74 9",
        "doi": "10.4088/JCP.12m08020",
        "urls": [
            "https://www.semanticscholar.org/paper/2f3785d99835cf486c4a78c4a6872f346577fcd5"
        ],
        "id": "id-6439591089373127477",
        "abstract": "OBJECTIVE\nDespite cognitive-behavioral therapy (CBT) being an effective treatment for posttraumatic stress disorder (PTSD), many patients do not respond to CBT. Understanding the neural bases of treatment response may inform treatment refinement, thereby improving treatment response rates. Adequate working memory function is proposed to enable engagement in CBT.\n\n\nMETHOD\nThis study employed a Go/No-Go task to examine inhibitory function and its functional brain correlates as predictors of response to CBT in PTSD. Participants were recruited between October 2003 and May 2005. Thirteen treatment-seeking patients who met DSM-IV criteria for PTSD completed the Go/No-Go task while undergoing functional magnetic resonance imaging (fMRI), after which they entered 8 once-weekly sessions of CBT. PTSD severity was measured before treatment and again at 6 months following treatment completion using the Clinician-Administered PTSD Scale (primary outcome measure).\n\n\nRESULTS\nAfter controlling for initial PTSD severity and ongoing depressive symptoms, greater activity in left dorsal striatal (Z = 3.19, P = .001) and frontal (Z = 3.03, P = .001) networks during inhibitory control was associated with lower PTSD symptom severity after treatment, suggesting better treatment response.\n\n\nCONCLUSIONS\nThese results suggest that neural circuitry underpinning inhibitory control plays a role in the outcome of CBT for patients with PTSD.\n\n\nTRIAL REGISTRATION\nanzctr.org Identifier: ACTRN12610000017022.",
        "versions": [],
        "rank": 33
    },
    {
        "authors": [
            "Wenxiong Kang",
            "Qiuxia Wu"
        ],
        "title": "Contactless Palm Vein Recognition Using a Mutual Foreground-Based Local Binary Pattern",
        "publication_date": "2014-11-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Transactions on Information Forensics and Security",
        "volume": "9",
        "doi": "10.1109/TIFS.2014.2361020",
        "urls": [
            "https://www.semanticscholar.org/paper/3b430466f7c7f26bf0dcb2f32646a67c0a611f0a"
        ],
        "id": "id-2827183083341899941",
        "abstract": "Local binary pattern (LBP) is popular for the texture representation owing to its discrimination ability and computational efficiency, but when used to describe the sparse texture in palm vein images, the discrimination ability is diluted, leading to lower performance, especially for contactless palm vein matching. In this paper, an improved mutual foreground LBP method is presented for achieving a better matching performance for contactless palm vein recognition. First, the normalized gradient-based maximal principal curvature algorithm and k -means method are utilized for texture extraction, which can effectively suppress noise and improve accuracy and robustness. Then, an LBP matching strategy was adopted for similarity measurements on the basis of extracted palm veins and their neighborhoods, which include the vast majority of useful distinctive information for identification while eliminating interference by excluding the background. To further improve the LBP performance, the matched pixel ratio was adopted to determine the best matching region (BMR). Finally, the matching score obtained in the process of finding the BMR was fused with results of LBP matching at the score level to further improve the identification performance. A series of rigorous contrast experiments using the palm vein data set in the CASIA multispectral palmprint image database were conducted. The obtained low equal error rate (0.267%) and comparisons with the most state-of-the-art approaches demonstrate that our method is feasible and effective for contactless palm vein recognition.",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.OPENALEX",
                "title": "Contactless Palm Vein Recognition Using a Mutual Foreground-Based Local Binary Pattern",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2031337792",
                    "https://doi.org/10.1109/tifs.2014.2361020"
                ],
                "doi": "10.1109/tifs.2014.2361020",
                "publication_date": "2014-11-01 00:00:00"
            }
        ],
        "rank": 34
    },
    {
        "authors": [
            "Brouwer, C.",
            "Kierkels, R.",
            "van \u2019t Veld, A.",
            "Sijtsema, N.",
            "Meertens, H."
        ],
        "title": "The effects of computed tomography image characteristics and knot spacing on the spatial accuracy of B-spline deformable image registration in the head and neck geometry",
        "publication_date": "2014-07-29 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1186/1748-717x-9-169",
        "urls": [
            "http://link.springer.com/content/pdf/10.1186/1748-717X-9-169.pdf",
            "http://dx.doi.org/10.1186/1748-717x-9-169"
        ],
        "id": "id-5308200589301063583",
        "abstract": "",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "The effects of computed tomography image characteristics and knot spacing on the spatial accuracy of B-spline deformable image registration in the head and neck geometry",
                "journal": "Radiation Oncology",
                "urls": [
                    "https://www.semanticscholar.org/paper/d3fdf2f193f1b9e815378fa2150752318f039b62",
                    "https://ro-journal.biomedcentral.com/counter/pdf/10.1186/1748-717X-9-169"
                ],
                "doi": "10.1186/1748-717X-9-169",
                "publication_date": "2014-07-29 00:00:00"
            }
        ],
        "rank": 35
    },
    {
        "authors": [
            "Kamlesh Mistry",
            "Li Zhang",
            "S. C. Neoh",
            "Ming Jiang",
            "Md. Alamgir Hossain",
            "Benoit Lafon"
        ],
        "title": "Intelligent Appearance and shape based facial emotion recognition for a humanoid robot",
        "publication_date": "2014-12-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/SKIMA.2014.7083542",
        "urls": [
            "https://www.semanticscholar.org/paper/3bf673a1f620015cb8b5106b85c7168431bb48ff"
        ],
        "id": "id2195310542604709004",
        "abstract": "In this paper, we present an intelligent facial emotion recognition system with real-time face tracking for a humanoid robot. The system is able to detect facial actions and emotions from images with up to 60 degrees of pose variations. We employ the Active Appearance Model to perform real-time face tracking and extract both texture and geometric representations of images. A POSIT algorithm is also used to identify head rotations. The extracted texture and shape features are employed to detect 18 facial actions and seven basic emotions. The overall system is integrated with a humanoid robot platform to further extend its vision APIs. The system is proved to be able to deal with challenging facial emotion recognition tasks with various pose variations.",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.OPENALEX",
                "title": "Intelligent Appearance and shape based facial emotion recognition for a humanoid robot",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2035238796",
                    "https://doi.org/10.1109/skima.2014.7083542"
                ],
                "doi": "10.1109/skima.2014.7083542",
                "publication_date": "2014-12-01 00:00:00"
            }
        ],
        "rank": 36
    },
    {
        "authors": [
            "S. Franchini",
            "A. Gentile",
            "F. Sorbello",
            "G. Vassallo",
            "S. Vitabile"
        ],
        "title": "ConformalALU: A Conformal Geometric Algebra Coprocessor for Medical Image Processing",
        "publication_date": "2015-04-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Transactions on Computers",
        "volume": "64",
        "doi": "10.1109/TC.2014.2315652",
        "urls": [
            "https://www.semanticscholar.org/paper/5144bd4120395f3a5f39027aa30f2278e2e04f07"
        ],
        "id": "id-911094476901148075",
        "abstract": "Medical imaging involves important computational geometric problems, such as image segmentation and analysis, shape approximation, three-dimensional (3D) modeling, and registration of volumetric data. In the last few years, Conformal Geometric Algebra (CGA), based on five-dimensional (5D) Clifford Algebra, is emerging as a new paradigm that offers simple and universal operators for the representation and solution of complex geometric problems. However, the widespread use of CGA has been so far hindered by its high dimensionality and computational complexity. This paper proposes a simplified formulation of the conformal geometric operations (reflections, rotations, translations, and uniform scaling) aimed at a parallel hardware implementation. A specialized coprocessing architecture (ConformalALU) that offers direct hardware support to the new CGA operators, is also presented. The ConformalALU has been prototyped as a complete System-on-Programmable-Chip (SoPC) on the Xilinx ML507 FPGA board, containing a Virtex-5 FPGA device. Experimental results show average speedups of one order of magnitude for CGA rotations, translations, and dilations with respect to the geometric algebra software library Gaigen running on the general-purpose PowerPC processor embedded in the target FPGA device. A suite of medical imaging applications, including segmentation, 3D modeling and registration of medical data, has been used as testbench to evaluate the coprocessor effectiveness.",
        "versions": [
            {
                "year": 2015,
                "source": "SupportedSources.OPENALEX",
                "title": "ConformalALU: A Conformal Geometric Algebra Coprocessor for Medical Image Processing",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2083937466",
                    "https://doi.org/10.1109/tc.2014.2315652"
                ],
                "doi": "10.1109/tc.2014.2315652",
                "publication_date": "2015-04-01 00:00:00"
            }
        ],
        "rank": 37
    },
    {
        "authors": [
            "Abidi, A.",
            "Singh, S."
        ],
        "title": "A Moving Least Square Based Framework for Thoracic CT Image Registration",
        "publication_date": "2020-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-981-10-5837-0_3",
        "urls": [
            "http://link.springer.com/content/pdf/10.1007/978-981-10-5837-0_3",
            "http://dx.doi.org/10.1007/978-981-10-5837-0_3"
        ],
        "id": "id2903366668149719040",
        "abstract": "",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "A Moving Least Square Based Framework for Thoracic CT Image Registration",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/473f33f4f87983cfa733367a79aa3435e50f0810"
                ],
                "doi": "10.1007/978-981-10-5837-0_3",
                "publication_date": "None"
            }
        ],
        "rank": 38
    },
    {
        "authors": [
            "Katharina Breininger",
            "Moritz Hanika",
            "Mareike Weule",
            "Markus Kowarschik",
            "Marcus Dr. Pfister",
            "Andreas Maier"
        ],
        "title": "Simultaneous reconstruction of multiple stiff wires from a single X-ray projection for endovascular aortic repair",
        "publication_date": "2019-08-22 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "14",
        "doi": "10.1007/s11548-019-02052-7",
        "urls": [
            "https://openalex.org/W2969478681",
            "https://doi.org/10.1007/s11548-019-02052-7"
        ],
        "id": "id-3453057346397175886",
        "abstract": "",
        "versions": [
            {
                "year": 2019,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Simultaneous reconstruction of multiple stiff wires from a single X-ray projection for endovascular aortic repair",
                "journal": "International Journal of Computer Assisted Radiology and Surgery",
                "urls": [
                    "https://www.semanticscholar.org/paper/80684c750aef0404dea92ce073805f5d3eef855e"
                ],
                "doi": "10.1007/s11548-019-02052-7",
                "publication_date": "2019-08-22 00:00:00"
            }
        ],
        "rank": 39
    },
    {
        "authors": [
            "R. Penjweini",
            "Michele M. Kim",
            "T. Zhu"
        ],
        "title": "Three\u2010dimensional finite\u2010element based deformable image registration for evaluation of pleural cavity irradiation during photodynamic therapy",
        "publication_date": "2017-07-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Medical Physics",
        "volume": "44",
        "doi": "10.1002/mp.12284",
        "urls": [
            "https://www.semanticscholar.org/paper/3c2e7e8d1ec31e101e82d422ce38b48953fd6f32",
            "https://aapm.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mp.12284"
        ],
        "id": "id-6192659886956116599",
        "abstract": "Purpose Photodynamic therapy (PDT) is used after surgical resection to treat the microscopic disease for malignant pleural mesothelioma and to increase survival rates. As accurate light delivery is imperative to PDT efficacy, the deformation of the pleural volume during the surgery is studied on its impact on the delivered light fluence. In this study, a three\u2010dimensional finite element\u2010based (3D FEM) deformable image registration is proposed to directly match the volume of lung to the volume of pleural cavity obtained during PDT to have accurate representation of the light fluence accumulated in the lung, heart and liver (organs\u2010at\u2010risk) during treatment. Methods A wand, comprised of a modified endotrachial tube filled with Intralipid and an optical fiber inside the tube, is used to deliver the treatment light. The position of the treatment is tracked using an optical tracking system with an attachment comprised of nine reflective passive markers that are seen by an infrared camera\u2013based navigation system. This information is used to obtain the surface contours of the plural cavity and the cumulative light fluence on every point of the cavity surface that is being treated. The lung, heart, and liver geometry are also reconstructed from a series of computed tomography (CT) scans of the organs acquired in the same patient before and after the surgery. The contours obtained with the optical tracking system and CTs are imported into COMSOL Multiphysics, where the 3D FEM\u2010based deformable image registration is obtained. The delivered fluence values are assigned to the respective positions (x, y, and z) on the optical tracking contour. The optical tracking contour is considered as the reference, and the CT contours are used as the target, which will be deformed. The data from three patients formed the basis for this study. Results The physical correspondence between the CT and optical tracking geometries, taken at different times, from different imaging devices was established using the 3D FEM\u2010based image deformable registration. The volume of lung was matched to the volume of pleural cavity and the distribution of light fluence on the surface of the heart, liver and deformed lung volumes was obtained. Conclusion The method used is appropriate for analyzing problems over complicated domains, such as when the domain changes (as in a solid\u2010state reaction with a moving boundary), when the desired precision varies over the entire domain, or when the solution lacks smoothness. Implementing this method in real\u2010time for clinical applications and in situ monitoring of the under\u2010 or over\u2010 exposed regions to light during PDT can significantly improve the treatment for mesothelioma.",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.CROSSREF",
                "title": "Three-dimensional finite-element based deformable image registration for evaluation of pleural cavity irradiation during photodynamic therapy",
                "journal": "",
                "urls": [
                    "https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fmp.12284",
                    "https://onlinelibrary.wiley.com/doi/full/10.1002/mp.12284",
                    "http://dx.doi.org/10.1002/mp.12284"
                ],
                "doi": "10.1002/mp.12284",
                "publication_date": "2017-05-24 00:00:00"
            }
        ],
        "rank": 40
    },
    {
        "authors": [
            "Zeyi Li",
            "Haitao Zhang",
            "Yihang Huang"
        ],
        "title": "A Rotation-Invariant Optical and SAR Image Registration Algorithm Based on Deep and Gaussian Features",
        "publication_date": "2021-07-04 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Remote. Sens.",
        "volume": "13",
        "doi": "10.3390/rs13132628",
        "urls": [
            "https://www.semanticscholar.org/paper/3767f15fe455d17163569fe24a8e892612670bfd",
            "https://www.mdpi.com/2072-4292/13/13/2628/pdf?version=1625403444"
        ],
        "id": "id7576376081420450698",
        "abstract": "Traditional feature matching methods of optical and synthetic aperture radar (SAR) used gradient are sensitive to non-linear radiation distortions (NRD) and the rotation between two images. To address this problem, this study presents a novel approach to solving the rigid body rotation problem by a two-step process. The first step proposes a deep learning neural network named RotNET to predict the rotation relationship between two images. The second step uses a local feature descriptor based on the Gaussian pyramid named Gaussian pyramid features of oriented gradients (GPOG) to match two images. The RotNET uses a neural network to analyze the gradient histogram of the two images to derive the rotation relationship between optical and SAR images. Subsequently, GPOG is depicted a keypoint by using the histogram of Gaussian pyramid to make one-cell block structure which is simpler and more stable than HOG structure-based descriptors. Finally, this paper designs experiments to prove that the gradient histogram of the optical and SAR images can reflect the rotation relationship and the RotNET can correctly predict them. The similarity map test and the image registration results obtained on experiments show that GPOG descriptor is robust to SAR speckle noise and NRD.",
        "versions": [],
        "rank": 41
    },
    {
        "authors": [
            "Torsten Rohlfing",
            "Calvin R. Maurer"
        ],
        "title": "Multi-classifier framework for atlas-based image segmentation",
        "publication_date": "2005-01-01 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "Elsevier BV",
        "volume": "",
        "doi": "10.1016/j.patrec.2005.03.017",
        "urls": [
            "https://web.archive.org/web/20050327172444/http://www.stanford.edu:80/~rohlfing/publications/2004-rohlfing-cvpr-multi_classifier_segmentation.pdf"
        ],
        "id": "id-1565426298929722772",
        "abstract": "We develop and evaluate in this paper a multi-classifier framework for atlas-based segmentation, a popular segmentation method in biomedical image analysis. An atlas is a spatial map of classes (e.g., anatomical structures), which is usually derived from a reference individual by manual segmentation. An atlas-based classification is generated by registering an image to an atlas, that is, by computing a semantically correct coordinate mapping between the two. In the present paper, the registration algorithm is an intensitybased non-rigid method that computes a free-form deformation (FFD) defined on a uniform grid of control points. The transformation is regularized by a weighted smoothness constraint term. Different atlases, as well as different parameterizations of the registration algorithm, lead to different and somewhat independent atlas-based classifiers. The outputs of these classifiers can be combined in order to improve overall classification accuracy. In an evaluation study, biomedical images from seven subjects are segmented 1) using three individual atlases; 2) using one atlas and three different resolutions of the FFD control point grid; 3) using one atlas and three different regularization constraint weights. In each case, the three individual segmentations are combined by Sum Rule fusion. For each individual and for each combined segmentation, its recognition rate (relative number of correctly labeled image voxels) is computed against a manual gold-standard segmentation. In all cases, classifier combination consistently improved classification accuracy. The biggest improvement was achieved using multiple atlases, a smaller gain resulted from multiple regularization constraint weights, and a marginal gain resulted from multiple control point spacings. We conclude that multi-classifier methods have a natural application to atlas-based segmentation and the potential to increase classification accuracy in real-world segmentation problems.",
        "versions": [],
        "rank": 42
    },
    {
        "authors": [
            "Srivastava, S.",
            "Lall, B."
        ],
        "title": "Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks",
        "publication_date": "2017-08-14 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1002/9781119242963.ch6",
        "urls": [
            "https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2F9781119242963.ch6",
            "https://onlinelibrary.wiley.com/doi/full/10.1002/9781119242963.ch6",
            "http://dx.doi.org/10.1002/9781119242963.ch6"
        ],
        "id": "id-4353799041608934615",
        "abstract": "",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/0d6c77b08bcdad80221f5c9645cbb9cad144650f"
                ],
                "doi": "10.1002/9781119242963.CH6",
                "publication_date": "2017-08-14 00:00:00"
            }
        ],
        "rank": 43
    },
    {
        "authors": [
            "Martin Engilberge",
            "Edo Collins",
            "S. S\u00fcsstrunk"
        ],
        "title": "Color representation in deep neural networks",
        "publication_date": "2017-09-14 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/ICIP.2017.8296790",
        "urls": [
            "https://www.semanticscholar.org/paper/dadb6314f27f325c40d9e8c5b7e6b7dd9208461e",
            "https://infoscience.epfl.ch/record/231742/files/Engilberge%20ICIP%202017.pdf"
        ],
        "id": "id5352343205430015923",
        "abstract": "Convolutional neural networks are top-performers on image classification tasks. Understanding how they make use of color information in images may be useful for various tasks. In this paper we analyze the representation learned by a popular CNN to detect and characterize color-related features. We confirm the existence of some object- and color-specific units, as well as the effect of layer-depth on color-sensitivity and class-invariance.",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.CROSSREF",
                "title": "Color representation in deep neural networks",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx7/8267582/8296222/08296790.pdf?arnumber=8296790",
                    "http://dx.doi.org/10.1109/icip.2017.8296790"
                ],
                "doi": "10.1109/icip.2017.8296790",
                "publication_date": "2017-01-01 00:00:00"
            }
        ],
        "rank": 44
    },
    {
        "authors": [
            "Che, T.",
            "Wang, X.",
            "Zhao, K.",
            "Zhao, Y.",
            "Zeng, D.",
            "Li, Q.",
            "Zheng, Y.",
            "Yang, N.",
            "Wang, J.",
            "Li, S."
        ],
        "title": "AMNet: Adaptive multi-level network for deformable registration of 3D brain MR images",
        "publication_date": "2023-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1016/j.media.2023.102740",
        "urls": [
            "https://api.elsevier.com/content/article/PII:S1361841523000014?httpAccept=text/xml",
            "https://api.elsevier.com/content/article/PII:S1361841523000014?httpAccept=text/plain",
            "http://dx.doi.org/10.1016/j.media.2023.102740"
        ],
        "id": "id6964970836276681515",
        "abstract": "",
        "versions": [
            {
                "year": 2023,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "AMNet: Adaptive multi-level network for deformable registration of 3D brain MR images",
                "journal": "Medical image analysis",
                "urls": [
                    "https://www.semanticscholar.org/paper/f6063c1e26b462070a9d80b68a0de430fbcf4a52"
                ],
                "doi": "10.1016/j.media.2023.102740",
                "publication_date": "2023-01-01 00:00:00"
            }
        ],
        "rank": 45
    },
    {
        "authors": [
            "Alldieck, Thiemo",
            "Mir, Aymen",
            "Pons-Moll, Gerard"
        ],
        "title": "Learning to Transfer Texture from Clothing Images to 3D Humans",
        "publication_date": "2020-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": "10.1109/cvpr42600.2020.00705",
        "urls": [
            "http://arxiv.org/abs/2003.02050"
        ],
        "id": "id328030896171457450",
        "abstract": "In this paper, we present a simple yet effective method to automatically\ntransfer textures of clothing images (front and back) to 3D garments worn on\ntop SMPL, in real time. We first automatically compute training pairs of images\nwith aligned 3D garments using a custom non-rigid 3D to 2D registration method,\nwhich is accurate but slow. Using these pairs, we learn a mapping from pixels\nto the 3D garment surface. Our idea is to learn dense correspondences from\ngarment image silhouettes to a 2D-UV map of a 3D garment surface using shape\ninformation alone, completely ignoring texture, which allows us to generalize\nto the wide range of web images. Several experiments demonstrate that our model\nis more accurate than widely used baselines such as thin-plate-spline warping\nand image-to-image translation networks while being orders of magnitude faster.\nOur model opens the door for applications such as virtual try-on, and allows\nfor generation of 3D humans with varied textures which is necessary for\nlearning.Comment: IEEE Conference on Computer Vision and Pattern Recognitio",
        "versions": [
            {
                "year": 2020,
                "source": "SupportedSources.ARXIV",
                "title": "Learning to Transfer Texture from Clothing Images to 3D Humans",
                "journal": null,
                "urls": [
                    "http://arxiv.org/pdf/2003.02050v2",
                    "http://arxiv.org/abs/2003.02050v2",
                    "http://arxiv.org/pdf/2003.02050v2"
                ],
                "doi": "",
                "publication_date": "2020-03-04 12:53:58+00:00"
            },
            {
                "year": 2020,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Learning to Transfer Texture from Clothing Images to 3D Humans",
                "journal": "",
                "urls": [
                    "https://web.archive.org/web/20200402065736/https://arxiv.org/pdf/2003.02050v2.pdf"
                ],
                "doi": "",
                "publication_date": "2020-03-30 00:00:00"
            }
        ],
        "rank": 46
    },
    {
        "authors": [
            "C. Davatzikos"
        ],
        "title": "Nonlinear registration of brain images using deformable models",
        "publication_date": "1996-06-21 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/MMBIA.1996.534061",
        "urls": [
            "https://www.semanticscholar.org/paper/28c57ad247fb064d4390c6210510e5895a9ba748"
        ],
        "id": "id7737115691962372981",
        "abstract": "A key issue in several brain imaging applications, including computer aided neurosurgery, functional image analysis, and morphometrics, is the spatial normalization and registration of tomographic images from different subjects. This paper proposes a technique for spatial normalization of brain images based on elastically deformable models. In the authors' approach they use a deformable surface algorithm to find a parametric representation of the outer cortical surface and then use this representation to obtain a map between corresponding regions of the outer cortex in two different images. Based on the resulting map, the authors then derive a three-dimensional elastic warping transformation which brings two images in register. This transformation models images as inhomogeneous elastic objects which are deformed into registration with each other by external force fields. The elastic properties of the images can vary from one region to the other, allowing more variable brain regions, such as the ventricles, to deform more freely than less variable ones. Finally, the authors use prestrained elasticity to model structural irregularities, and in particular the ventricular expansion occurring with aging or diseases. The performance of the authors' algorithm is demonstrated on magnetic resonance images.",
        "versions": [
            {
                "year": 1996,
                "source": "SupportedSources.CROSSREF",
                "title": "Nonlinear registration of brain images using deformable models",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx3/3806/11134/00534061.pdf?arnumber=534061",
                    "http://dx.doi.org/10.1109/mmbia.1996.534061"
                ],
                "doi": "10.1109/mmbia.1996.534061",
                "publication_date": "1996-01-01 00:00:00"
            }
        ],
        "rank": 47
    },
    {
        "authors": [
            "D. Mandl",
            "K. M. Yi",
            "Peter Mohr",
            "P. Roth",
            "P. Fua",
            "Vincent Lepetit",
            "D. Schmalstieg",
            "D. Kalkofen"
        ],
        "title": "Learning Lightprobes for Mixed Reality Illumination",
        "publication_date": "2017-10-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/ISMAR.2017.25",
        "urls": [
            "https://www.semanticscholar.org/paper/758fbcb288bc42164dd18dbcdfd2f211272c0fea",
            "https://hal.archives-ouvertes.fr/hal-02506341/file/mandl_ismar17.pdf"
        ],
        "id": "id-5238781681562713871",
        "abstract": "This paper presents the first photometric registration pipeline for Mixed Reality based on high quality illumination estimation using convolutional neural networks (CNNs). For easy adaptation and deployment of the system, we train the CNNs using purely synthetic images and apply them to real image data. To keep the pipeline accurate and efficient, we propose to fuse the light estimation results from multiple CNN instances and show an approach for caching estimates over time. For optimal performance, we furthermore explore multiple strategies for the CNN training. Experimental results show that the proposed method yields highly accurate estimates for photo-realistic augmentations.",
        "versions": [],
        "rank": 48
    },
    {
        "authors": [
            "O. Fluck",
            "S. Aharon",
            "A. Kamen"
        ],
        "title": "Efficient framework for deformable 2D-3D registration",
        "publication_date": "2008-03-06 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1117/12.772911",
        "urls": [
            "https://www.semanticscholar.org/paper/7c45b3a4b4f5e1a4acbeff9649f093e0feeeeccd"
        ],
        "id": "id6751246158702050040",
        "abstract": "Using 2D-3D registration it is possible to extract the body transformation between the coordinate systems of X-ray and volumetric CT images. Our initial motivation is the improvement of accuracy of external beam radiation therapy, an effective method for treating cancer, where CT data play a central role in radiation treatment planning. Rigid body transformation is used to compute the correct patient setup. The drawback of such approaches is that the rigidity assumption on the imaged object is not valid for most of the patient cases, mainly due to respiratory motion. In the present work, we address this limitation by proposing a flexible framework for deformable 2D-3D registration consisting of a learning phase incorporating 4D CT data sets and hardware accelerated free form DRR generation, 2D motion computation, and 2D-3D back projection.",
        "versions": [
            {
                "year": 2008,
                "source": "SupportedSources.CROSSREF",
                "title": "Efficient framework for deformable 2D-3D registration",
                "journal": "",
                "urls": [
                    "http://dx.doi.org/10.1117/12.772911"
                ],
                "doi": "10.1117/12.772911",
                "publication_date": "2008-03-06 00:00:00"
            }
        ],
        "rank": 49
    },
    {
        "authors": [
            "X. Li",
            "Y. Zhang",
            "Y. Shi",
            "L. Zhou",
            "X. Zhen"
        ],
        "title": "Evaluation of deformable image registration for contour propagation between CT and cone-beam CT images in adaptive head and neck radiotherapy.",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine",
        "volume": "24 Suppl 2",
        "doi": "10.3233/THC-161204",
        "urls": [
            "https://www.semanticscholar.org/paper/ad4c8914de19cd644b39146bc39f72fda0941d62",
            "https://content.iospress.com:443/download/technology-and-health-care/thc1204?id=technology-and-health-care%2Fthc1204"
        ],
        "id": "id-526349160106910584",
        "abstract": "Deformable image registration (DIR) is a critical technic in adaptive radiotherapy (ART) to propagate contours between planning computerized tomography (CT) images and treatment CT/Cone-beam CT (CBCT) image to account for organ deformation for treatment re-planning. To validate the ability and accuracy of DIR algorithms in organ at risk (OAR) contours mapping, seven intensity-based DIR strategies are tested on the planning CT and weekly CBCT images from six Head & Neck cancer patients who underwent a 6 \u223c 7 weeks intensity-modulated radiation therapy (IMRT). Three similarity metrics, i.e. the Dice similarity coefficient (DSC), the percentage error (PE) and the Hausdorff distance (HD), are employed to measure the agreement between the propagated contours and the physician delineated ground truths. It is found that the performance of all the evaluated DIR algorithms declines as the treatment proceeds. No statistically significant performance difference is observed between different DIR algorithms (p> 0.05), except for the double force demons (DFD) which yields the worst result in terms of DSC and PE. For the metric HD, all the DIR algorithms behaved unsatisfactorily with no statistically significant performance difference (p= 0.273). These findings suggested that special care should be taken when utilizing the intensity-based DIR algorithms involved in this study to deform OAR contours between CT and CBCT, especially for those organs with low contrast.",
        "versions": [
            {
                "year": 2016,
                "source": "SupportedSources.CROSSREF",
                "title": "Evaluation of deformable image registration for contour propagation between CT and cone-beam CT images in adaptive head and neck radiotherapy",
                "journal": "",
                "urls": [
                    "https://content.iospress.com/download?id=10.3233/THC-161204",
                    "http://dx.doi.org/10.3233/thc-161204"
                ],
                "doi": "10.3233/thc-161204",
                "publication_date": "2016-06-13 00:00:00"
            }
        ],
        "rank": 50
    },
    {
        "authors": [
            "Itzik Malkiel",
            "Sangtae Ahn",
            "Valentina Taviani",
            "Anne Menini",
            "Lior Wolf",
            "Christopher J. Hardy"
        ],
        "title": "Conditional WGANs with Adaptive Gradient Balancing for Sparse MRI Reconstruction",
        "publication_date": "2019-05-02 22:26:06+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "",
        "urls": [
            "http://arxiv.org/pdf/1905.00985v1",
            "http://arxiv.org/abs/1905.00985v1",
            "http://arxiv.org/pdf/1905.00985v1"
        ],
        "id": "id4949488146382725784",
        "abstract": "Recent sparse MRI reconstruction models have used Deep Neural Networks (DNNs)\nto reconstruct relatively high-quality images from highly undersampled k-space\ndata, enabling much faster MRI scanning. However, these techniques sometimes\nstruggle to reconstruct sharp images that preserve fine detail while\nmaintaining a natural appearance. In this work, we enhance the image quality by\nusing a Conditional Wasserstein Generative Adversarial Network combined with a\nnovel Adaptive Gradient Balancing technique that stabilizes the training and\nminimizes the degree of artifacts, while maintaining a high-quality\nreconstruction that produces sharper images than other techniques.",
        "versions": [],
        "rank": 51
    },
    {
        "authors": [
            "Huangxing Lin",
            "Yanlong Li",
            "Xinghao Ding",
            "Weihong Zeng",
            "Yue Huang",
            "John Paisley"
        ],
        "title": "Rain O'er Me: Synthesizing real rain to derain with data distillation",
        "publication_date": "2019-04-09 11:38:24+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "10.1109/TIP.2020.3005517",
        "urls": [
            "http://arxiv.org/pdf/1904.04605v2",
            "http://dx.doi.org/10.1109/TIP.2020.3005517",
            "http://arxiv.org/abs/1904.04605v2",
            "http://arxiv.org/pdf/1904.04605v2"
        ],
        "id": "id-5125581561962264885",
        "abstract": "We present a supervised technique for learning to remove rain from images\nwithout using synthetic rain software. The method is based on a two-stage data\ndistillation approach: 1) A rainy image is first paired with a coarsely\nderained version using on a simple filtering technique (\"rain-to-clean\"). 2)\nThen a clean image is randomly matched with the rainy soft-labeled pair.\nThrough a shared deep neural network, the rain that is removed from the first\nimage is then added to the clean image to generate a second pair\n(\"clean-to-rain\"). The neural network simultaneously learns to map both images\nsuch that high resolution structure in the clean images can inform the\nderaining of the rainy images. Demonstrations show that this approach can\naddress those visual characteristics of rain not easily synthesized by software\nin the usual way.",
        "versions": [],
        "rank": 52
    },
    {
        "authors": [
            "Zongze Wu",
            "Hongchen Chen",
            "S. Du"
        ],
        "title": "Robust affine iterative closest point algorithm based on correntropy for 2D point set registration",
        "publication_date": "2016-07-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/IJCNN.2016.7727364",
        "urls": [
            "https://www.semanticscholar.org/paper/fe9c942957745fde0d193f5139ff229c87d2cf5b"
        ],
        "id": "id-5496946682530742359",
        "abstract": "The traditional affine iterative closest point (ICP) algorithm is fast and accuracy for affine registration of point sets, but it performs worse when the point sets with large outliers. This paper introduces a novel algorithm based on correntropy for affine registration of point sets with outliers. First, a novel objective function is proposed by introducing the maximum correntropy criterion (MCC) because of the outlier-rejection property of correntropy. Then, a new affine ICP algorithm is proposed to solve this energy function. This method uses a simple iterative algorithm and computes the affine transformation quickly at each iterative step. Similar to the ICP algorithm, this new algorithm converges monotonically to a local maximum for any given initial parameters. Experimental results demonstrate that our algorithm has the high speed and accuracy for affine registration with outliers compared with the traditional ICP algorithm and the state-of-the-art algorithms.",
        "versions": [
            {
                "year": 2016,
                "source": "SupportedSources.CROSSREF",
                "title": "Robust affine iterative closest point algorithm based on correntropy for 2D point set registration",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx7/7593175/7726591/07727364.pdf?arnumber=7727364",
                    "http://dx.doi.org/10.1109/ijcnn.2016.7727364"
                ],
                "doi": "10.1109/ijcnn.2016.7727364",
                "publication_date": "2016-01-01 00:00:00"
            }
        ],
        "rank": 53
    },
    {
        "authors": [
            "DE CARVALHO, A.",
            "FAIRHURST, M.",
            "BISSET, D."
        ],
        "title": "COMBINING TWO BOOLEAN NEURAL NETWORKS FOR IMAGE CLASSIFICATION",
        "publication_date": "1998-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1142/9789812816849_0019",
        "urls": [
            "http://dx.doi.org/10.1142/9789812816849_0019"
        ],
        "id": "id-6191082648190459912",
        "abstract": "",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Combining two boolean neural networks for image classification",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/ecc750bf1edc89d63a7514bdeebb41221bc072ad"
                ],
                "doi": "10.1142/9789812816849_0019",
                "publication_date": "None"
            }
        ],
        "rank": 54
    },
    {
        "authors": [
            "Jianwen Lou",
            "Yiming Wang",
            "C. Nduka",
            "M. Hamedi",
            "I. Mavridou",
            "Fei-Yue Wang",
            "Hui Yu"
        ],
        "title": "Realistic Facial Expression Reconstruction for VR HMD Users",
        "publication_date": "2020-03-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Transactions on Multimedia",
        "volume": "22",
        "doi": "10.1109/TMM.2019.2933338",
        "urls": [
            "https://www.semanticscholar.org/paper/071af7f6eb4a74c347d95fee9a02fdc7ccd1b0eb",
            "https://pure.port.ac.uk/ws/files/15356816/Realistic_facial_expression_reconstruction.pdf"
        ],
        "id": "id-6442942626010618024",
        "abstract": "We present a system for sensing and reconstructing facial expressions of the virtual reality (VR) head-mounted display (HMD) user. The HMD occludes a large portion of the user's face, which makes most existing facial performance capturing techniques intractable. To tackle this problem, a novel hardware solution with electromyography (EMG) sensors being attached to the headset frame is applied to track facial muscle movements. For realistic facial expression recovery, we first reconstruct the user's 3D face from a single image and generate the personalized blendshapes associated with seven facial action units (AUs) on the most emotionally salient facial parts (ESFPs). We then utilize pre-processed EMG signals for measuring activations of AU-coded facial expressions to drive pre-built personalized blendshapes. Since facial expressions appear as important nonverbal cues of the subject's internal emotional states, we further investigate the relationship between six basic emotions - anger, disgust, fear, happiness, sadness and surprise, and detected AUs using a fern classifier. Experiments show the proposed system can accurately sense and reconstruct high-fidelity common facial expressions while providing useful information regarding the emotional state of the HMD user.",
        "versions": [
            {
                "year": 2020,
                "source": "SupportedSources.OPENALEX",
                "title": "Realistic Facial Expression Reconstruction for VR HMD Users",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2968749391",
                    "https://doi.org/10.1109/tmm.2019.2933338",
                    "https://pure.port.ac.uk/ws/files/15356816/Realistic_facial_expression_reconstruction.pdf"
                ],
                "doi": "10.1109/tmm.2019.2933338",
                "publication_date": "2020-03-01 00:00:00"
            }
        ],
        "rank": 55
    },
    {
        "authors": [
            "G. Goltz",
            "E. H. Shiguemori",
            "H. Velho"
        ],
        "title": "Position Estimation Of UAV By Image Processing With Neural Networks",
        "publication_date": "2016-03-16 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "ChemBioChem",
        "volume": "",
        "doi": "10.21528/CBIC2011-03.6",
        "urls": [
            "https://www.semanticscholar.org/paper/889fdc66368e463675456667ee79b16638b8d891",
            "http://abricom.org.br/wp-content/uploads/2016/03/st_03.6.pdf"
        ],
        "id": "id8941303854038554314",
        "abstract": "Abstract \u2013 This paper presents a study of three artificial neural networks with supervised training and different architectures: network with radial basis function, multilayer perceptron and cellular neural network. These networks were applied to edge detection in aerial and satellite images, for later correlation calculation in spatial domain between these images to simulate the estimation of the geographical position of a unmanned aerial vehicle UAV. The neural networks results were compared with Sobel and Canny operators.",
        "versions": [
            {
                "year": 2016,
                "source": "SupportedSources.CROSSREF",
                "title": "Position Estimation Of UAV By Image Processing With Neural Networks",
                "journal": "",
                "urls": [
                    "http://dx.doi.org/10.21528/cbic2011-03.6"
                ],
                "doi": "10.21528/cbic2011-03.6",
                "publication_date": "2016-03-16 00:00:00"
            }
        ],
        "rank": 56
    },
    {
        "authors": [
            "Gao, Wen"
        ],
        "title": "Sonar sensor interpretation for ectogeneous robots",
        "publication_date": "2005-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/235408331.pdf"
        ],
        "id": "id8083437009988136653",
        "abstract": "We have developed four generations of sonar scanning systems to automatically interpret surrounding environment. The first two are stationary 3D air-coupled ultrasound scanning systems and the last two are packaged as sensor heads for mobile robots. Template matching analysis is applied to distinguish simple indoor objects. It is conducted by comparing the tested echo with the reference echoes. Important features are then extracted and drawn in the phase plane. The computer then analyzes them and gives the best choices of the tested echoes automatically. For cylindrical objects outside, an algorithm has been presented to distinguish trees from smooth circular poles based on analysis of backscattered sonar echoes. The echo data is acquired by a mobile robot which has a 3D air-coupled ultrasound scanning system packaged as the sensor head. Four major steps are conducted. The final Average Asymmetry-Average Squared Euclidean Distance phase plane is segmented to tell a tree from a pole by the location of the data points for the objects interested. For extended objects outside, we successfully distinguished seven objects in the campus by taking a sequence scans along each object, obtaining the corresponding backscatter vs. scan angle plots, forming deformable template matching, extracting interesting feature vectors and then categorizing them in a hyper-plane. We have also successfully taught the robot to distinguish three pairs of objects outside. Multiple scans are conducted at different distances. A two-step feature extraction is conducted based on the amplitude vs. scan angle plots. The final Slope1 vs. Slope2 phase plane not only separates the rectangular objects from the corresponding cylindrical",
        "versions": [],
        "rank": 57
    },
    {
        "authors": [
            "Karen L\u00f3pez-Linares",
            "Maialen Stephens",
            "Inmaculada Garc\u00eda",
            "I. Mac\u00eda",
            "M. Ballester",
            "Ra\u00fal San Jos\u00e9 Est\u00e9par"
        ],
        "title": "Abdominal Aortic Aneurysm Segmentation Using Convolutional Neural Networks Trained with Images Generated with a Synthetic Shape Model",
        "publication_date": "2019-10-13 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Machine learning and medical engineering for cardiovascular health and intravascular imaging and computer assisted stenting : first International Workshop, MLMECH 2019, and 8th Joint International Workshop, CVII-STENT 2019, held in conj...",
        "volume": "11794",
        "doi": "10.1007/978-3-030-33327-0_20",
        "urls": [
            "https://www.semanticscholar.org/paper/36d870e11d9af8ad71fc70a7c355b70e0d953f0a"
        ],
        "id": "id4255686628059944913",
        "abstract": null,
        "versions": [],
        "rank": 58
    },
    {
        "authors": [
            "Shaubari, Ezak Fadzrin Ahmad"
        ],
        "title": "Automatic segmentation of the human thigh muscles in magnetic resonanceimaging",
        "publication_date": "2018-06-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/161893939.pdf"
        ],
        "id": "id-2639162646024422012",
        "abstract": "Advances in magnetic resonance imaging (MRI) and analysis techniques have improved\n\ndiagnosis and patient treatment pathways. Typically, image analysis requires substantial\n\ntechnical and medical expertise and MR images can su\u21b5er from artefacts, echo and\n\nintensity inhomogeneity due to gradient pulse eddy currents and inherent e\u21b5ects of pulse\n\nradiation on MRI radio frequency (RF) coils that complicates the analysis. Processing\n\nand analysing serial sections of MRI scans to measure tissue volume is an additional\n\nchallenge as the shapes and the borders between neighbouring tissues change significantly\n\nby anatomical location. Medical imaging solutions are needed to avoid laborious manual\n\nsegmentation of specified regions of interest (ROI) and operator errors.\n\nThe work set out in this thesis has addressed this challenge with a specific focus on\n\nskeletal muscle segmentation of the thigh. The aim was to develop an MRI segmentation\n\nframework for the quadriceps muscles, femur and bone marrow. Four contributions of\n\nthis research include: (1) the development of a semi-automatic segmentation framework\n\nfor a single transverse-plane image; (2) automatic segmentation of a single transverseplane\n\nimage; (3) the automatic segmentation of multiple contiguous transverse-plane\n\nimages from a full MRI thigh scan; and (4) the use of deep learning for MRI thigh\n\nquadriceps segmentation.\n\nNovel image processing, statistical analysis and machine learning algorithms were developed\n\nfor all solutions and they were compared against current gold-standard manual\n\nsegmentation. Frameworks (1) and (3) require minimal input from the user to delineate\n\nthe muscle border. Overall, the frameworks in (1), (2) and (3) o\u21b5er very good\n\noutput performance, with respective framework\u2019s mean segmentation accuracy by JSI\n\nand processing time of: (1) 0.95 and 17 sec; (2) 0.85 and 22 sec; and (3) 0.93 and 3 sec.\n\nFor the framework in (4), the ImageNet trained model was customized by replacing the\n\nfully-connected layers in its architecture to convolutional layers (hence the name of Fully\n\nConvolutional Network (FCN)) and the pre-trained model was transferred for the ROI\n\nsegmentation task. With the implementation of post-processing for image filtering and\n\nmorphology to the segmented ROI, we have successfully accomplished a new benchmark\n\nfor thigh MRI analysis. The mean accuracy and processing time with this framework\n\nare 0.9502 (by JSI ) and 0.117 sec per image, respectively",
        "versions": [],
        "rank": 59
    },
    {
        "authors": [
            "Yuanchao Zhang",
            "Tung Ping Su",
            "Bing Liu",
            "Yuan Zhou",
            "Kun Hsien Chou",
            "Chun-Yi Zac Lo",
            "Chia-Chun Hung",
            "Weiling Chen",
            "Tianzi Jiang",
            "Ching Po Lin"
        ],
        "title": "Disrupted thalamo-cortical connectivity in schizophrenia: A morphometric correlation analysis",
        "publication_date": "2014-03-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "153",
        "doi": "10.1016/j.schres.2014.01.023",
        "urls": [
            "https://openalex.org/W2005034087",
            "https://doi.org/10.1016/j.schres.2014.01.023"
        ],
        "id": "id-6155156759675846280",
        "abstract": "",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Disrupted thalamo-cortical connectivity in schizophrenia: A morphometric correlation analysis",
                "journal": "Schizophrenia Research",
                "urls": [
                    "https://www.semanticscholar.org/paper/60e58e68f28e82c6399f5e5a8cafd9ca5223d752"
                ],
                "doi": "10.1016/j.schres.2014.01.023",
                "publication_date": "2014-03-01 00:00:00"
            }
        ],
        "rank": 60
    },
    {
        "authors": [
            "Jingyi Xi",
            "Lin Luo",
            "Jinlong Li",
            "Yu Zhang",
            "Yong Wang",
            "Dan Jiang"
        ],
        "title": "An attention-based residual neural network for deformable image registration",
        "publication_date": "2022-12-15 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1117/12.2653313",
        "urls": [
            "https://www.semanticscholar.org/paper/c53bd3f4b873e163d27f3083dfe6408a264cc00e"
        ],
        "id": "id-437661721675532207",
        "abstract": "Deformable image registration is a basic image processing task, especially widely used in medical image processing and analysis. Different from rigid registration, its purpose is to find the optimal nonlinear transformation between two images and establish corresponding relationship, so as to achieve image consistency. In recent years, deformable registration methods based on deep learning have been studied a lot. Compared with traditional methods, they show great advantages in registration performance. This paper proposes an attention-based residual neural network for deformable image registration, which utilizes the U-Net encoder-decoder structure to design a convolutional neural network to predict the deformation field, and uses the residual module and attention mechanism enhances the ability of the model to extract features, and finally uses the spatial transformation function to obtain the registered image, and the entire network is trained in an unsupervised manner. We conducted experiments on the MNIST dataset and 2D brain magnetic resonance images (MRI) respectively. The experimental results show that the deformable registration network proposed in this paper has good performance and shows good results in registration accuracy.",
        "versions": [
            {
                "year": 2022,
                "source": "SupportedSources.CROSSREF",
                "title": "An attention-based residual neural network for deformable image registration",
                "journal": "",
                "urls": [
                    "http://dx.doi.org/10.1117/12.2653313"
                ],
                "doi": "10.1117/12.2653313",
                "publication_date": "2022-12-15 00:00:00"
            },
            {
                "year": 0,
                "source": "SupportedSources.UNPAYWALL",
                "title": "An attention-based residual neural network for deformable image registration",
                "journal": "Thirteenth International Conference on Information Optics and Photonics (CIOP 2022)",
                "urls": [
                    "https://doi.org/10.1117/12.2653313"
                ],
                "doi": "10.1117/12.2653313",
                "publication_date": "None"
            }
        ],
        "rank": 61
    },
    {
        "authors": [
            "Srikrishna Jaganathan",
            "Jian Wang",
            "Anja Borsdorf",
            "Karthik Shetty",
            "Andreas Maier"
        ],
        "title": "Deep Iterative 2D/3D Registration",
        "publication_date": "2021-07-21 10:51:29+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "10.1007/978-3-030-87202-1_37",
        "urls": [
            "http://arxiv.org/pdf/2107.10004v1",
            "http://dx.doi.org/10.1007/978-3-030-87202-1_37",
            "http://arxiv.org/abs/2107.10004v1",
            "http://arxiv.org/pdf/2107.10004v1"
        ],
        "id": "id177810371678628805",
        "abstract": "Deep Learning-based 2D/3D registration methods are highly robust but often\nlack the necessary registration accuracy for clinical application. A refinement\nstep using the classical optimization-based 2D/3D registration method applied\nin combination with Deep Learning-based techniques can provide the required\naccuracy. However, it also increases the runtime. In this work, we propose a\nnovel Deep Learning driven 2D/3D registration framework that can be used\nend-to-end for iterative registration tasks without relying on any further\nrefinement step. We accomplish this by learning the update step of the 2D/3D\nregistration framework using Point-to-Plane Correspondences. The update step is\nlearned using iterative residual refinement-based optical flow estimation, in\ncombination with the Point-to-Plane correspondence solver embedded as a known\noperator. Our proposed method achieves an average runtime of around 8s, a mean\nre-projection distance error of 0.60 $\\pm$ 0.40 mm with a success ratio of 97\npercent and a capture range of 60 mm. The combination of high registration\naccuracy, high robustness, and fast runtime makes our solution ideal for\nclinical applications.",
        "versions": [],
        "rank": 62
    },
    {
        "authors": [
            "Bhatt, Ravi R.",
            "Gadewar, Shruti P.",
            "Gari, Iyad Ba",
            "Jahanshad, Neda",
            "Javid, Shayan",
            "Nourollahimoghadam, Elnaz",
            "Ramesh, Abhinaav",
            "Thomopoulos, Sophia",
            "Thompson, Paul M.",
            "Zhu, Alyssa H."
        ],
        "title": "A Comprehensive Corpus Callosum Segmentation Tool for Detecting Callosal  Abnormalities and Genetic Associations from Multi Contrast MRIs",
        "publication_date": "2023-05-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "http://arxiv.org/abs/2305.01107"
        ],
        "id": "id-9197751989015896033",
        "abstract": "Structural alterations of the midsagittal corpus callosum (midCC) have been\nassociated with a wide range of brain disorders. The midCC is visible on most\nMRI contrasts and in many acquisitions with a limited field-of-view. Here, we\npresent an automated tool for segmenting and assessing the shape of the midCC\nfrom T1w, T2w, and FLAIR images. We train a UNet on images from multiple public\ndatasets to obtain midCC segmentations. A quality control algorithm is also\nbuilt-in, trained on the midCC shape features. We calculate intraclass\ncorrelations (ICC) and average Dice scores in a test-retest dataset to assess\nsegmentation reliability. We test our segmentation on poor quality and partial\nbrain scans. We highlight the biological significance of our extracted features\nusing data from over 40,000 individuals from the UK Biobank; we classify\nclinically defined shape abnormalities and perform genetic analyses",
        "versions": [],
        "rank": 63
    },
    {
        "authors": [
            "Dong Wei",
            "Ying Sun",
            "Sim-Heng Ong",
            "Ping Chai",
            "Lynette L. Teo",
            "Adrian F. Low"
        ],
        "title": "Three-Dimensional Segmentation of the Left Ventricle in Late Gadolinium Enhanced MR Images of Chronic Infarction Combining Long- and Short-Axis Information",
        "publication_date": "2022-05-21 09:47:50+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "10.1016/j.media.2013.03.001",
        "urls": [
            "http://arxiv.org/pdf/2205.10548v1",
            "http://dx.doi.org/10.1016/j.media.2013.03.001",
            "http://arxiv.org/abs/2205.10548v1",
            "http://arxiv.org/pdf/2205.10548v1"
        ],
        "id": "id-5244628941049089334",
        "abstract": "Automatic segmentation of the left ventricle (LV) in late gadolinium enhanced\n(LGE) cardiac MR (CMR) images is difficult due to the intensity heterogeneity\narising from accumulation of contrast agent in infarcted myocardium. In this\npaper, we present a comprehensive framework for automatic 3D segmentation of\nthe LV in LGE CMR images. Given myocardial contours in cine images as a priori\nknowledge, the framework initially propagates the a priori segmentation from\ncine to LGE images via 2D translational registration. Two meshes representing\nrespectively endocardial and epicardial surfaces are then constructed with the\npropagated contours. After construction, the two meshes are deformed towards\nthe myocardial edge points detected in both short-axis and long-axis LGE images\nin a unified 3D coordinate system. Taking into account the intensity\ncharacteristics of the LV in LGE images, we propose a novel parametric model of\nthe LV for consistent myocardial edge points detection regardless of\npathological status of the myocardium (infarcted or healthy) and of the type of\nthe LGE images (short-axis or long-axis). We have evaluated the proposed\nframework with 21 sets of real patient and 4 sets of simulated phantom data.\nBoth distance- and region-based performance metrics confirm the observation\nthat the framework can generate accurate and reliable results for myocardial\nsegmentation of LGE images. We have also tested the robustness of the framework\nwith respect to varied a priori segmentation in both practical and simulated\nsettings. Experimental results show that the proposed framework can greatly\ncompensate variations in the given a priori knowledge and consistently produce\naccurate segmentations.",
        "versions": [],
        "rank": 64
    },
    {
        "authors": [
            "Marco Reisert",
            "Maximilian Russe",
            "Samer Elsheikh",
            "Elias Kellner",
            "Henrik Skibbe"
        ],
        "title": "Deep Neural Patchworks: Coping with Large Segmentation Tasks",
        "publication_date": "2022-06-07 12:07:18+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "",
        "urls": [
            "http://arxiv.org/pdf/2206.03210v1",
            "http://arxiv.org/abs/2206.03210v1",
            "http://arxiv.org/pdf/2206.03210v1"
        ],
        "id": "id-1941771359687692668",
        "abstract": "Convolutional neural networks are the way to solve arbitrary image\nsegmentation tasks. However, when images are large, memory demands often exceed\nthe available resources, in particular on a common GPU. Especially in\nbiomedical imaging, where 3D images are common, the problems are apparent. A\ntypical approach to solve this limitation is to break the task into smaller\nsubtasks by dividing images into smaller image patches. Another approach, if\napplicable, is to look at the 2D image sections separately, and to solve the\nproblem in 2D. Often, the loss of global context makes such approaches less\neffective; important global information might not be present in the current\nimage patch, or the selected 2D image section. Here, we propose Deep Neural\nPatchworks (DNP), a segmentation framework that is based on hierarchical and\nnested stacking of patch-based networks that solves the dilemma between global\ncontext and memory limitations.",
        "versions": [],
        "rank": 65
    },
    {
        "authors": [
            "Xuanyi Dong",
            "Shoou-I Yu",
            "Xinshuo Weng",
            "Shih-En Wei",
            "Yi Yang",
            "Yaser Sheikh"
        ],
        "title": "Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors",
        "publication_date": "2018-07-03 03:52:45+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "",
        "urls": [
            "http://arxiv.org/pdf/1807.00966v2",
            "http://arxiv.org/abs/1807.00966v2",
            "http://arxiv.org/pdf/1807.00966v2"
        ],
        "id": "id4497004540294401469",
        "abstract": "In this paper, we present supervision-by-registration, an unsupervised\napproach to improve the precision of facial landmark detectors on both images\nand video. Our key observation is that the detections of the same landmark in\nadjacent frames should be coherent with registration, i.e., optical flow.\nInterestingly, the coherency of optical flow is a source of supervision that\ndoes not require manual labeling, and can be leveraged during detector\ntraining. For example, we can enforce in the training loss function that a\ndetected landmark at frame$_{t-1}$ followed by optical flow tracking from\nframe$_{t-1}$ to frame$_t$ should coincide with the location of the detection\nat frame$_t$. Essentially, supervision-by-registration augments the training\nloss function with a registration loss, thus training the detector to have\noutput that is not only close to the annotations in labeled images, but also\nconsistent with registration on large amounts of unlabeled videos. End-to-end\ntraining with the registration loss is made possible by a differentiable\nLucas-Kanade operation, which computes optical flow registration in the forward\npass, and back-propagates gradients that encourage temporal coherency in the\ndetector. The output of our method is a more precise image-based facial\nlandmark detector, which can be applied to single images or video. With\nsupervision-by-registration, we demonstrate (1) improvements in facial landmark\ndetection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities),\nand (2) significant reduction of jittering in video detections.",
        "versions": [],
        "rank": 66
    },
    {
        "authors": [
            "Angelos Amanatiadis",
            "Evangelos Karakasis",
            "Loukas Bampis",
            "Stylianos Ploumpis",
            "Antonios Gasteratos"
        ],
        "title": "ViPED: On-road vehicle passenger detection for autonomous vehicles",
        "publication_date": "2019-02-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "112",
        "doi": "10.1016/j.robot.2018.12.002",
        "urls": [
            "https://openalex.org/W2903819884",
            "https://doi.org/10.1016/j.robot.2018.12.002"
        ],
        "id": "id8640273664952480258",
        "abstract": "",
        "versions": [
            {
                "year": 2019,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "ViPED: On-road vehicle passenger detection for autonomous vehicles",
                "journal": "Robotics Auton. Syst.",
                "urls": [
                    "https://www.semanticscholar.org/paper/9ef29d1bf323e53b1fc21cc8857050a7948a110d"
                ],
                "doi": "10.1016/j.robot.2018.12.002",
                "publication_date": "2019-02-01 00:00:00"
            }
        ],
        "rank": 67
    },
    {
        "authors": [
            "Qiegen Liu",
            "H. Leung"
        ],
        "title": "Log-Euclidean metric for robust multi-modal deformable registration",
        "publication_date": "2017-07-10 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.23919/ICIF.2017.8009688",
        "urls": [
            "https://www.semanticscholar.org/paper/f6699b2a9ad49f86d5ba65fbd85539e7f44f8c5b"
        ],
        "id": "id-47101775013356233",
        "abstract": "Registration of images from different modalities in the presence of intra-image fluctuation and noise contamination is a challenging task. The accuracy and robustness of the deformable registration largely depend on the definition of appropriate objective function, measuring the similarity between the images. Among them the multi-dimensional modality independent neighbourhood descriptor (MIND) is a promising method, yet its ability is limited by non-uniform bias fields and image noise, etc. Motivated by the fact that Log-Euclidean metric has promising invariance properties such as inversion invariant and similarity invariant, this paper introduces an objective function that embeds Log-Euclidean similarity metric between patches to form a multi-dimensional descriptor. The Gaussian-like penalty function consisting of the log-Euclidean metric between images to be registered is incorporated to better reflect the degree of preserving feature discriminability and structure ordering. Experimental results show the advantages of the proposed method over state-of-the-art techniques both quantitatively and qualitatively.",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.CROSSREF",
                "title": "Log-Euclidean metric for robust multi-modal deformable registration",
                "journal": "",
                "urls": [
                    "http://xplorestaging.ieee.org/ielx7/8002854/8009614/08009688.pdf?arnumber=8009688",
                    "http://dx.doi.org/10.23919/icif.2017.8009688"
                ],
                "doi": "10.23919/icif.2017.8009688",
                "publication_date": "2017-01-01 00:00:00"
            }
        ],
        "rank": 68
    },
    {
        "authors": [
            "Golyanik, V.",
            "Lassner, C.",
            "Tewari, A.",
            "Theobalt, C.",
            "Tretschk, E.",
            "Zollh\u00f6fer, M."
        ],
        "title": "Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video",
        "publication_date": "2020-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/387901368.pdf"
        ],
        "id": "id-8113497539448097164",
        "abstract": "In this tech report, we present the current state of our ongoing work on reconstructing Neural Radiance Fields (NERF) of general non-rigid scenes via ray bending. Non-rigid NeRF (NR-NeRF) takes RGB images of a deforming object (e.g., from a monocular video) as input and then learns a geometry and appearance representation that not only allows to reconstruct the input sequence but also to re-render any time step into novel camera views with high fidelity. In particular, we show that a consumer-grade camera is sufficient to synthesize convincing bullet-time videos of short and simple scenes. In addition, the resulting representation enables correspondence estimation across views and time, and provides rigidity scores for each point in the scene. We urge the reader to watch the supplemental videos for qualitative results. We will release our code",
        "versions": [],
        "rank": 69
    },
    {
        "authors": [
            "Han Zhang",
            "W. Ni",
            "W. Yan",
            "D. Xiang",
            "Junzheng Wu",
            "Xiaoliang Yang",
            "Hui Bian"
        ],
        "title": "Registration of Multimodal Remote Sensing Image Based on Deep Fully Convolutional Neural Network",
        "publication_date": "2019-06-04 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "volume": "12",
        "doi": "10.1109/JSTARS.2019.2916560",
        "urls": [
            "https://www.semanticscholar.org/paper/e252f5cef8ac0d84f32c2cab38b15b438d40041a"
        ],
        "id": "id-4826348028749060860",
        "abstract": "Multimodal image registration is the fundamental technique for scene analysis with series remote sensing images of different spectrum region. Due to the highly nonlinear radiometric relationship, it is quite challenging to find common features between images of different modal types. This paper resorts to the deep neural network, and tries to learn descriptors for multimodal image patch matching, which is the key issue of image registration. A Siamese fully convolutional network is set up and trained with a novel loss function, which adopts the strategy of maximizing the feature distance between positive and hard negative samples. The two branches of the Siamese network are connected by the convolutional operation, resulting in the similarity score between the two input image patches. The similarity score value is used, not only for correspondence point location, but also for outlier identification. A generalized workflow for deep feature based multimodal RS image registration is constructed, including the training data curation, candidate feature point generation, and outlier removal. The proposed network is tested on a variety of optical, near infrared, thermal infrared, SAR, and map images. Experiment results verify the superiority over other state-of-the-art approaches.",
        "versions": [],
        "rank": 70
    },
    {
        "authors": [
            "Kainmueller, D."
        ],
        "title": "Deformable Meshes for Automatic Segmentation",
        "publication_date": "2014-08-19 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-3-658-07015-1_3",
        "urls": [
            "http://link.springer.com/content/pdf/10.1007/978-3-658-07015-1_3",
            "http://dx.doi.org/10.1007/978-3-658-07015-1_3"
        ],
        "id": "id-3220214176892536306",
        "abstract": "",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Deformable Meshes for Automatic Segmentation",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/a9171586ab52faab8d9b5a1d7dcbc7603e4b3420"
                ],
                "doi": "10.1007/978-3-658-07015-1_3",
                "publication_date": "None"
            }
        ],
        "rank": 71
    },
    {
        "authors": [
            "Zou, J.",
            "Liu, L.",
            "Song, Y.",
            "Choi, K.",
            "Qin, J."
        ],
        "title": "Deformable Lung CT Registration by\u00a0Decomposing Large Deformation",
        "publication_date": "2022-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-3-031-11203-4_20",
        "urls": [
            "https://link.springer.com/content/pdf/10.1007/978-3-031-11203-4_20",
            "http://dx.doi.org/10.1007/978-3-031-11203-4_20"
        ],
        "id": "id-557757961503048993",
        "abstract": "",
        "versions": [
            {
                "year": 0,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Deformable Lung CT Registration by Decomposing Large Deformation",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/ef3bc373c8f9bb02886eaced7b07ce7112a47baf"
                ],
                "doi": "10.1007/978-3-031-11203-4_20",
                "publication_date": "None"
            }
        ],
        "rank": 72
    },
    {
        "authors": [
            "Xia, Yingda"
        ],
        "title": "Towards Robust Deep Learning for Medical Image Analysis",
        "publication_date": "2022-07-25 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/527750973.pdf"
        ],
        "id": "id-7082954026051988720",
        "abstract": "Multi-dimensional medical data are rapidly collected to enhance healthcare. With the recent advance in artificial intelligence, deep learning techniques have been widely applied to medical images, constituting a significant proportion of medical data. The techniques of automated medical image analysis have the potential to benefit general clinical procedures, e.g., disease screening, malignancy diagnosis, patient risk prediction, and surgical planning. Although preliminary success takes place, the robustness of these approaches requires to be cautiously validated and sufficiently guaranteed before their application to real-world clinical problems. \nIn this thesis, we propose different approaches to improve the robustness of deep learning algorithms for automated medical image analysis. (i) In terms of network architecture, we leverage the advantages of both 2D and 3D networks, and propose an alternative 2.5D approach for 3D organ segmentation. (ii) To improve data efficiency and utilize large-scale unlabeled medical data, we propose a unified framework for semi-supervised medical image segmentation and domain adaptation. (iii) For the safety-critical applications, we design a unified approach for failure detection and anomaly segmentation. (iv) We study the problem of Federated Learning, which enables collaborative learning and preserves data privacy, and improve the robustness of the algorithm in the non-i.i.d setting. (v) We incorporate multi-phase information for more accurate pancreatic tumor detection. (vi) Finally, we show our discovery for potential pancreatic cancer screening on non-contrast CT scans which outperform expert radiologists",
        "versions": [],
        "rank": 73
    },
    {
        "authors": [
            "Gede Putra Kusuma",
            "Chin-Seng Chua"
        ],
        "title": "PCA-based image recombination for multimodal 2D+3D face recognition",
        "publication_date": "2011-04-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "29",
        "doi": "10.1016/j.imavis.2010.12.003",
        "urls": [
            "https://openalex.org/W1998481243",
            "https://doi.org/10.1016/j.imavis.2010.12.003"
        ],
        "id": "id1247351313216969103",
        "abstract": "",
        "versions": [
            {
                "year": 2011,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "PCA-based image recombination for multimodal 2D\u00a0+\u00a03D face recognition",
                "journal": "Image Vis. Comput.",
                "urls": [
                    "https://www.semanticscholar.org/paper/22fcf74fd2985165d8464adfd93d0770eab9b440"
                ],
                "doi": "10.1016/j.imavis.2010.12.003",
                "publication_date": "2011-04-01 00:00:00"
            }
        ],
        "rank": 74
    },
    {
        "authors": [
            "J. Lee",
            "Peng Liu",
            "Jun Cheng",
            "H. Fu"
        ],
        "title": "A Deep Step Pattern Representation for Multimodal Retinal Image Registration",
        "publication_date": "2019-10-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "10.1109/ICCV.2019.00518",
        "urls": [
            "https://www.semanticscholar.org/paper/d19f0404bbeb6fa8ae4c40e7a5b6c439277030c4"
        ],
        "id": "id6538033216009424804",
        "abstract": "This paper presents a novel feature-based method that is built upon a convolutional neural network (CNN) to learn the deep representation for multimodal retinal image registration. We coined the algorithm deep step patterns, in short DeepSPa. Most existing deep learning based methods require a set of manually labeled training data with known corresponding spatial transformations, which limits the size of training datasets. By contrast, our method is fully automatic and scale well to different image modalities with no human intervention. We generate feature classes from simple step patterns within patches of connecting edges formed by vascular junctions in multiple retinal imaging modalities. We leverage CNN to learn and optimize the input patches to be used for image registration. Spatial transformations are estimated based on the output possibility of the fully connected layer of CNN for a pair of images. One of the key advantages of the proposed algorithm is its robustness to non-linear intensity changes, which widely exist on retinal images due to the difference of acquisition modalities. We validate our algorithm on extensive challenging datasets comprising poor quality multimodal retinal images which are adversely affected by pathologies (diseases), speckle noise and low resolutions. The experimental results demonstrate the robustness and accuracy over state-of-the-art multimodal image registration algorithms.",
        "versions": [],
        "rank": 75
    },
    {
        "authors": [
            "Jianhua Zhang",
            "Lei Chen",
            "Xiaoyan Wang",
            "Z. Teng",
            "A. Brown",
            "J. Gillard",
            "Q. Guan",
            "Shengyong Chen"
        ],
        "title": "Compounding Local Invariant Features and Global Deformable Geometry for Medical Image Registration",
        "publication_date": "2014-08-28 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "PLoS ONE",
        "volume": "9",
        "doi": "10.1371/journal.pone.0105815",
        "urls": [
            "https://www.semanticscholar.org/paper/803ce145a73c2f5d5275e1f7eea4880ff1b90b0b",
            "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0105815&type=printable"
        ],
        "id": "id-5658698845883178638",
        "abstract": "Using deformable models to register medical images can result in problems of initialization of deformable models and robustness and accuracy of matching of inter-subject anatomical variability. To tackle these problems, a novel model is proposed in this paper by compounding local invariant features and global deformable geometry. This model has four steps. First, a set of highly-repeatable and highly-robust local invariant features, called Key Features Model (KFM), are extracted by an effective matching strategy. Second, local features can be matched more accurately through the KFM for the purpose of initializing a global deformable model. Third, the positional relationship between the KFM and the global deformable model can be used to precisely pinpoint all landmarks after initialization. And fourth, the final pose of the global deformable model is determined by an iterative process with a lower time cost. Through the practical experiments, the paper finds three important conclusions. First, it proves that the KFM can detect the matching feature points well. Second, the precision of landmark locations adjusted by the modeled relationship between KFM and global deformable model is greatly improved. Third, regarding the fitting accuracy and efficiency, by observation from the practical experiments, it is found that the proposed method can improve % of the fitting accuracy and reduce around 50% of the computational time compared with state-of-the-art methods.",
        "versions": [
            {
                "year": 2014,
                "source": "SupportedSources.CROSSREF",
                "title": "Compounding Local Invariant Features and Global Deformable Geometry for Medical Image Registration",
                "journal": "",
                "urls": [
                    "http://dx.plos.org/10.1371/journal.pone.0105815",
                    "http://dx.doi.org/10.1371/journal.pone.0105815"
                ],
                "doi": "10.1371/journal.pone.0105815",
                "publication_date": "2014-08-28 00:00:00"
            }
        ],
        "rank": 76
    },
    {
        "authors": [
            "Qi Dou",
            "Lequan Yu",
            "Hao Chen",
            "Yueming Jin",
            "Xin Yang",
            "Jing Qin",
            "Pheng-Ann Heng"
        ],
        "title": "3D deeply supervised network for automated segmentation of volumetric medical images",
        "publication_date": "2017-10-01 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "41",
        "doi": "10.1016/j.media.2017.05.001",
        "urls": [
            "https://openalex.org/W2613041730",
            "https://doi.org/10.1016/j.media.2017.05.001"
        ],
        "id": "id-1156658024451828450",
        "abstract": "",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "3D deeply supervised network for automated segmentation of volumetric medical images",
                "journal": "Medical Image Analysis",
                "urls": [
                    "https://www.semanticscholar.org/paper/b31192d40ef06eafc80fef13698ef765bf8e0e68"
                ],
                "doi": "10.1016/j.media.2017.05.001",
                "publication_date": "2017-10-01 00:00:00"
            }
        ],
        "rank": 77
    },
    {
        "authors": [
            "Sarmadi, Hamid"
        ],
        "title": "T\u00e9cnicas de coste reducido para el posicionamiento del paciente en radioterapia percut\u00e1nea utilizando un sistema de im\u00e1genes \u00f3pticas",
        "publication_date": "2021-01-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": null,
        "urls": [
            "https://core.ac.uk/download/427728108.pdf"
        ],
        "id": "id-5802275045865134905",
        "abstract": "Patient positioning is an important part of radiation therapy which is one of the main solutions for the treatment of malignant tissue in the human body. Currently, the most common patient positioning methods expose healthy tissue of the patient's body to extra dangerous radiations. Other non-invasive positioning methods are either not very accurate or are very costly for an average hospital. In this thesis, we explore the possibility of developing a system comprised of affordable hardware and advanced computer vision algorithms that facilitates patient positioning. Our algorithms are based on the usage of affordable RGB-D sensors, image features, ArUco planar markers, and other geometry registration methods. Furthermore, we take advantage of consumer-level computing hardware to make our systems widely accessible. More specifically, we avoid the usage of approaches that need to take advantage of dedicated GPU hardware for general-purpose computing since they are more costly. In different publications, we explore the usage of the mentioned tools to increase the accuracy of reconstruction/localization of the patient in its pose. We also take into account the visualization of the patient's target position with respect to their current position in order to assist the person who performs patient positioning. Furthermore, we make usage of augmented reality in conjunction with a real-time 3D tracking algorithm for better interaction between the program and the operator. We also solve more fundamental problems about ArUco markers that could be used in the future to improve our systems. These include highquality multi-camera calibration and mapping using ArUco markers plus detection of these markers in event cameras which are very useful in the presence of fast camera movement. In the end, we conclude that it is possible to increase the accuracy of 3D reconstruction and localization by combining current computer vision algorithms with fiducial planar markers with RGB-D sensors. This is reflected in the low amount of error we have achieved in our experiments for patient positioning, pushing forward the state of the art for this application.En el tratamiento de tumores malignos en el cuerpo, el posicionamiento del paciente en las sesiones de radioterapia es una cuesti\u00f3n crucial. Actualmente, los m\u00e9todos m\u00e1s comunes de posicionamiento del paciente exponen tejido sano del mismo a radiaciones peligrosas debido a que no es posible asegurar que la posici\u00f3n del paciente siempre sea la misma que la que tuvo cuando se planific\u00f3 la zona a radiar. Los m\u00e9todos que se usan actualmente, o no son precisos o tienen costes que los hacen inasequibles para ser usados en hospitales con financiaci\u00f3n limitada. En esta Tesis hemos analizado la posibilidad de desarrollar un sistema compuesto por hardware de bajo coste y m\u00e9todos avanzados de visi\u00f3n por ordenador que ayuden a que el posicionamiento del paciente sea el mismo en las diferentes sesiones de radioterapia, con respecto a su pose cuando fue se planific\u00f3 la zona a radiar. La soluci\u00f3n propuesta como resultado de la Tesis se basa en el uso de sensores RGB-D, caracter\u00edsticas extra\u00eddas de la imagen, marcadores cuadrados denominados ArUco y m\u00e9todos de registro de la geometr\u00eda en la imagen. Adem\u00e1s, en la soluci\u00f3n propuesta, se aprovecha la existencia de hardware convencional de bajo coste para hacer nuestro sistema ampliamente accesible. M\u00e1s espec\u00edficamente, evitamos el uso de enfoques que necesitan aprovechar GPU, de mayores costes, para computaci\u00f3n de prop\u00f3sito general. Se han obtenido diferentes publicaciones para conseguir el objetivo final. Las mismas describen m\u00e9todos para aumentar la precisi\u00f3n de la reconstrucci\u00f3n y la localizaci\u00f3n del paciente en su pose, teniendo en cuenta la visualizaci\u00f3n de la posici\u00f3n ideal del paciente con respecto a su posici\u00f3n actual, para ayudar al profesional que realiza la colocaci\u00f3n del paciente. Tambi\u00e9n se han propuesto m\u00e9todos de realidad aumentada junto con algoritmos para seguimiento 3D en tiempo real para conseguir una mejor interacci\u00f3n entre el sistema ideado y el profesional que debe realizar esa labor. De forma a\u00f1adida, tambi\u00e9n se han propuesto soluciones para problemas fundamentales relacionados con el uso de marcadores cuadrados que han sido utilizados para conseguir el objetivo de la Tesis. Las soluciones propuestas pueden ser empleadas en el futuro para mejorar otros sistemas. Los problemas citados incluyen la calibraci\u00f3n y el mapeo multic\u00e1mara de alta calidad utilizando los marcadores y la detecci\u00f3n de estos marcadores en c\u00e1maras de eventos, que son muy \u00fatiles en presencia de movimientos r\u00e1pidos de la c\u00e1mara. Al final, concluimos que es posible aumentar la precisi\u00f3n de la reconstrucci\u00f3n y localizaci\u00f3n en 3D combinando los actuales algoritmos de visi\u00f3n por ordenador, que usan marcadores cuadrados de referencia, con sensores RGB-D. Los resultados obtenidos con respecto al error que el sistema obtiene al reproducir el posicionamiento del paciente suponen un importante avance en el estado del arte de este t\u00f3pico",
        "versions": [],
        "rank": 78
    },
    {
        "authors": [
            "Matthias Grimm",
            "Javier Esteban",
            "Mathias Unberath",
            "Nassir Navab"
        ],
        "title": "Pose-dependent weights and Domain Randomization for fully automatic X-ray to CT Registration",
        "publication_date": "2021-04-15 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://web.archive.org/web/20210419224840/https://arxiv.org/pdf/2011.07294v2.pdf"
        ],
        "id": "id-9025841735624399335",
        "abstract": "Fully automatic X-ray to CT registration requires a solid initialization to provide an initial alignment within the capture range of existing intensity-based registrations. This work adresses that need by providing a novel automatic initialization, which enables end to end registration. First, a neural network is trained once to detect a set of anatomical landmarks on simulated X-rays. A domain randomization scheme is proposed to enable the network to overcome the challenge of being trained purely on simulated data and run inference on real Xrays. Then, for each patient CT, a patient-specific landmark extraction scheme is used. It is based on backprojecting and clustering the previously trained networks predictions on a set of simulated X-rays. Next, the network is retrained to detect the new landmarks. Finally the combination of network and 3D landmark locations is used to compute the initialization using a perspective-n-point algorithm. During the computation of the pose, a weighting scheme is introduced to incorporate the confidence of the network in detecting the landmarks. The algorithm is evaluated on the pelvis using both real and simulated x-rays. The mean (+-standard deviation) target registration error in millimetres is 4.1 +- 4.3 for simulated X-rays with a success rate of 92% and 4.2 +- 3.9 for real X-rays with a success rate of 86.8%, where a success is defined as a translation error of less than 30mm.",
        "versions": [
            {
                "year": 2020,
                "source": "SupportedSources.ARXIV",
                "title": "Pose-dependent weights and Domain Randomization for fully automatic X-ray to CT Registration",
                "journal": null,
                "urls": [
                    "http://arxiv.org/pdf/2011.07294v2",
                    "http://arxiv.org/abs/2011.07294v2",
                    "http://arxiv.org/pdf/2011.07294v2"
                ],
                "doi": "",
                "publication_date": "2020-11-14 12:50:32+00:00"
            }
        ],
        "rank": 79
    },
    {
        "authors": [
            "Othman, A."
        ],
        "title": "Medical Image Thresholding Using Online Trained Neural Networks",
        "publication_date": "2012-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1007/978-3-642-34500-5_79",
        "urls": [
            "http://link.springer.com/content/pdf/10.1007/978-3-642-34500-5_79.pdf",
            "http://dx.doi.org/10.1007/978-3-642-34500-5_79"
        ],
        "id": "id2914572171883682569",
        "abstract": "",
        "versions": [
            {
                "year": 2012,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Medical Image Thresholding Using Online Trained Neural Networks",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/e7832c8c6f4db5ff2869eb51bf8f9a7977854fc4"
                ],
                "doi": "10.1007/978-3-642-34500-5_79",
                "publication_date": "2012-11-12 00:00:00"
            }
        ],
        "rank": 80
    },
    {
        "authors": [
            "Mohsen Ghafoorian",
            "N. Karssemeijer",
            "I. V. Uden",
            "F.\u2010E. Leeuw",
            "T. Heskes",
            "B. Ginneken",
            "E. Marchiori",
            "B. Platel"
        ],
        "title": "Fall 2015 Meeting of the NVPHBV",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://www.semanticscholar.org/paper/6368b6d0ede9eb21b624b90fc66bcaa2bfb8ef44"
        ],
        "id": "id3174976294325502404",
        "abstract": "Keynote Lecture 01: Amsterdam Computer Vision: from the lab to the real world Prof. dr. Theo Gevers Informatics Institute, University of Amsterdam, Science Park 904, 1098 XH, Amsterdam Email: th.gevers@uva.nl Today, the moment has come that computer vision technology is accurate and fast enough to be applied in an increasing number of applications such as image retrieval, face recognition and surveillance. In this talk, I will discuss the computer vision technology of three different UvA spin-offs converting scientific ideas and concepts, created in a lab environment, into commercial software to be used in real world scenarios for (1) object recognition (2) human behavior analysis and (3) 3D reconstruction. Keynote Lecture 02: CAD and risk models for coronary artery disease Dr.ir. Jouke Dijkstra Leiden University Medical Center, Lab for Clinical and Experimental Image Processing, Leiden Email: jouke.dijkstra@lumc.nl Heart diseases and more specifically coronary artery diseases are still one of the major causes of death in the western world. Imaging plays and important role in the diagnosis and treatment of coronary artery disease. Diagnostic imaging with computed tomography (CT) results in large 3D data sets which have to be inspected by the radiologist or cardiologist. Computer aided diagnosis approaches to analyze the individual arteries helps the medical doctors to look for suspicious regions in these arteries and to rule out regions which are not interesting to investigate. This information can be combined in risk models. Image processing also plays an important role during the treatment of coronary artery disease. It provides essential measurements for device selection like scaffold length and diameter and treatment planning like the exact location of the stent placement. In this lecture the different aspect of using advanced image processing techniques for the diagnoses and treatment of coronary artery disease will be presented. Why does synthesized data improve classification of multi-sequence medical images? Gijs van Tulder (work done with Marleen de Bruijne) Biomedical Imaging Group Rotterdam, Erasmus MC Email: g.vantulder@erasmusmc.nl Incomplete datasets can make it difficult to train a classifier, because most classifiers assume that the same data is available for all training samples. The classification and registration of incomplete multimodal medical images, such as multi-sequence MRI with missing sequences, can sometimes be improved by replacing the missing modalities with synthesized data. This may seem counter-intuitive: synthetic data is derived from data that is already available, so it does not add new information. Why can it still improve performance? We will discuss possible explanations, based on experiments with two classifiers (linear SVMs and random forests) and two synthesis models (neural networks and restricted Boltzmann machines) on data from a brain tumor segmentation challenge with multi-modal MRI scans. Computer-aided detection of early esophageal cancer Fons van de Sommen, Peter de With Eindhoven University of Technology, Dept. of Electrical Engineering, Video Coding & Architectures Email: fvdsommen@tue.nl, P.H.N.de.With@tue.nl Over the past decade, the imaging tools for endoscopists have improved drastically. This has enabled visual inspection of the intestinal tissue for early signs of malignant lesions. Furthermore, it has paved the way for image analysis algorithms, to support the gastroenterologist in finding these early signs of developing cancer. We explore various methods for characterizing and segmenting the malignant tissue in endoscopic imagery, where different color and texture features are compared and combined with machine learning methods such as Support Vector Machine (SVM) and Random Forests (RF). Multi-expert validation on 100 images of 39 patients shows promising results where both the sensitivity and specificity of the system exceed 0.80. Automatic detection of ductal carcinoma in situ in whole slide histopathological images Babak Ehteshami Bejnordi, Maschenka Balkenhol, Geert Litjens, Roland Holland, Peter Bult, Nico Karssemeijer, and Jeroen AWM van der Laak Departments of Radiology and Pathology, Radboud University Medical Center, Nijmegen, Netherlands Hamamatsu Tissue Imaging and Analysis Center, University of Heidelberg, Heidelberg, Germany Corresponding author: babak.ehteshamibejnordi@radboudumc.nl This study presents and evaluates a fully automatic method for detection of ductal carcinoma in situ (DCIS) in digitized hematoxylin and eosin (H&E) stained histopathological slides of breast tissue. The proposed method applies multi-scale super-pixel classification to detect regions of interest in wholeslide images (WSIs). Subsequently, spatial clustering is utilized to delineate regions representing meaningful structures within the tissue. A classifier employing statistical and structural texture features and architectural features is then trained to discriminate between DCIS and benign/normal structures. Evaluation was conducted both on the slide and the lesion level using (F)ROC analysis. The result of the per-slide evaluation shows a sensitivity of 95% and 100% at an average of 2 and 2.6 false positive detections per WSI, respectively. The results of the per-lesion evaluation show that it is possible to detect 80% and 83% of the DCIS lesions in an abnormal slide, at an average of 2.0 and 3.0 false positive detections per WSI, respectively. Collectively, the result of the experiments demonstrate the efficacy and accuracy of the proposed method as well as its potential for application in routine pathological diagnostics. Context-preserving deep convolutional neural networks for white matter hyper-intensity segmentation Mohsen Ghafoorian, Nico Karssemeijer, Inge W.M. van Uden, Frank-Erik de Leeuw, Tom Heskes, Bram van Ginneken, Elena Marchiori and Bram Platel Radboud University, Institute for Computing and Information Sciences, Nijmegen, the Netherlands Radboud University Medical Center, Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Nijmegen, the Netherlands Radboud University Medical Center, Donders Institute for Brain, Cognition and Behaviour, Department of Neurology, Nijmegen, the Netherlands Corresponding author: Mohsen.ghafoorian@radboudumc.nl The anatomical location of imaging features is of crucial importance for accurate diagnosis in many medical tasks. Convolutional neural networks (CNN) have had huge successes in computer vision, but they lack the natural ability to incorporate the anatomical location in their decision making process, hindering success in some medical image analysis tasks. In this work, to integrate the anatomical location information into the network, we propose several deep CNN architectures that consider multi-scale patches or take explicit location features while training. We apply and compare the proposed architectures for segmentation of white matter hyperintensities in brain MR images on a large dataset. As a result, we observe that the CNNs that incorporate location information substantially outperform a conventional segmentation method with hand-crafted features as well as CNNs that do not integrate location information. On a test set of 46 scans, the best configuration of our networks obtained a Dice score of 0.791, compared to 0.797 for an independent human observer. Performance levels of the machine and the independent human observer were not statistically significant (p-value=0.17). RetinaCheck, a new platform for early detection of diabetic retinopathy Behdad Dashtbozorg*, Fan Huang, Jiong Zhang, Samaneh Abbasi, Erik Bekkers, Bart ter Haar Romeny Department of Biomedical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands *Corresponding author: b.dasht.bozorg@tue.nl Automated detection of retinopathy in retinal images using digital image analysis methods has huge potential benefits. It offers the possibility of examination of a large number of images with time and cost savings and offers more objective measurements than current observation techniques. Advantages in a clinical context include the potential to perform automated screening for conditions such as diabetic retinopathy, and hence to reduce the workload required from manual trained graders. The high resolution fundus camera images generate a wealth of information, and due to the large size and huge numbers, specialized software is developed to do a fully automatic analysis and diagnosis. The integration of the many individual applications in one integrated system, both for the researcher and clinical user, is the main aim of RetinaCheck platform. RetinaCheck platform facilitates the automatic retinal image analysis by establishing a common repeatable procedure, thus increasing the performance and reliability of the entire analysis, and it helps with the image storage and management, allowing the collaboration between experts in different locations for the different studies. This platform includes several tools for automatic retinal image analysis, such as vessel segmentation and tracking, optic disc and fovea detection, optic segmentation, vessel caliber measurement, artery/vein classification, microaneurysm, hemorrhage and drusen detection, assessment of alternation in vessel calibers and etc. Several requirements were considered in the development of this software. This interface automatically computes several parameters from retinal images in a repeatable and objective manner. RetinaCheck allows the integration of new image processing modules easily and it will provide several features and tools to increase interactivity and usability. Using polynomial approximation to improve classification of spectral data Friedrich Melchert, Udo Seiffert, Michael Biehl University of Groningen, Johann Bernoulli Institute for Mathematics and Computer Science Fraunhofer Institute for Factory Operation and Automati",
        "versions": [],
        "rank": 81
    },
    {
        "authors": [
            "Cheng Zhi"
        ],
        "title": "A Non-Rigid Registration Algorithm of Large-Scale Deformable Models",
        "publication_date": "None",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Chinese Journal of Computers",
        "volume": "",
        "doi": "10.3724/sp.j.1016.2011.00539",
        "urls": [
            "https://www.semanticscholar.org/paper/77b33416fa6a5ab29b096c414d17f68274efff69"
        ],
        "id": "id6284218003957813615",
        "abstract": "For the discrete frames,non-rigid registration is the key problem to reconstruct one dynamic geometrical model,representing a time-varying dynamic object.The paper presents a robust pairwise non-rigid registration algorithm,which is mainly performed in two steps: explicit correspondence computation and global deformation optimization.Firstly,after an efficient detection and analysis of slippage features on the source and target models,the salient feature points are extracted unsupervised.Then the correspondences among the feature points are correctly established.Secondly,by using the explicit correspondences,the approach finishes the non-rigid registration by a single global deformation optimization.The optimization maximizes the region of overlap and the spatial coherence of the deformation while minimizing the optimized energy function.The algorithm is fully tested on a series of real data sets,which are available from two ways: captured by the scanner devices or manually created from synthetic deformation tools.The results demonstrate that the approach is highly versatile,and applicable to large-scale deformable scenes that could not be handled by alternative techniques assuming that inner-frame motions are small.",
        "versions": [
            {
                "year": 2011,
                "source": "SupportedSources.CROSSREF",
                "title": "A Non-Rigid Registration Algorithm of Large-Scale Deformable Models",
                "journal": "",
                "urls": [
                    "http://task.scichina.com/u/cms/www/pdf/0254-4164/2011/3/21342.pdf",
                    "http://dx.doi.org/10.3724/sp.j.1016.2011.00539"
                ],
                "doi": "10.3724/sp.j.1016.2011.00539",
                "publication_date": "2011-05-19 00:00:00"
            }
        ],
        "rank": 82
    },
    {
        "authors": [
            "M. Sartori",
            "J. W. Fernandez",
            "L. Modenese",
            "C. P. Carty",
            "L. A. Barber",
            "K. Oberhofer",
            "J. Zhang",
            "G. G. Handsfield",
            "N. S. Stott",
            "T. F. Besier",
            "D. Farina",
            "D. G. Lloyd"
        ],
        "title": "Toward modeling locomotion using electromyography-informed 3D models: application to cerebral palsy",
        "publication_date": "2016-12-21 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "Wiley",
        "volume": "",
        "doi": "10.1002/wsbm.1368",
        "urls": [
            "https://web.archive.org/web/20200505113529/https://research-repository.griffith.edu.au/bitstream/handle/10072/100850/SartoriPUB2689.pdf;jsessionid=55FD33857436EA5CAFB009CB1C18F076?sequence=1"
        ],
        "id": "id-6830635795589982941",
        "abstract": "This position paper proposes a modeling pipeline to develop clinically relevant neuromusculoskeletal models to understand and treat complex neurological disorders. Although applicable to a variety of neurological conditions we provide direct pipeline applicative examples in the context of cerebral palsy (CP). This paper highlights technologies in; (i) Patient-specific segmental rigid body models developed from Magnetic Resonance Imaging for use in inverse kinematics and inverse dynamics pipelines; (ii) Efficient population based approaches to derive skeletal models and muscle origins/insertions that are useful for population statistics and consistent creation of continuum models; (iii) Continuum muscle descriptions to account for complex muscle architecture including spatially varying material properties with muscle wrapping; (iv) Muscle and tendon properties specific to CP; and (v) Neural based electromyography-informed methods for muscle force prediction. This represents a novel modeling pipeline that couples for the first time electromyography extracted features of disrupted neuromuscular behavior with advanced numerical methods for modelling CP-specific musculoskeletal morphology and function. The translation of such pipeline to the clinical level will provide a new class of biomarkers that objectively describe the neuro-musculo-skeletal determinants of pathological locomotion and complement current clinical assessment techniques, which often rely on subjective judgment.",
        "versions": [
            {
                "year": 2017,
                "source": "SupportedSources.OPENALEX",
                "title": "Toward modeling locomotion using electromyography\u2010informed 3D models: application to cerebral palsy",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2580585293",
                    "https://doi.org/10.1002/wsbm.1368",
                    "https://research-repository.griffith.edu.au/bitstream/10072/100850/1/SartoriPUB2689.pdf"
                ],
                "doi": "10.1002/wsbm.1368",
                "publication_date": "2017-03-01 00:00:00"
            }
        ],
        "rank": 83
    },
    {
        "authors": [
            "Heerden, L.",
            "Visser, J.",
            "Koedooder, C.",
            "Rasch, C.",
            "Pieters, B.",
            "Bel, A."
        ],
        "title": "OC-0174: Deformable image registration for dose accumulation of adaptive EBRT and BT in cervical cancer",
        "publication_date": "2018-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1016/s0167-8140(18)30484-5",
        "urls": [
            "https://api.elsevier.com/content/article/PII:S0167814018304845?httpAccept=text/xml",
            "https://api.elsevier.com/content/article/PII:S0167814018304845?httpAccept=text/plain",
            "http://dx.doi.org/10.1016/s0167-8140(18)30484-5"
        ],
        "id": "id-5007201591229289453",
        "abstract": "",
        "versions": [
            {
                "year": 2018,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "OC-0174: Deformable image registration for dose accumulation of adaptive EBRT and BT in cervical cancer",
                "journal": "Radiotherapy and Oncology",
                "urls": [
                    "https://www.semanticscholar.org/paper/f05ea4ddbc957c11e09fc5064ed5bda976073630"
                ],
                "doi": "10.1016/S0167-8140(18)30484-5",
                "publication_date": "2018-04-01 00:00:00"
            }
        ],
        "rank": 84
    },
    {
        "authors": [
            "Xinyue Wang",
            "Yingyao He",
            "Kelong Lu",
            "Chenglong Deng",
            "Xinuo Qiao",
            "Ning Hao"
        ],
        "title": "How does the embodied metaphor affect creative thinking?",
        "publication_date": "2019-08-20 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "202",
        "doi": "10.1016/j.neuroimage.2019.116114",
        "urls": [
            "https://openalex.org/W2969233863",
            "https://doi.org/10.1016/j.neuroimage.2019.116114"
        ],
        "id": "id3229694081952860457",
        "abstract": "",
        "versions": [
            {
                "year": 2019,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "How does the embodied metaphor affect creative thinking?",
                "journal": "NeuroImage",
                "urls": [
                    "https://www.semanticscholar.org/paper/b7a74077ec5873c945ce872e73c6eebf2cc1b39b"
                ],
                "doi": "10.1016/j.neuroimage.2019.116114",
                "publication_date": "2019-08-20 00:00:00"
            }
        ],
        "rank": 85
    },
    {
        "authors": [
            "M. Lloyd-Hart",
            "R. Dekany",
            "D. Sandler",
            "D. Wittman",
            "R. Angel",
            "D. Mccarthy"
        ],
        "title": "Progress in diffraction-limited imaging at the Multiple Mirror Telescope with adaptive optics",
        "publication_date": "1994-02-01 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Journal of The Optical Society of America A-optics Image Science and Vision",
        "volume": "11",
        "doi": "10.1364/JOSAA.11.000846",
        "urls": [
            "https://www.semanticscholar.org/paper/2ab1545c5d32ba47fbd9934fc4d3f9606ea64dfd"
        ],
        "id": "id-5988750190550169904",
        "abstract": "Low spatial frequencies of atmospheric turbulence are especially troublesome to astronomers because the phase distortions that these frequencies cause have a large amplitude. We have begun experiments at the Multiple Mirror Telescope to remove these errors with tip, tilt, and piston control of pieces of the wave front that are defined by the telescope\u2019s six 1.8-m primary mirrors. We present long-exposure images that were recorded at the telescope with a resolution of as high as 0.08 arcsec under piston control, and 0.32 arcsec under tilt control, by use of an adaptive instrument designed to restore diffraction-limited imaging in the near infrared. Of particular importance for high-resolution imaging is the control of the piston or the mean phase errors between the segments. These errors can be calculated from the Fourier transform of the short-exposure combined-focus image, but the accuracy of the reconstructed wave front depends critically on the signal-to-noise ratio of the data. We present a theoretical analysis of the effects of photon and detector read noise on the derived piston errors and computer simulations of wave-front reconstructor algorithms. We find that a Wiener filter combined with nonlinear weighting of the piston errors minimizes the impact of noise. Finally, we summarize expected improvements to our system and discuss the application of these techniques to forthcoming large telescopes.",
        "versions": [
            {
                "year": 1994,
                "source": "SupportedSources.OPENALEX",
                "title": "Progress in diffraction-limited imaging at the Multiple Mirror Telescope with adaptive optics",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2045530253",
                    "https://doi.org/10.1364/josaa.11.000846"
                ],
                "doi": "10.1364/josaa.11.000846",
                "publication_date": "1994-02-01 00:00:00"
            }
        ],
        "rank": 86
    },
    {
        "authors": [
            "H. A. Taha"
        ],
        "title": "Human-Machine Interaction by Tracking Hand Movements",
        "publication_date": "2013-05-11 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://www.semanticscholar.org/paper/1792fba856df8cf578a1629bebef65065dff7cf7"
        ],
        "id": "id2164705686276552709",
        "abstract": "The vision-based hand tracking and gesture recognition is an extremely challenging problem due to the intricate nature of hand gestures this is a reason that available computer vision algorithms are computationally complex. In this research work a new methodology for 3D human hand gestures detection and recognition is proposed, which can be used for natural and intuitive human-computer interaction and other robotic systems. The proposed method based on Fourier\u2019s descriptors, neural networks and morphology approaches to solve the problem of human hand tracking and gesture recognition of 3D objects from a single silhouette image. There are many constrains and challenges are there for the recognition of an object, like size change, translation, rotation around the three axes, partial occlusion, low intensity of light as well as the deformation of the shape. In this research work we used invariant Fourier\u2019s descriptors and back propagation neural networks techniques for 3D objects recognitions from their 2D silhouette pattern to solve above mentioned challenges. The proposed approach used Fourier\u2019s descriptors coefficients and back propagation neural network with different numbers of hidden layers to build the optimal classifier of 3D pattern from a single silhouette image. Besides that, another method is proposed using image processing and morphology technique in conjunction with various mathematical formulas to calculate hand position and orientation. The recognised objects are exposed to different intensities of light, are partially occluded, with size change, translation, rotation about all the axes and we used also deformed shapes. This new proposed method was applied and tested on the simulated Manipulated Robotic System (UniMAP Robot Manipulator Simulation System) that allows this robotic system to act as an intelligent system to track a human hand in 3D space and estimate its orientation and position in real time with the goal of ultimately using the algorithm with a robotic spherical wrist system. During experiment, there was no need for continuous camera calibration, and it required only once at the beginning for the registration of the hand and using proposed technique large number of hand movements and orientations are correctly identify. Experimental result shows that proposed method is a robust technique, unlike other approaches that use costly leaning functions or generalization methods. The high performance was achieved during experiments because of the accurate hand movement identification and the low computational load that results in a fast processing time. The proposed method could therefore be used with different types of teleoperated robotic manipulators or in other human-computer interaction applications in which a fast processing time was important.",
        "versions": [],
        "rank": 87
    },
    {
        "authors": [
            "Xiaojuan Guo",
            "Yuan Han",
            "Kewei Chen",
            "Yan Wang",
            "Li Yao"
        ],
        "title": "Mapping joint grey and white matter reductions in Alzheimer's disease using joint independent component analysis",
        "publication_date": "2012-12-07 00:00:00",
        "source": "SupportedSources.OPENALEX",
        "journal": "",
        "volume": "531",
        "doi": "10.1016/j.neulet.2012.10.038",
        "urls": [
            "https://openalex.org/W1968603716",
            "https://doi.org/10.1016/j.neulet.2012.10.038",
            "https://europepmc.org/articles/pmc3652975?pdf=render"
        ],
        "id": "id9211643847538488275",
        "abstract": "",
        "versions": [
            {
                "year": 2012,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Mapping joint grey and white matter reductions in Alzheimer's disease using joint independent component analysis",
                "journal": "Neuroscience Letters",
                "urls": [
                    "https://www.semanticscholar.org/paper/e0a16a5d2066da7ff1e46ab46ecc90e78d08d9b3",
                    "https://europepmc.org/articles/pmc3652975?pdf=render"
                ],
                "doi": "10.1016/j.neulet.2012.10.038",
                "publication_date": "2012-12-01 00:00:00"
            }
        ],
        "rank": 88
    },
    {
        "authors": [
            "Aliasghar Mortazi",
            "Jeremy Burt",
            "Ulas Bagci"
        ],
        "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
        "publication_date": "2018-01-01 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "Springer International Publishing",
        "volume": "",
        "doi": "10.1007/978-3-319-75541-0_21",
        "urls": [
            "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
        ],
        "id": "id-7952112184099688693",
        "abstract": "Non-invasive detection of cardiovascular disorders from radiology scans requires quantitative image analysis of the heart and its substructures. There are well-established measurements that radiologists use for diseases assessment such as ejection fraction, volume of four chambers, and myocardium mass. These measurements are derived as outcomes of precise segmentation of the heart and its substructures. The aim of this paper is to provide such measurements through an accurate image segmentation algorithm that automatically delineates seven substructures of the heart from MRI and/or CT scans. Our proposed method is based on multi-planar deep convolutional neural networks (CNN) with an adaptive fusion strategy where we automatically utilize complementary information from different planes of the 3D scans for improved delineations. For CT and MRI, we have separately designed three CNNs (the same architectural configuration) for three planes, and have trained the networks from scratch for voxel-wise labeling for the following cardiac structures: myocardium of left ventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA), right ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We have evaluated the proposed method with 4-fold-cross-validation on the multi-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The precision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for CT and MR images, respectively. While a CT volume was segmented about 50 seconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA implementation.",
        "versions": [
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            },
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            },
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            },
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            },
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            },
            {
                "year": 2018,
                "source": "SupportedSources.INTERNET_ARCHIVE",
                "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT",
                "journal": "Springer International Publishing",
                "urls": [
                    "https://web.archive.org/web/20190227013540/http://pdfs.semanticscholar.org/7a41/589be1c2d892e2737e92582496d5c7d8e4d5.pdf"
                ],
                "doi": "10.1007/978-3-319-75541-0_21",
                "publication_date": "2018-01-01 00:00:00"
            }
        ],
        "rank": 89
    },
    {
        "authors": [
            "Francisco J. Mart\u00ednez-Murcia",
            "Juan Manuel G\u00f3rriz",
            "Javier Ram\u00edrez"
        ],
        "title": "Computer-Aided Diagnosis in Neuroimaging",
        "publication_date": "2016-12-07 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "InTech",
        "volume": "",
        "doi": "10.5772/64980",
        "urls": [
            "https://web.archive.org/web/20190501030021/https://cdn.intechopen.com/pdfs/52202.pdf"
        ],
        "id": "id3215299195591382476",
        "abstract": "This chapter is intended to provide an overview to the most used methods for computeraided diagnosis in neuroimaging and its application to neurodegenerative diseases. The fundamental preprocessing steps, and how they are applied to different image modalities, will be thoroughly presented. We introduce a number of widely used neuroimaging analysis algorithms, together with a wide overview on the recent advances in brain imaging processing. Finally, we provide a general conclusion on the state of the art in brain imaging processing and possible future developments.",
        "versions": [
            {
                "year": 2016,
                "source": "SupportedSources.CORE",
                "title": "Computer-Aided Diagnosis in Neuroimaging",
                "journal": "",
                "urls": [
                    "https://core.ac.uk/download/322428090.pdf"
                ],
                "doi": "10.5772/64980",
                "publication_date": "2016-12-07 00:00:00"
            }
        ],
        "rank": 90
    },
    {
        "authors": [
            "Zhou, Xiaoyun"
        ],
        "title": "3D shape instantiation for intra-operative navigation from a single 2D projection",
        "publication_date": "2020-03-01 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": "10.25560/80269",
        "urls": [
            "https://core.ac.uk/download/326509814.pdf"
        ],
        "id": "id6330856706160590606",
        "abstract": "Unlike traditional open surgery where surgeons can see the operation area clearly, in robot-assisted Minimally Invasive Surgery (MIS), a surgeon\u2019s view of the region of interest is usually limited. Currently, 2D images from fluoroscopy, Magnetic Resonance Imaging (MRI), endoscopy or ultrasound are used for intra-operative guidance as real-time 3D volumetric acquisition is not always possible due to the acquisition speed or exposure constraints. 3D reconstruction, however, is key to navigation in complex in vivo geometries and can help resolve this issue. Novel 3D shape instantiation schemes are developed in this thesis, which can reconstruct the high-resolution 3D shape of a target from limited 2D views, especially a single 2D projection or slice. To achieve a complete and automatic 3D shape instantiation pipeline, segmentation schemes based on deep learning are also investigated. These include normalization schemes for training U-Nets and network architecture design of Atrous Convolutional Neural Networks (ACNNs).\r\n\r\nFor U-Net normalization, four popular normalization methods are reviewed, then Instance-Layer Normalization (ILN) is proposed. It uses a sigmoid function to linearly weight the feature map after instance normalization and layer normalization, and cascades group normalization after the weighted feature map. Detailed validation results potentially demonstrate the practical advantages of the proposed ILN for effective and robust segmentation of different anatomies.\r\n\r\nFor network architecture design in training Deep Convolutional Neural Networks (DCNNs), the newly proposed ACNN is compared to traditional U-Net where max-pooling and deconvolutional layers are essential. Only convolutional layers are used in the proposed ACNN with different atrous rates and it has been shown that the method is able to provide a fully-covered receptive field with a minimum number of atrous convolutional layers. ACNN enhances the robustness and generalizability of the analysis scheme by cascading multiple atrous blocks. Validation results have shown the proposed method achieves comparable results to the U-Net in terms of medical image segmentation, whilst reducing the trainable parameters, thus improving the convergence and real-time instantiation speed. \r\n\r\nFor 3D shape instantiation of soft and deforming organs during MIS, Sparse Principle Component Analysis (SPCA) has been used to analyse a  3D Statistical Shape Model (SSM) and to determine the most informative scan plane. Synchronized 2D images are then scanned at the most informative scan plane and are expressed in a 2D SSM. Kernel Partial Least Square Regression (KPLSR) has been applied to learn the relationship between the 2D and 3D SSM. It has been shown that the KPLSR-learned model developed in this thesis is able to predict the intra-operative 3D target shape from a single 2D projection or slice, thus permitting real-time 3D navigation. Validation results have shown the intrinsic accuracy achieved and the potential clinical value of the technique.\r\n\r\nThe proposed 3D shape instantiation scheme is further applied to intra-operative stent graft deployment for the robot-assisted treatment of aortic aneurysms. Mathematical modelling is first used to simulate the stent graft characteristics. This is then followed by the Robust Perspective-n-Point (RPnP) method to instantiate the 3D pose of fiducial markers of the graft. Here, Equally-weighted Focal U-Net is proposed with a cross-entropy and an additional focal loss function. Detailed validation has been performed on patient-specific stent grafts with an accuracy between 1-3mm. Finally, the relative merits and potential pitfalls of all the methods developed in this thesis are discussed, followed by potential future research directions and additional challenges that need to be tackled.Open Acces",
        "versions": [],
        "rank": 91
    },
    {
        "authors": [
            "Alexander Kim",
            "Kyuhyup Lee",
            "Seojoon Lee",
            "Jinwoo Song",
            "Soonwook Kwon",
            "Suwan Chung"
        ],
        "title": "Synthetic Data and Computer-Vision-Based Automated Quality Inspection System for Reused Scaffolding",
        "publication_date": "2022-10-08 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "MDPI AG",
        "volume": "",
        "doi": "10.3390/app121910097",
        "urls": [
            "https://web.archive.org/web/20221010112617/https://mdpi-res.com/d_attachment/applsci/applsci-12-10097/article_deploy/applsci-12-10097.pdf?version=1665227763"
        ],
        "id": "id5531433493585637264",
        "abstract": "Regular scaffolding quality inspection is an essential part of construction safety. However, current evaluation methods and quality requirements for temporary structures are based on subjective visual inspection by safety managers. Accordingly, the assessment process and results depend on an inspector's competence, experience, and human factors, making objective analysis complex. The safety inspections performed by specialized services bring additional costs and increase evaluation times. Therefore, a temporary structure quality and safety evaluation system based on experts' experience and independent of the human factor is the relevant solution in intelligent construction. This study aimed to present a quality evaluation system prototype for scaffolding parts based on computer vision. The main steps of the proposed system development are preparing a dataset, designing a neural network (NN) model, and training and evaluating the model. Since traditional methods of preparing a dataset are very laborious and time-consuming, this work used mixed real and synthetic datasets modeled in Blender. Further, the resulting datasets were processed using artificial intelligence algorithms to obtain information about defect type, size, and location. Finally, the tested parts' quality classes were calculated based on the obtained defect values.",
        "versions": [],
        "rank": 92
    },
    {
        "authors": [
            "K. Badhiwala",
            "Daniel L. Gonzales",
            "Daniel G Vercosa",
            "B. Avants",
            "Jacob T. Robinson"
        ],
        "title": "Microfluidics for electrophysiology, imaging, and behavioral analysis of Hydra.",
        "publication_date": "2018-08-21 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Lab on a chip",
        "volume": "18 17",
        "doi": "10.1039/c8lc00475g",
        "urls": [
            "https://www.semanticscholar.org/paper/aa64b11cb3946b76fd4253223ef952892d8a663e",
            "https://www.biorxiv.org/content/biorxiv/early/2018/02/08/257691.full.pdf"
        ],
        "id": "id3066509751731140857",
        "abstract": "The nervous system of the cnidarian Hydra vulgaris exhibits remarkable regenerative abilities. When cut in two, the bisected tissue reorganizes into fully behaving animals in approximately 48 hours. Furthermore, new animals can reform from aggregates of dissociated cells. Understanding how behaviors are coordinated by this highly plastic nervous system could reveal basic principles of neural circuit dynamics underlying behaviors. However, Hydra's deformable and contractile body makes it difficult to manipulate the local environment while recording neural activity. Here, we present the first microfluidic technologies capable of simultaneous electrical, chemical, and optical interrogation of these soft, deformable organisms. Specifically, we demonstrate devices that can immobilize Hydra for hours-long simultaneous electrical and optical recording, and chemical stimulation of behaviors revealing neural activity during muscle contraction. We further demonstrate quantitative locomotive and behavioral tracking made possible by confining the animal to quasi-two-dimensional micro-arenas. Together, these proof-of-concept devices show that microfluidics provide a platform for scalable, quantitative cnidarian neurobiology. The experiments enabled by this technology may help reveal how highly plastic networks of neurons provide robust control of animal behavior.",
        "versions": [],
        "rank": 93
    },
    {
        "authors": [
            "San, Wai"
        ],
        "title": "Accurate dense depth from light field technology for object segmentation and 3D computer vision",
        "publication_date": "2020-07-06 00:00:00",
        "source": "SupportedSources.CORE",
        "journal": "",
        "volume": "",
        "doi": "10.14264/uql.2020.887",
        "urls": [
            "https://core.ac.uk/download/328923467.pdf"
        ],
        "id": "id-8939694837840215109",
        "abstract": null,
        "versions": [
            {
                "year": 2020,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Accurate dense depth from light field technology for object segmentation and 3D computer vision",
                "journal": "",
                "urls": [
                    "https://www.semanticscholar.org/paper/1c34906ec8c7a50cac8b97801c6ea1391c9b4266"
                ],
                "doi": "10.14264/uql.2020.887",
                "publication_date": "2020-07-06 00:00:00"
            }
        ],
        "rank": 94
    },
    {
        "authors": [
            "Andrew Lizarraga",
            "David Lee",
            "Antoni Kubicki",
            "Ashish Sahib",
            "Elvis Nunez",
            "Katherine Narr",
            "Shantanu H. Joshi"
        ],
        "title": "Alignment of Tractography Streamlines using Deformation Transfer via Parallel Transport",
        "publication_date": "2021-08-08 17:58:30+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "",
        "urls": [
            "http://arxiv.org/pdf/2108.03697v1",
            "http://arxiv.org/abs/2108.03697v1",
            "http://arxiv.org/pdf/2108.03697v1"
        ],
        "id": "id7895144524771651575",
        "abstract": "We present a geometric framework for aligning white matter fiber tracts. By\nregistering fiber tracts between brains, one expects to see overlap of\nanatomical structures that often provide meaningful comparisons across\nsubjects. However, the geometry of white matter tracts is highly heterogeneous,\nand finding direct tract-correspondence across multiple individuals remains a\nchallenging problem. We present a novel deformation metric between tracts that\nallows one to compare tracts while simultaneously obtaining a registration. To\naccomplish this, fiber tracts are represented by an intrinsic mean along with\nthe deformation fields represented by tangent vectors from the mean. In this\nsetting, one can determine a parallel transport between tracts and then\nregister corresponding tangent vectors. We present the results of bundle\nalignment on a population of 43 healthy adult subjects.",
        "versions": [],
        "rank": 95
    },
    {
        "authors": [
            "D\u2032Souza, W.",
            "Castro-Pareja, C.",
            "Nazareth, D.",
            "Lei, P.",
            "Naqvi, S.",
            "Shekhar, R."
        ],
        "title": "Intra-Fraction Dose Registration using Relative Phase-Time Dose-Weighted 3D Dose Distributions and Deformable Image Registration",
        "publication_date": "2005-01-01 00:00:00",
        "source": "SupportedSources.CROSSREF",
        "journal": "",
        "volume": "",
        "doi": "10.1016/j.ijrobp.2005.07.915",
        "urls": [
            "https://api.elsevier.com/content/article/PII:S0360301605021024?httpAccept=text/xml",
            "https://api.elsevier.com/content/article/PII:S0360301605021024?httpAccept=text/plain",
            "http://dx.doi.org/10.1016/j.ijrobp.2005.07.915"
        ],
        "id": "id-4864468887797376927",
        "abstract": "",
        "versions": [
            {
                "year": 2005,
                "source": "SupportedSources.SEMANTIC_SCHOLAR",
                "title": "Intra-Fraction Dose Registration using Relative Phase-Time Dose-Weighted 3D Dose Distributions and Deformable Image Registration",
                "journal": "International Journal of Radiation Oncology Biology Physics",
                "urls": [
                    "https://www.semanticscholar.org/paper/31144f102ad8b9e5f8824373764c2c256497b3f7",
                    "http://www.redjournal.org/article/S0360301605021024/pdf"
                ],
                "doi": "10.1016/J.IJROBP.2005.07.915",
                "publication_date": "2005-10-01 00:00:00"
            }
        ],
        "rank": 96
    },
    {
        "authors": [
            "Jun Li",
            "Wanrong Hong",
            "Yusheng Xiang"
        ],
        "title": "A Short Review on Data Modelling for Vector Fields",
        "publication_date": "2020-09-01 00:00:00",
        "source": "SupportedSources.INTERNET_ARCHIVE",
        "journal": "",
        "volume": "",
        "doi": "",
        "urls": [
            "https://web.archive.org/web/20200908192736/https://arxiv.org/pdf/2009.00577v1.pdf"
        ],
        "id": "id-2217760982653373440",
        "abstract": "Machine learning methods based on statistical principles have proven highly successful in dealing with a wide variety of data analysis and analytics tasks. Traditional data models are mostly concerned with independent identically distributed data. The recent success of end-to-end modelling scheme using deep neural networks equipped with effective structures such as convolutional layers or skip connections allows the extension to more sophisticated and structured practical data, such as natural language, images, videos, etc. On the application side, vector fields are an extremely useful type of data in empirical sciences, as well as signal processing, e.g. non-parametric transformations of 3D point clouds using 3D vector fields, the modelling of the fluid flow in earth science, and the modelling of physical fields. This review article is dedicated to recent computational tools of vector fields, including vector data representations, predictive model of spatial data, as well as applications in computer vision, signal processing, and empirical sciences.",
        "versions": [],
        "rank": 97
    },
    {
        "authors": [
            "Fereshteh S. Bashiri",
            "Ahmadreza Baghaie",
            "Reihaneh Rostami",
            "Zeyun Yu",
            "R. D'Souza"
        ],
        "title": "Multi-Modal Medical Image Registration with Full or Partial Data: A Manifold Learning Approach",
        "publication_date": "2018-12-30 00:00:00",
        "source": "SupportedSources.SEMANTIC_SCHOLAR",
        "journal": "Journal of Imaging",
        "volume": "5",
        "doi": "10.3390/jimaging5010005",
        "urls": [
            "https://www.semanticscholar.org/paper/0a3fad1cf0706ccea9a58705e7e79b584d944b0e",
            "https://www.mdpi.com/2313-433X/5/1/5/pdf"
        ],
        "id": "id-6935212185819306682",
        "abstract": "Multi-modal image registration is the primary step in integrating information stored in two or more images, which are captured using multiple imaging modalities. In addition to intensity variations and structural differences between images, they may have partial or full overlap, which adds an extra hurdle to the success of registration process. In this contribution, we propose a multi-modal to mono-modal transformation method that facilitates direct application of well-founded mono-modal registration methods in order to obtain accurate alignment of multi-modal images in both cases, with complete (full) and incomplete (partial) overlap. The proposed transformation facilitates recovering strong scales, rotations, and translations. We explain the method thoroughly and discuss the choice of parameters. For evaluation purposes, the effectiveness of the proposed method is examined and compared with widely used information theory-based techniques using simulated and clinical human brain images with full data. Using RIRE dataset, mean absolute error of 1.37, 1.00, and 1.41 mm are obtained for registering CT images with PD-, T1-, and T2-MRIs, respectively. In the end, we empirically investigate the efficacy of the proposed transformation in registering multi-modal partially overlapped images.",
        "versions": [
            {
                "year": 2018,
                "source": "SupportedSources.OPENALEX",
                "title": "Multi-Modal Medical Image Registration with Full or Partial Data: A Manifold Learning Approach",
                "journal": "",
                "urls": [
                    "https://openalex.org/W2908425794",
                    "https://doi.org/10.3390/jimaging5010005",
                    "https://www.mdpi.com/2313-433X/5/1/5/pdf"
                ],
                "doi": "10.3390/jimaging5010005",
                "publication_date": "2018-12-30 00:00:00"
            }
        ],
        "rank": 98
    },
    {
        "authors": [
            "Boyun Li",
            "Yijie Lin",
            "Xiao Liu",
            "Peng Hu",
            "Jiancheng Lv",
            "Xi Peng"
        ],
        "title": "Unsupervised Neural Rendering for Image Hazing",
        "publication_date": "2021-07-14 13:15:14+00:00",
        "source": "SupportedSources.ARXIV",
        "journal": null,
        "volume": "",
        "doi": "10.1109/TIP.2022.3177321",
        "urls": [
            "http://arxiv.org/pdf/2107.06681v1",
            "http://dx.doi.org/10.1109/TIP.2022.3177321",
            "http://arxiv.org/abs/2107.06681v1",
            "http://arxiv.org/pdf/2107.06681v1"
        ],
        "id": "id1701626846874410815",
        "abstract": "Image hazing aims to render a hazy image from a given clean one, which could\nbe applied to a variety of practical applications such as gaming, filming,\nphotographic filtering, and image dehazing. To generate plausible haze, we\nstudy two less-touched but challenging problems in hazy image rendering,\nnamely, i) how to estimate the transmission map from a single image without\nauxiliary information, and ii) how to adaptively learn the airlight from\nexemplars, i.e., unpaired real hazy images. To this end, we propose a neural\nrendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN\nis a knowledge-driven neural network which estimates the transmission map by\nleveraging a new prior, i.e., there exists the structure similarity (e.g.,\ncontour and luminance) between the transmission map and the input clean image.\nTo adaptively learn the airlight, we build a neural module based on another new\nprior, i.e., the rendered hazy image and the exemplar are similar in the\nairlight distribution. To the best of our knowledge, this could be the first\nattempt to deeply rendering hazy images in an unsupervised fashion. Comparing\nwith existing haze generation methods, HazeGEN renders the hazy images in an\nunsupervised, learnable, and controllable manner, thus avoiding the\nlabor-intensive efforts in paired data collection and the domain-shift issue in\nhaze generation. Extensive experiments show the promising performance of our\nmethod comparing with some baselines in both qualitative and quantitative\ncomparisons. The code will be released on GitHub after acceptance.",
        "versions": [],
        "rank": 99
    }
]